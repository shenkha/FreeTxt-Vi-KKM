{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25664,
     "status": "ok",
     "timestamp": 1688464810300,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "qgdkjVf_oB8j",
    "outputId": "5cbb67cd-3f72-4de4-b0bb-e01ac86bbe77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaYxfGHveC2e"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1835,
     "status": "ok",
     "timestamp": 1688464814093,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "SVYGV0Q28Vfe",
    "outputId": "3eb8a457-a31c-49be-cc35-3a31fc6485ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CS117ToxicComment'...\n",
      "remote: Enumerating objects: 16, done.\u001b[K\n",
      "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 16 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (16/16), 317.11 KiB | 1.62 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/IrisPham74/CS117ToxicComment.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2tmjbD8IWI0u"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datafolderPath = \"/content/CS117ToxicComment/Dataset.xlsx\"\n",
    "\n",
    "#dev = pd.read_csv(datafolderPath+\"/dev.csv\")\n",
    "#test = pd.read_csv(datafolderPath+\"/test.csv\")\n",
    "dataset = pd.read_excel(datafolderPath)\n",
    "\n",
    "#dataset = pd.concat([train, dev, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 483,
     "status": "ok",
     "timestamp": 1688464818919,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "Tavb8gacc4eK",
    "outputId": "24ba3744-3430-4421-ab7d-bd9eb2aec0e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-66384ed6-97c6-4079-9767-87ed0e98a5a9\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>đu v l</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>mấy a Favelas nhồng vl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Liên hoan xác thịt:v</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Cá rô phi ghê vcl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>nip tay to vcl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8088</th>\n",
       "      <td>8089</td>\n",
       "      <td>ruler bắn cháy vãi lồn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8089</th>\n",
       "      <td>8090</td>\n",
       "      <td>vãi thật mới 1 game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>8091</td>\n",
       "      <td>999 lam may qua ac vl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8091</th>\n",
       "      <td>8092</td>\n",
       "      <td>đâu r</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>8093</td>\n",
       "      <td>T2 cay</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8093 rows × 3 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66384ed6-97c6-4079-9767-87ed0e98a5a9')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-66384ed6-97c6-4079-9767-87ed0e98a5a9 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-66384ed6-97c6-4079-9767-87ed0e98a5a9');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       No.                 Comment  Label\n",
       "0        1                  đu v l      1\n",
       "1        2  mấy a Favelas nhồng vl      1\n",
       "2        3    Liên hoan xác thịt:v      0\n",
       "3        4       Cá rô phi ghê vcl      1\n",
       "4        5          nip tay to vcl      1\n",
       "...    ...                     ...    ...\n",
       "8088  8089  ruler bắn cháy vãi lồn      1\n",
       "8089  8090     vãi thật mới 1 game      1\n",
       "8090  8091   999 lam may qua ac vl      1\n",
       "8091  8092                   đâu r      0\n",
       "8092  8093                  T2 cay      0\n",
       "\n",
       "[8093 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmvowkcWV49I"
   },
   "source": [
    "#ULTIMATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VNgDTB69Odxj"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCoTBsdNRBIu"
   },
   "source": [
    "Xoá HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZRkT0txOPzz"
   },
   "outputs": [],
   "source": [
    "def remove_html(txt):\n",
    "    return re.sub(r\"http\\S+\", \"\", txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otueGdYQUDYV"
   },
   "source": [
    "##Chuyển câu văn về kiểu gõ telex khi không bật Unikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1688464827666,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "8x5-4pmiUEjk",
    "outputId": "b0fcc63a-ad17-4fcb-ce53-b51dfec5e13d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n    End section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Start section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\n",
    "    Ví dụ: thủy = thuyr, tượng = tuwowngj\n",
    "\"\"\"\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "\n",
    "def vn_word_to_telex_type(word):\n",
    "    dau_cau = 0\n",
    "    new_word = ''\n",
    "    for char in word:\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            new_word += char\n",
    "            continue\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "        new_word += bang_nguyen_am[x][-1]\n",
    "    new_word += bang_ky_tu_dau[dau_cau]\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def vn_sentence_to_telex_type(sentence):\n",
    "    \"\"\"\n",
    "    Chuyển câu tiếng việt có dấu về kiểu gõ telex.\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        words[index] = vn_word_to_telex_type(word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    End section: Chuyển câu văn về kiểu gõ telex khi không bật Unikey\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1688464830838,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "R8fRLySjpbAd",
    "outputId": "907d8e24-bc6b-4894-c76a-51a6069aff35"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'gaf'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "vn_word_to_telex_type('gà')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNG-qc3tRF3Z"
   },
   "source": [
    "# **Chuẩn hoá unicode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr_EHfVoRUex"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import regex as re\n",
    "\n",
    "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
    "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
    "\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
    "        '|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
    "        '|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "\n",
    "\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "\n",
    "def convert_unicode(txt):\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8BMFB0kUNsu"
   },
   "source": [
    "Dùng òa úy thay oà uý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1688464837005,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "yjJPkrKgUOwH",
    "outputId": "2d66f7c0-32a4-4a64-dc90-a46a23c8140d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anh hòa, đang làm.. gì\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    Start section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
    "    Xem tại đây:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1:\n",
    "            continue\n",
    "        elif x == 9:  # check qu\n",
    "            if index != 0 and chars[index - 1] == 'q':\n",
    "                chars[index] = 'u'\n",
    "                qu_or_gi = True\n",
    "        elif x == 5:  # check gi\n",
    "            if index != 0 and chars[index - 1] == 'g':\n",
    "                chars[index] = 'i'\n",
    "                qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y\n",
    "            chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1:\n",
    "            nguyen_am_index.append(index)\n",
    "    if len(nguyen_am_index) < 2:\n",
    "        if qu_or_gi:\n",
    "            if len(chars) == 2:\n",
    "                x, y = nguyen_am_to_ids.get(chars[1])\n",
    "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
    "            else:\n",
    "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
    "                if x != -1:\n",
    "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
    "                else:\n",
    "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
    "            return ''.join(chars)\n",
    "        return word\n",
    "\n",
    "    for index in nguyen_am_index:\n",
    "        x, y = nguyen_am_to_ids[chars[index]]\n",
    "        if x == 4 or x == 8:  # ê, ơ\n",
    "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
    "            # for index2 in nguyen_am_index:\n",
    "            #     if index2 != index:\n",
    "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
    "            #         chars[index2] = bang_nguyen_am[x][0]\n",
    "            return ''.join(chars)\n",
    "\n",
    "    if len(nguyen_am_index) == 2:\n",
    "        if nguyen_am_index[-1] == len(chars) - 1:\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
    "        else:\n",
    "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "    else:\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
    "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
    "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
    "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
    "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
    "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
    "    return ''.join(chars)\n",
    "\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    \"\"\"\n",
    "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
    "        :param sentence:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
    "        # print(cw)\n",
    "        if len(cw) == 3:\n",
    "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
    "        words[index] = ''.join(cw)\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    End section: Chuyển câu văn về cách gõ dấu kiểu cũ: dùng òa úy thay oà uý\n",
    "    Xem tại đây: https://vi.wikipedia.org/wiki/Quy_tắc_đặt_dấu_thanh_trong_chữ_quốc_ngữ\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    print(chuan_hoa_dau_cau_tieng_viet('anh hoà, đang làm.. gì'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7gXpXMuVnY_"
   },
   "source": [
    "Loại bỏ ký tự cố tính viết dài trong câu:\n",
    "\n",
    "eg: đâyyyyyyyyyy -> đây"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOKJRUGXXiwU"
   },
   "outputs": [],
   "source": [
    "# lấy dữ liệu cho teencode\n",
    "import pandas as pd\n",
    "teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
    "teencode_list = teencode_df['teencode'].to_list()\n",
    "map_list = teencode_df['map'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2509,
     "status": "ok",
     "timestamp": 1688464845550,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "NzP1ndUSOiyt",
    "outputId": "8651a730-99b2-4529-a6f9-ec60f916f93f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Unzipping misc/perluniprops.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nltk for connect sent\n",
    "import nltk\n",
    "nltk.download('perluniprops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6796,
     "status": "ok",
     "timestamp": 1688464852344,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "UqHtjTzw-P3I",
    "outputId": "0c0d3fd5-b458-4a7a-af33-179c161e088a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=2144176336516680137b78ef5f6ee248ef9f0efff75a977820b495882c5ca13c\n",
      "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses\n",
      "Successfully installed sacremoses-0.0.53\n"
     ]
    }
   ],
   "source": [
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1688464855340,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "xX5-Iyl3Ou21",
    "outputId": "1296ae31-16ce-4d11-9294-38fbcfcee563"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Story: I wish my dog's hair was fluffier, and he ate better\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = ['Story', ':', 'I', 'wish', 'my', 'dog', \"'s\", 'hair', 'was', 'fluffier', ',', 'and', 'he', 'ate', 'better']\n",
    "\n",
    "#from nltk.tokenize.moses import MosesDetokenizer\n",
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "detokens = MosesDetokenizer().detokenize(test, return_str=True)\n",
    "detokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27868,
     "status": "ok",
     "timestamp": 1688464886336,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "sjE3lUbq7PZk",
    "outputId": "e02f5d30-2c07-4396-8b81-c2d3085459e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  aspell aspell-en dictionaries-common emacsen-common hunspell-en-us\n",
      "  libaspell15 libenchant1c2a libhunspell-1.7-0 libtext-iconv-perl\n",
      "Suggested packages:\n",
      "  aspell-doc spellutils wordlist hunspell openoffice.org-hunspell\n",
      "  | openoffice.org-core libenchant-voikko\n",
      "The following NEW packages will be installed:\n",
      "  aspell aspell-en dictionaries-common emacsen-common enchant hunspell-en-us\n",
      "  libaspell15 libenchant1c2a libhunspell-1.7-0 libtext-iconv-perl\n",
      "0 upgraded, 10 newly installed, 0 to remove and 15 not upgraded.\n",
      "Need to get 1,316 kB of archives.\n",
      "After this operation, 5,474 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libtext-iconv-perl amd64 1.7-7 [13.8 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libaspell15 amd64 0.60.8-1ubuntu0.1 [328 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 emacsen-common all 3.0.4 [14.9 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 dictionaries-common all 1.28.1 [178 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 aspell amd64 0.60.8-1ubuntu0.1 [88.4 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 hunspell-en-us all 1:2018.04.16-1 [170 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libhunspell-1.7-0 amd64 1.7.0-2build2 [147 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal/universe amd64 libenchant1c2a amd64 1.6.0-11.3build1 [64.7 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal/universe amd64 enchant amd64 1.6.0-11.3build1 [12.4 kB]\n",
      "Fetched 1,316 kB in 2s (651 kB/s)\n",
      "Preconfiguring packages ...\n",
      "Selecting previously unselected package libtext-iconv-perl.\n",
      "(Reading database ... 123069 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libtext-iconv-perl_1.7-7_amd64.deb ...\n",
      "Unpacking libtext-iconv-perl (1.7-7) ...\n",
      "Selecting previously unselected package libaspell15:amd64.\n",
      "Preparing to unpack .../1-libaspell15_0.60.8-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\n",
      "Selecting previously unselected package emacsen-common.\n",
      "Preparing to unpack .../2-emacsen-common_3.0.4_all.deb ...\n",
      "Unpacking emacsen-common (3.0.4) ...\n",
      "Selecting previously unselected package dictionaries-common.\n",
      "Preparing to unpack .../3-dictionaries-common_1.28.1_all.deb ...\n",
      "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
      "Unpacking dictionaries-common (1.28.1) ...\n",
      "Selecting previously unselected package aspell.\n",
      "Preparing to unpack .../4-aspell_0.60.8-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking aspell (0.60.8-1ubuntu0.1) ...\n",
      "Selecting previously unselected package aspell-en.\n",
      "Preparing to unpack .../5-aspell-en_2018.04.16-0-1_all.deb ...\n",
      "Unpacking aspell-en (2018.04.16-0-1) ...\n",
      "Selecting previously unselected package hunspell-en-us.\n",
      "Preparing to unpack .../6-hunspell-en-us_1%3a2018.04.16-1_all.deb ...\n",
      "Unpacking hunspell-en-us (1:2018.04.16-1) ...\n",
      "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
      "Preparing to unpack .../7-libhunspell-1.7-0_1.7.0-2build2_amd64.deb ...\n",
      "Unpacking libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\n",
      "Selecting previously unselected package libenchant1c2a:amd64.\n",
      "Preparing to unpack .../8-libenchant1c2a_1.6.0-11.3build1_amd64.deb ...\n",
      "Unpacking libenchant1c2a:amd64 (1.6.0-11.3build1) ...\n",
      "Selecting previously unselected package enchant.\n",
      "Preparing to unpack .../9-enchant_1.6.0-11.3build1_amd64.deb ...\n",
      "Unpacking enchant (1.6.0-11.3build1) ...\n",
      "Setting up libtext-iconv-perl (1.7-7) ...\n",
      "Setting up libaspell15:amd64 (0.60.8-1ubuntu0.1) ...\n",
      "Setting up emacsen-common (3.0.4) ...\n",
      "Setting up libhunspell-1.7-0:amd64 (1.7.0-2build2) ...\n",
      "Setting up dictionaries-common (1.28.1) ...\n",
      "Setting up aspell (0.60.8-1ubuntu0.1) ...\n",
      "Setting up hunspell-en-us (1:2018.04.16-1) ...\n",
      "Setting up aspell-en (2018.04.16-0-1) ...\n",
      "Setting up libenchant1c2a:amd64 (1.6.0-11.3build1) ...\n",
      "Setting up enchant (1.6.0-11.3build1) ...\n",
      "Processing triggers for man-db (2.9.1-1) ...\n",
      "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
      "Processing triggers for dictionaries-common (1.28.1) ...\n",
      "aspell-autobuildhash: processing: en [en-common].\n",
      "aspell-autobuildhash: processing: en [en-variant_0].\n",
      "aspell-autobuildhash: processing: en [en-variant_1].\n",
      "aspell-autobuildhash: processing: en [en-variant_2].\n",
      "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
      "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
      "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
      "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
      "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
      "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
      "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
      "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
      "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
      "Collecting pyenchant\n",
      "  Downloading pyenchant-3.2.2-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyenchant\n",
      "Successfully installed pyenchant-3.2.2\n"
     ]
    }
   ],
   "source": [
    "# For chec a word is an eng word\n",
    "!apt install enchant\n",
    "!pip3 install pyenchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2gQHRZhvwcN"
   },
   "outputs": [],
   "source": [
    "import enchant\n",
    "eng = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1688464991966,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "GtchyroHIsAj",
    "outputId": "4736292f-f9a4-4983-ab1a-574856d57439"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'helo, cute vcl tộc trưởng đi đâu chơi đấy :)))))))) ??????. Đc c, được luôn. Hahahahahaha ?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'hello, cute vcllll tộc ttrưởng đi đâu chơi đấyyyyyy :)))))))) ??????. Đc cc, được luôn. Hahahahahahaaaaaaaaaaaaaaaa ?'\n",
    "\n",
    "def remove_dub_char(sentence):\n",
    "  sentence = str(sentence)\n",
    "  words = []\n",
    "  for word in sentence.strip().split():\n",
    "    if word in teencode_list:\n",
    "      words.append(word)\n",
    "      continue\n",
    "    if eng.check(str(word)):\n",
    "      words.append(word)\n",
    "      continue\n",
    "    words.append(re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE))\n",
    "  return ' '.join(words)\n",
    "\n",
    "def remove_dub_spec_char(sentence):\n",
    "    sentence = str(sentence)\n",
    "    words = []\n",
    "    for word in sentence.strip().split():\n",
    "        if word in teencode_list:\n",
    "            words.append(word)\n",
    "            continue\n",
    "        if eng.check(str(word)):\n",
    "            words.append(word)\n",
    "            continue\n",
    "        cleaned_word = re.sub(r'([^\\w\\s])\\1+', r'\\1', word)\n",
    "        words.append(cleaned_word)\n",
    "    return ' '.join(words)\n",
    "\n",
    "  # #Tokenize\n",
    "  # tokens_list = word_tokenize(sentence)\n",
    "  # for idx, word in enumerate(tokens_list):\n",
    "  #   if word in teencode_list:\n",
    "  #     continue\n",
    "  #   if eng.check(str(word)):\n",
    "  #     continue\n",
    "  #   worded = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n",
    "  #   if worded != word:\n",
    "  #     noneed_count += 1\n",
    "  #   tokens_list[idx] = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n",
    "\n",
    "  # tokens_list = word_tokenize()\n",
    "  # nestList_tokens = rdrsegmenter.tokenize(sentence)\n",
    "  # for tokens_idx, text_tokens in enumerate(nestList_tokens):\n",
    "  #   lenn += len(text_tokens)\n",
    "  #   for idx, word in enumerate(text_tokens):\n",
    "  #     #Neu tu co trong danh sach teencode thi continue\n",
    "  #     if word in teencode_list:\n",
    "  #       continue\n",
    "  #     if eng.check(str(word)):\n",
    "  #       continue\n",
    "  #     text_tokens[idx] = re.sub(r'([A-Z])\\1+', lambda m: m.group(1), word, flags = re.IGNORECASE)\n",
    "  #   nestList_tokens[tokens_idx] = text_tokens\n",
    "\n",
    "  # MAKE LIST FLAT\n",
    "  # flat_list = [item for sublist in tokens_list for item in sublist]\n",
    "\n",
    "  # detokens = MosesDetokenizer().detokenize(tokens_list, return_str=True)\n",
    "\n",
    "  # return detokens\n",
    "\n",
    "remove_dub_char(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Wti4h-oUxSx"
   },
   "source": [
    "**Tách từ tiếng Việt**\n",
    "\n",
    "Học sinh học sinh học ⇒ Học_sinh học sinh_học"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTptNXUy7sEu"
   },
   "source": [
    "# GIAI ĐOẠN 1\n",
    "1. lowercase sentences\n",
    "2. delete redundant spaces\n",
    "3. delete links\n",
    "4. normalize unicode\n",
    "5. delete redundant characters\n",
    "6. normalize accented letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7hgkCT_vDpE"
   },
   "outputs": [],
   "source": [
    "link_count = 0\n",
    "unicode_count = 0\n",
    "dau_count = 0\n",
    "lower_count = 0\n",
    "noneed_count = 0\n",
    "space_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LOK9e25VuPX"
   },
   "outputs": [],
   "source": [
    "def text_preprocess(document):\n",
    "    global link_count\n",
    "    global unicode_count\n",
    "    global dau_count\n",
    "    global lower_count\n",
    "    global noneed_count\n",
    "    global space_count\n",
    "\n",
    "    document = str(document)\n",
    "\n",
    "    # đưa về lower\n",
    "    ducument_before_lower = document\n",
    "    document = document.lower()\n",
    "    if ducument_before_lower != document:\n",
    "      # print(\"Cau chua xu ly:\", ducument_before_lower)\n",
    "      # print(\"Cau da xu ly:\", document)\n",
    "      lower_count += 1\n",
    "    # del document1\n",
    "\n",
    "    # xóa khoảng trắng thừa\n",
    "    ducument_before_space = document\n",
    "    document = re.sub(r'\\s+', ' ', document).strip()\n",
    "    if ducument_before_space != document:\n",
    "      # print(\"Cau chua xu ly:\", ducument_before_space)\n",
    "      # print(\"Cau da xu ly:\", document)\n",
    "      space_count += 1\n",
    "    # del document1\n",
    "\n",
    "    # xóa html code\n",
    "    ducument_before_html = document\n",
    "    document = remove_html(document)\n",
    "    if ducument_before_html != document:\n",
    "      # print(\"Cau chua xu ly:\", ducument_before_html)\n",
    "      # print(\"Cau da xu ly:\", document)\n",
    "      link_count += 1\n",
    "    # del document1\n",
    "\n",
    "    # chuẩn hóa unicode\n",
    "    ducument_before_unicode = document\n",
    "    document = convert_unicode(document)\n",
    "    if ducument_before_unicode != document:\n",
    "      unicode_count += 1\n",
    "    # del document1\n",
    "\n",
    "    # xóa các ký tự không cần thiết\n",
    "    ducument_before_redundant = document\n",
    "    document = remove_dub_char(document)\n",
    "    if ducument_before_redundant != document:\n",
    "      # print(\"Cau chua xu ly:\", ducument_before_redundant)\n",
    "      # print(\"Cau da xu ly:\", document)\n",
    "      noneed_count += 1\n",
    "    # del document1\n",
    "\n",
    "\n",
    "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
    "    ducument_before_dau = document\n",
    "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
    "    if ducument_before_dau != document:\n",
    "      print(\"Cau chua xu ly:\", ducument_before_dau)\n",
    "      print(\"Cau da xu ly:\", document)\n",
    "      dau_count += 1\n",
    "    # del document1\n",
    "\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1688454874842,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "EPBjgWJtts3t",
    "outputId": "a5e7681a-d439-40f8-c137-8a7e42ffb66d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hiếu thuận'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocess('Hiếu THUẬN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1688454875343,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "gVIIBGL6uUVW",
    "outputId": "89a083c1-0fcf-41d1-94ec-efa61dd7c503"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hiếu thuận' == 'Hiếu THUẬN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYwwNVpAd2PE"
   },
   "source": [
    "#DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9Hs8uurt0vD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datafolderPath = \"/content/CS117ToxicComment/Dataset.xlsx\"\n",
    "\n",
    "#dev = pd.read_csv(datafolderPath+\"/dev.csv\")\n",
    "#test = pd.read_csv(datafolderPath+\"/test.csv\")\n",
    "#train = pd.read_csv(datafolderPath+\"/train.csv\")\n",
    "\n",
    "#dataset = pd.concat([train, dev, test], ignore_index=True)\n",
    "dataset = pd.read_excel(datafolderPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wqr8Je3NVowX"
   },
   "outputs": [],
   "source": [
    "#VLSP\n",
    "#import pandas as pd\n",
    "#train = pd.read_csv(\"/content/drive/MyDrive/PROJECT/HATE SPEECH SENTIMENT/DataFromVLSP/Copy of trainbert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zoT9UyarVxA-"
   },
   "outputs": [],
   "source": [
    "#VLSP\n",
    "#kq = train\n",
    "kq = dataset\n",
    "x_train = kq.iloc[:, 1] #0 - 1\n",
    "x_train = pd.DataFrame(x_train)\n",
    "x_val = kq.iloc[:, 2] #1-2 #values\n",
    "x_val = pd.DataFrame(x_val)\n",
    "x_val_full = kq.iloc[:, 2:] #1-2 #values\n",
    "x_val_full = pd.DataFrame(x_val_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3045,
     "status": "ok",
     "timestamp": 1688454896965,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "Adfs_BbveIDZ",
    "outputId": "829970b7-89c9-4b81-d4a7-adb0248de8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cau chua xu ly: xạo choá quá anh\n",
      "Cau da xu ly: xạo chóa quá anh\n",
      "Cau chua xu ly: cerberus lương cao nhất việt nam, nhưng so với sin, indo thì bằng 1/3 =))\n",
      "Cau da xu ly: cerberus lương cao nhất việt nam, nhưng so với sin, indo thì bằng 13 =))\n",
      "Cau chua xu ly: :v nói vậy thì thà xoá luôn khẩu vandal đi =))\n",
      "Cau da xu ly: :v nói vậy thì thà xóa luôn khẩu vandal đi =))\n",
      "Cau chua xu ly: vn mình view còn ko = 1/10 ngta\n",
      "Cau da xu ly: vn mình view còn ko = 110 ngta\n",
      "Cau chua xu ly: bảnh đêý\n",
      "Cau da xu ly: bảnh đếy\n",
      "Cau chua xu ly: đêm...nhức nhối nỗi niềm riêng khôn tảtiếng côn trùng réo rắt quyện tiếng mưakhép lại nhé những tủi hờn uất hậncuộc sống bất công biết mấy cho vừai định nghĩa thế gian này muôn mặt\n",
      "Cau da xu ly: đêm...nhức nhối nỗi niềm riêng khôn tảtiếng côn trùng réo rắt quyện tiếng mưakhép lại nhé những tủi hờn uất hậncuộc sống bất công biết mấy cho vưài định nghĩa thế gian này muôn mặt\n",
      "Cau chua xu ly: 2019 aog. 2020 gcs. 2021 rpl. 2022 1/2 aog rồi\n",
      "Cau da xu ly: 2019 aog. 2020 gcs. 2021 rpl. 2022 12 aog rồi\n",
      "Cau chua xu ly: ám ảnh giọng chói tai cuả lyly quá\n",
      "Cau da xu ly: ám ảnh giọng chói tai của lyly quá\n",
      "Cau chua xu ly: các cháu xem giải mà lúc nào cũng cmt vô văn hoá thất học v\n",
      "Cau da xu ly: các cháu xem giải mà lúc nào cũng cmt vô văn hóa thất học v\n",
      "Cau chua xu ly: có giỏi qua mà bắn sao lai chửi ngừoi ta\n",
      "Cau da xu ly: có giỏi qua mà bắn sao lai chửi ngưòi ta\n",
      "Cau chua xu ly: con bọ phải tuỳ lúc mới nhảy được vào. ko nộp mạng ak.\n",
      "Cau da xu ly: con bọ phải tùy lúc mới nhảy được vào. ko nộp mạng ak.\n",
      "Cau chua xu ly: đấu trường danh vọng kết thúc lâu rồi mà ba ://\n",
      "Cau da xu ly: đấu trường danh vọng kết thúc lâu rồi mà ba :\n",
      "Cau chua xu ly: hay qúa bro\n",
      "Cau da xu ly: hay quá bro\n",
      "Cau chua xu ly: hay toá\n",
      "Cau da xu ly: hay tóa\n",
      "Cau chua xu ly: hq xoá game luôn đi đừng có mà tham gia trận nào nx\n",
      "Cau da xu ly: hq xóa game luôn đi đừng có mà tham gia trận nào nx\n",
      "Cau chua xu ly: khoá thì chưa bị nhưng nhớ các ac rules gần 200 củ , game xoá vĩnh viễn\n",
      "Cau da xu ly: khóa thì chưa bị nhưng nhớ các ac rules gần 200 củ , game xóa vĩnh viễn\n",
      "Cau chua xu ly: mâý thg ngáo rồi à gow tan rã rồi\n",
      "Cau da xu ly: mấy thg ngáo rồi à gow tan rã rồi\n",
      "Cau chua xu ly: moá đánh mà bị bắt bài\n",
      "Cau da xu ly: móa đánh mà bị bắt bài\n",
      "Cau chua xu ly: ngoài kia sấm đánh ầm ầm/ trần bình đánh giải tưởng nhầm lai bâng\n",
      "Cau da xu ly: ngoài kia sấm đánh ầm ầm trần bình đánh giải tưởng nhầm lai bâng\n",
      "Cau chua xu ly: ngx a khoẻ hơn\n",
      "Cau da xu ly: ngx a khỏe hơn\n",
      "Cau chua xu ly: thái nhọ gê qúa\n",
      "Cau da xu ly: thái nhọ gê quá\n",
      "Cau chua xu ly: toàn bọn vô hăn hoá chat\n",
      "Cau da xu ly: toàn bọn vô hăn hóa chat\n",
      "Cau chua xu ly: @hisu tv bữa tôi đặt 1/10 con asimov bú mạnh ông ạ\n",
      "Cau da xu ly: @hisu tv bữa tôi đặt 110 con asimov bú mạnh ông ạ\n",
      "Cau chua xu ly: 1/0/3 đúng 4 agi luôn :v\n",
      "Cau da xu ly: 103 đúng 4 agi luôn :v\n",
      "Cau chua xu ly: 30/4 à?\n",
      "Cau da xu ly: 304 à?\n",
      "Cau chua xu ly: 30/4 mode\n",
      "Cau da xu ly: 304 mode\n",
      "Cau chua xu ly: à jbo xoá danh bạ chặn số r\n",
      "Cau da xu ly: à jbo xóa danh bạ chặn số r\n",
      "Cau chua xu ly: bắt đâuù rồi\n",
      "Cau da xu ly: bắt đâùu rồi\n",
      "Cau chua xu ly: batman hoá chó r\n",
      "Cau da xu ly: batman hóa chó r\n",
      "Cau chua xu ly: carry 500tr 14/0 awesome\n",
      "Cau da xu ly: carry 500tr 140 awesome\n",
      "Cau chua xu ly: fan bòiz say bia\n",
      "Cau da xu ly: fan boìz say bia\n",
      "Cau chua xu ly: lên xe awesome/\n",
      "Cau da xu ly: lên xe awesome\n",
      "Cau chua xu ly: moá ní\n",
      "Cau da xu ly: móa ní\n",
      "Cau chua xu ly: ngài đánh 2 trận slark hoà vốn ah ae?\n",
      "Cau da xu ly: ngài đánh 2 trận slark hòa vốn ah ae?\n",
      "Cau chua xu ly: omi nó khoẻ vcl\n",
      "Cau da xu ly: omi nó khỏe vcl\n",
      "Cau chua xu ly: tự nhiên anh wind khoẻ luôn :))\n",
      "Cau da xu ly: tự nhiên anh wind khỏe luôn :))\n",
      "Cau chua xu ly: ulti lấy music doạ nó chứ\n",
      "Cau da xu ly: ulti lấy music dọa nó chứ\n",
      "Cau chua xu ly: update w/l @quocthaiak\n",
      "Cau da xu ly: update wl @quocthaiak\n",
      "Cau chua xu ly: vcl học chơi thì xem demo 1 thằng bắn vị trí giống mình chứ đi xem giải thì học được moẹ gì awesome\n",
      "Cau da xu ly: vcl học chơi thì xem demo 1 thằng bắn vị trí giống mình chứ đi xem giải thì học được mọe gì awesome\n",
      "Cau chua xu ly: vcl tự huỷ kiểu gì thế :))\n",
      "Cau da xu ly: vcl tự hủy kiểu gì thế :))\n",
      "Cau chua xu ly: wỷ 3/3/3 awesome\n",
      "Cau da xu ly: wỷ 333 awesome\n",
      "Cau chua xu ly: bộ 3 huỷ diệt :))))))) bá đần\n",
      "Cau da xu ly: bộ 3 hủy diệt :))))))) bá đần\n",
      "Cau chua xu ly: thắng đc ván gỡ hoà mà cứ như chết đi sống lại gáy ghê thật :))\n",
      "Cau da xu ly: thắng đc ván gỡ hòa mà cứ như chết đi sống lại gáy ghê thật :))\n",
      "Cau chua xu ly: rồif sao team m chịu được\n",
      "Cau da xu ly: rôìf sao team m chịu được\n",
      "Cau chua xu ly: po5 đi moà\n",
      "Cau da xu ly: po5 đi mòa\n",
      "Cau chua xu ly: đồ hoạ msi độc lạ quá\n",
      "Cau da xu ly: đồ họa msi độc lạ quá\n",
      "Cau chua xu ly: địt mẹ chúng mày bán độ lộ lắm mấy con chó này /)))\n",
      "Cau da xu ly: địt mẹ chúng mày bán độ lộ lắm mấy con chó này )))\n",
      "Cau chua xu ly: g2 đánh con cặc gì vậy ???/\n",
      "Cau da xu ly: g2 đánh con cặc gì vậy ???\n",
      "Cau chua xu ly: ??? bắt đc ad rồi mà thua đc.? atroc nó khoẻ hay sao ấy kyen nữa khoẻ\n",
      "Cau da xu ly: ??? bắt đc ad rồi mà thua đc.? atroc nó khỏe hay sao ấy kyen nữa khỏe\n",
      "Cau chua xu ly: ối dồi ôi ai độ e tôi mà 1/6 thế kia\n",
      "Cau da xu ly: ối dồi ôi ai độ e tôi mà 16 thế kia\n",
      "Cau chua xu ly: rồng hoá kỹ khác biệt\n",
      "Cau da xu ly: rồng hóa kỹ khác biệt\n",
      "Cau chua xu ly: ngài pyos.. à nhầm mark đoán gam, blg với đội mạnh nhất bảng còn lại sẽ vào; ngài đúng 2/3 rồi :((\n",
      "Cau da xu ly: ngài pyos.. à nhầm mark đoán gam, blg với đội mạnh nhất bảng còn lại sẽ vào; ngài đúng 23 rồi :((\n",
      "Cau chua xu ly: 7 cỏ bỏ bóng đá người, vận động viên đấu vật chuyên nghiệp, hung thần đập điện thoại fan nhí, kẻ huỷ diệt giải lạc đà hạng 66.\n",
      "Cau da xu ly: 7 cỏ bỏ bóng đá người, vận động viên đấu vật chuyên nghiệp, hung thần đập điện thoại fan nhí, kẻ hủy diệt giải lạc đà hạng 66.\n",
      "Cau chua xu ly: tiến hoá 4 chiêu =)))\n",
      "Cau da xu ly: tiến hóa 4 chiêu =)))\n",
      "Cau chua xu ly: 0/0/0 mà có tiền thưởng\n",
      "Cau da xu ly: 000 mà có tiền thưởng\n"
     ]
    }
   ],
   "source": [
    "link_count = 0\n",
    "unicode_count = 0\n",
    "dau_count = 0\n",
    "lower_count = 0\n",
    "noneed_count = 0\n",
    "space_count = 0\n",
    "#train['free_text'] = train['free_text'].apply(lambda x:text_preprocess(x))\n",
    "dataset['Comment'] = dataset['Comment'].apply(lambda x:text_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1688454904282,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "VqgtqjiN85bH",
    "outputId": "288de53d-4471-45a6-8d72-88aed6288222"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'đíu ra dao'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Comment'][6976]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1688454904767,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "yWmwXYNGt_44",
    "outputId": "65850c33-3304-4ce1-e32b-dd12333782e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "55\n",
      "2171\n",
      "458\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "print(link_count)\n",
    "print(unicode_count)\n",
    "print(dau_count)\n",
    "print(lower_count)\n",
    "print(noneed_count)\n",
    "print(space_count)\n",
    "# print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1688454906520,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "3BgffCFZe8ka",
    "outputId": "da8623b9-e416-4e83-8a60-57fd3396d7ff"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'This is a smiley face 😂'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = u'This is a smiley face 😂'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6831,
     "status": "ok",
     "timestamp": 1688454245640,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "cFnYCXtDM7tJ",
    "outputId": "fc922e95-a404-4480-f10a-3201cab8440f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacymoji\n",
      "  Downloading spacymoji-3.1.0-py2.py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from spacymoji) (3.5.3)\n",
      "Collecting emoji<3.0,>=2.0 (from spacymoji)\n",
      "  Downloading emoji-2.6.0.tar.gz (356 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/356.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m348.2/356.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m356.6/356.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.22.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->spacymoji) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacymoji) (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacymoji) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacymoji) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacymoji) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacymoji) (2.1.3)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-2.6.0-py2.py3-none-any.whl size=351311 sha256=fcd390083e63411f45b37d36688d5fb0782564c3e65218650dd3baf252b6b0bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/0b/64/114bc939d0083621aa41521e21be246c888260b8aa21e6c1ad\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, spacymoji\n",
      "Successfully installed emoji-2.6.0 spacymoji-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacymoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27732,
     "status": "ok",
     "timestamp": 1688454273367,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "kPj5H-HXN5ht",
    "outputId": "056478d9-50fc-479f-fd78-d937c9959c40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-04 07:04:13.817482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-04 07:04:15.861318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1267,
     "status": "ok",
     "timestamp": 1688454912354,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "s7Owkp-ZNMq6",
    "outputId": "6d5672fb-7115-4a97-e9af-3a1eeac5b6ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sale', 'hay', 'sela', ':))']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacymoji import Emoji\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "emoji = Emoji(nlp)\n",
    "nlp.add_pipe('emoji', first=True)\n",
    "\n",
    "doc = nlp(\"sale hay sela :))\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1688454914073,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "y8AljlN8R4F6",
    "outputId": "59c3ea10-72a5-445a-ea2c-30b6f1e109a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sale', 'hay', 'sela', ':)))', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'thật', 'vãi', 'lồn', 'ha']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"sale hay sela :)))))))))))))) thật vãi lồn ha\")\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1688454915285,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "HpooafddW_s0",
    "outputId": "48d8ec46-763b-4487-8e4b-d14ff61ec032"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[6]._.is_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RTI-eg9BjxN"
   },
   "outputs": [],
   "source": [
    "#!pip install python-rdrsegmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvqmOXGZPd2t"
   },
   "outputs": [],
   "source": [
    "word_segmented_text = rdrsegmenter.tokenize(\"sale hay sela :D thật vãi lồn ha 😂😂😂😂😂\")\n",
    "word_segmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4866,
     "status": "ok",
     "timestamp": 1688454924267,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "aK7GdrRkT3yl",
    "outputId": "9144480f-f776-431e-daaa-8619b007d649"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in /usr/local/lib/python3.10/dist-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1688454924270,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "LdLqiFAjTqN-",
    "outputId": "8ca8723f-f7c7-4b0d-886e-904b6a1310ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-77-eb011a9810ad>:2: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "import demoji\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1688454924273,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "CsncdHBOUFSP",
    "outputId": "b6036115-535c-41df-9b15-79294d7ff611"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'sale hay sela :D thật vãi lồn ha 😂😂😂😂😂'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"sale hay sela :D thật vãi lồn ha 😂😂😂😂😂\"\n",
    "re.sub(r'([A-Z])\\1+', lambda m: m.group(1), text, flags = re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1688454924274,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "Z4QrjEo4U86u",
    "outputId": "ac9d14b0-4947-434c-b8e5-c735f52658af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'😂': 'face with tears of joy'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demoji.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1688454924274,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "H4T9ipq7MFNA",
    "outputId": "d31d6940-5153-49b9-fd82-b25b5cd05f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cau chua xu ly: tiên tri nói quằn qùe hay vcl :)) )\n",
      "Cau da xu ly: tiên tri nói quằn què hay vcl :)) )\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "text = '       tiên tri nói         quằn qùeeeee hay vcllll :))       ) '\n",
    "phase1_text = text_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1688454924274,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "63imBtbYMcoR",
    "outputId": "f90012b6-da64-435f-96b9-aa2124184e87"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'tiên tri nói quằn què hay vcl :)) )'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phase1_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP_RDw3m1Xke"
   },
   "source": [
    "# Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8035,
     "status": "ok",
     "timestamp": 1688453709769,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "fx7gu5UHL7oJ",
    "outputId": "398d99a1-846c-4115-a2bd-7a180524dd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vncorenlp\n",
      "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4)\n",
      "Building wheels for collected packages: vncorenlp\n",
      "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645933 sha256=ff45ffa2372ffc5d4e53b43f0e99173edc0931a2e0169487ad1a281bd9faba00\n",
      "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
      "Successfully built vncorenlp\n",
      "Installing collected packages: vncorenlp\n",
      "Successfully installed vncorenlp-1.0.3\n",
      "--2023-07-04 06:55:07--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 27412575 (26M) [application/octet-stream]\n",
      "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
      "\n",
      "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M   168MB/s    in 0.2s    \n",
      "\n",
      "2023-07-04 06:55:08 (168 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
      "\n",
      "--2023-07-04 06:55:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 526544 (514K) [application/octet-stream]\n",
      "Saving to: ‘vi-vocab’\n",
      "\n",
      "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2023-07-04 06:55:08 (10.1 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
      "\n",
      "--2023-07-04 06:55:08--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 128508 (125K) [text/plain]\n",
      "Saving to: ‘wordsegmenter.rdr’\n",
      "\n",
      "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-07-04 06:55:08 (5.54 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word segmenter\n",
    "!pip3 install vncorenlp\n",
    "\n",
    "# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter)\n",
    "!mkdir -p vncorenlp/models/wordsegmenter\n",
    "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
    "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
    "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
    "!mv VnCoreNLP-1.1.1.jar vncorenlp/\n",
    "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
    "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6274,
     "status": "ok",
     "timestamp": 1688453720257,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "R_0dQ9J7L8WJ",
    "outputId": "fdac1c05-a77f-435d-ecca-c51a96ab7aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[['hello', ',', 'hoà_thuận']]\n"
     ]
    }
   ],
   "source": [
    "from vncorenlp import VnCoreNLP\n",
    "rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "\n",
    "text = \"hello, hòa thuận\"\n",
    "\n",
    "word_segmented_text = rdrsegmenter.tokenize(text)\n",
    "print(type(word_segmented_text))\n",
    "print(word_segmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Y9c4CH6P-JQ"
   },
   "source": [
    "##Teencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYirdMTpWKB_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXnHXdThfxoD"
   },
   "outputs": [],
   "source": [
    "teencode_list = teencode_df['teencode'].to_list()\n",
    "map_list = teencode_df['map'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFDlRrGa1yYx"
   },
   "outputs": [],
   "source": [
    "def searchTeencode(word):\n",
    "  try:\n",
    "    global teencode_count\n",
    "    index = teencode_list.index(word)\n",
    "    map_word = map_list[index]\n",
    "    teencode_count += 1\n",
    "    return map_word\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j_LvroNVD6b"
   },
   "source": [
    "##Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gowXKnnQgYk"
   },
   "outputs": [],
   "source": [
    "x=2\n",
    "if (x==0)!= (x==1):\n",
    "  print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lrxeq8rCVMdm"
   },
   "outputs": [],
   "source": [
    "# stopword = ['bị', 'bởi', 'cả', 'các', 'cái', 'cần', 'càng', 'chỉ', 'chiếc', 'cho', 'chứ', 'chưa', 'chuyện', 'có', 'có_thể', 'cứ',\n",
    "#             'của', 'cùng', 'cũng', 'đã', 'đang', 'đây', 'để', 'đến_nỗi', 'đều', 'điều', 'do', 'đó', 'được', 'dưới', 'gì', 'khi',\n",
    "#             'không', 'là', 'lại', 'lên', 'lúc', 'mà', 'mỗi', 'một_cách', 'này', 'nên', 'nếu', 'ngay', 'nhiều', 'như', 'nhưng',\n",
    "#             'những', 'nơi', 'nữa', 'phải', 'qua', 'ra', 'rằng', 'rất', 'rồi', 'sau', 'sẽ', 'so', 'sự', 'tại', 'theo', 'thì', 'trên',\n",
    "#             'trước', 'từ', 'từng', 'và', 'vẫn', 'vào', 'vậy', 'vì', 'việc', 'với', 'vừa']\n",
    "\n",
    "STOPWORDS = '/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/vietnamese-stopwords-dash.txt'\n",
    "\n",
    "# features extraction\n",
    "with open(STOPWORDS, \"r\") as ins:\n",
    "    stopword = []\n",
    "    for line in ins:\n",
    "        stopword.append(line.strip('\\n'))\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    global stopword_count\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "        if word in stopword:\n",
    "            stopword_count += 1\n",
    "    return words\n",
    "\n",
    "# with open(STOPWORDS, \"r\") as ins:\n",
    "#     stop_words = []\n",
    "#     for line in ins:\n",
    "#         stop_words.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zuc_i7KZfGFj"
   },
   "source": [
    "##Combine teencode + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjZWbRZFfFfx"
   },
   "outputs": [],
   "source": [
    "#DE STOPWORD_TEENCODE BY VNCORENLP\n",
    "stopword_count = 0\n",
    "teencode_count =0\n",
    "def stopWords_Teencode(sentence):\n",
    "  lenn = 0\n",
    "  sentence = str(sentence)\n",
    "  #Tokenize\n",
    "  nestList_tokens = rdrsegmenter.tokenize(sentence)\n",
    "  for tokens_idx, text_tokens in enumerate(nestList_tokens):\n",
    "    #Teencode\n",
    "    lenn += len(text_tokens)\n",
    "    for idx, word in enumerate(text_tokens):\n",
    "      deteencoded = searchTeencode(word)\n",
    "      if(deteencoded != None):\n",
    "        text_tokens[idx] = deteencoded\n",
    "    nestList_tokens[tokens_idx] = text_tokens\n",
    "\n",
    "  # deteencode_sentence = (\" \").join(nestList_tokens)\n",
    "\n",
    "  #Stopwords\n",
    "  flat_list = [item for sublist in nestList_tokens for item in sublist]\n",
    "\n",
    "  List_without_sw = remove_stopwords(flat_list)\n",
    "\n",
    "  #Detokenize\n",
    "  detokens = MosesDetokenizer().detokenize(List_without_sw, return_str=True)\n",
    "\n",
    "  return detokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ur6RzLdjM92E"
   },
   "outputs": [],
   "source": [
    "sent = 'tiên tri nói quằn què hay clm :)) ) '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1688453779892,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "KOTuF6eLMu9a",
    "outputId": "efd0d939-af94-435f-d77e-b7f6a3b8f317"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'tiên_tri quằn què cái lồn má:)))'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords_Teencode(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PU3MAi7kgZRd"
   },
   "outputs": [],
   "source": [
    "stopword_count = 0\n",
    "teencode_count =0\n",
    "dataset['Comment'] = dataset['Comment'].apply(lambda x:stopWords_Teencode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1688453826755,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "YNcqaGM4O7y4",
    "outputId": "fcb03595-b9d4-4a3c-ca35-d1d15f60545e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17480, 2793)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_count, teencode_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFm-1YpqU8ez"
   },
   "source": [
    "#WORD TOKENIZE BY UNDERTHESEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18097,
     "status": "ok",
     "timestamp": 1688454324213,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "cqrwpmOiVtNe",
    "outputId": "f5163eba-0dc8-47c2-dbc2-cf25dce98fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting underthesea\n",
      "  Downloading underthesea-6.3.0-py3-none-any.whl (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.3)\n",
      "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
      "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.27.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0)\n",
      "Collecting underthesea-core==1.0.4 (from underthesea)\n",
      "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2022.10.31)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.1.0)\n",
      "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
      "Successfully installed python-crfsuite-0.9.9 underthesea-6.3.0 underthesea-core-1.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install underthesea\n",
    "\n",
    "from underthesea import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr2T8sEbVqHN"
   },
   "source": [
    "##Teencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERALhVzTVyy6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)\n",
    "teencode_list = teencode_df['teencode'].to_list()\n",
    "map_list = teencode_df['map'].to_list()\n",
    "def searchTeencode(word):\n",
    "  try:\n",
    "    global teencode_count\n",
    "    index = teencode_list.index(word)\n",
    "    map_word = map_list[index]\n",
    "    teencode_count += 1\n",
    "    return map_word\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihpm8p8FV1WC"
   },
   "source": [
    "##Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMjO3xjuVywx"
   },
   "outputs": [],
   "source": [
    "# stopword = ['bị', 'bởi', 'cả', 'các', 'cái', 'cần', 'càng', 'chỉ', 'chiếc', 'cho', 'chứ', 'chưa', 'chuyện', 'có', 'có_thể', 'cứ',\n",
    "#             'của', 'cùng', 'cũng', 'đã', 'đang', 'đây', 'để', 'đến_nỗi', 'đều', 'điều', 'do', 'đó', 'được', 'dưới', 'gì', 'khi',\n",
    "#             'không', 'là', 'lại', 'lên', 'lúc', 'mà', 'mỗi', 'một_cách', 'này', 'nên', 'nếu', 'ngay', 'nhiều', 'như', 'nhưng',\n",
    "#             'những', 'nơi', 'nữa', 'phải', 'qua', 'ra', 'rằng', 'rất', 'rồi', 'sau', 'sẽ', 'so', 'sự', 'tại', 'theo', 'thì', 'trên',\n",
    "#             'trước', 'từ', 'từng', 'và', 'vẫn', 'vào', 'vậy', 'vì', 'việc', 'với', 'vừa']\n",
    "\n",
    "STOPWORDS = '/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/vietnamese-stopwords-dash.txt'\n",
    "\n",
    "# features extraction\n",
    "with open(STOPWORDS, \"r\") as ins:\n",
    "    stopword = []\n",
    "    for line in ins:\n",
    "        stopword.append(line.strip('\\n'))\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    global stopword_count\n",
    "    words = []\n",
    "    for word in line.strip().split():\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "        if word in stopword:\n",
    "            stopword_count += 1\n",
    "    return ' '.join(words)\n",
    "\n",
    "# with open(STOPWORDS, \"r\") as ins:\n",
    "#     stop_words = []\n",
    "#     for line in ins:\n",
    "#         stop_words.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL3znVvBV6dX"
   },
   "source": [
    "##COMBINE TEEEN AND STOPWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9RAG6a-OVytU"
   },
   "outputs": [],
   "source": [
    "stopword_count = 0\n",
    "teencode_count =0\n",
    "def stopWords_Teencode(sentence):\n",
    "  lenn = 0\n",
    "  sentence = str(sentence)\n",
    "  #Tokenize\n",
    "  List_tokens = word_tokenize(sentence,format='text')\n",
    "  List_tokens = word_tokenize(List_tokens)\n",
    "\n",
    "  #Teencode\n",
    "  for tokens_idx, text_tokens in enumerate(List_tokens):\n",
    "    deteencoded = searchTeencode(text_tokens)\n",
    "    if (deteencoded != None):\n",
    "        List_tokens[tokens_idx] = deteencoded\n",
    "\n",
    "  deteencode_sentence = (\" \").join(List_tokens)\n",
    "\n",
    "  #Stopwords\n",
    "  tokens_without_sw = remove_stopwords(deteencode_sentence)\n",
    "\n",
    "  return tokens_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIJwn4gOXEAy"
   },
   "outputs": [],
   "source": [
    "sent = 'hoàng gia phú đẹp trai vl, đéo thể tin được nhưng mà còn ngu nữa, haha. Con cặc, đjt mẹ, thg đàn bà bê đê chúa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 445,
     "status": "ok",
     "timestamp": 1688454367962,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "zp8n5gJMXq8g",
    "outputId": "e7ac01ab-93f1-481f-e946-14c1f8b699fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hoàng gia',\n",
       " 'phú',\n",
       " 'đẹp trai',\n",
       " 'vl',\n",
       " ',',\n",
       " 'đéo thể',\n",
       " 'tin',\n",
       " 'được',\n",
       " 'nhưng',\n",
       " 'mà',\n",
       " 'còn',\n",
       " 'ngu',\n",
       " 'nữa',\n",
       " ',',\n",
       " 'haha',\n",
       " '. Con',\n",
       " 'cặc',\n",
       " ',',\n",
       " 'đjt',\n",
       " 'mẹ',\n",
       " ',',\n",
       " 'thg',\n",
       " 'đàn bà',\n",
       " 'bê',\n",
       " 'đê',\n",
       " 'chúa']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6wPEg7aX3Tn"
   },
   "outputs": [],
   "source": [
    "#rdrsegmenter.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1688454403224,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "nju9p84hXG1J",
    "outputId": "7683748a-1101-4243-8d54-1d6901b20e12"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hoàng_gia phú đẹp_trai vãi lồn , đéo_thể ngu , haha . _Con cặc , đjt mẹ , thg đàn_bà bê đê chúa'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords_Teencode(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StrQPCecWClh"
   },
   "source": [
    "##DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28GJXoaHVyqt"
   },
   "outputs": [],
   "source": [
    "stopword_count = 0\n",
    "teencode_count =0\n",
    "dataset['Comment'] = dataset['Comment'].apply(lambda x:stopWords_Teencode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1688454422086,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "Pe7X1plSVynU",
    "outputId": "b88bc7ca-5fef-44ac-d7f5-27bd5651ee4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2854, 18529)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teencode_count, stopword_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_RNtGiqhCTJ"
   },
   "outputs": [],
   "source": [
    "test1 = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbnhXpLLX55E"
   },
   "source": [
    "# Word Tokenize NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57302,
     "status": "ok",
     "timestamp": 1688454986929,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "kiOQ9GNjX55Q",
    "outputId": "4f3ae051-3476-406b-cd21-d1472db7740a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!sudo pip install nltk\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1688454994289,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "aPXj9xBBX55R",
    "outputId": "22ade6cd-926b-41bf-9b57-22ef80ff1fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hoàng', 'gia', 'phú', 'đẹp', 'trai', 'vl', ',', 'đéo', 'thể', 'tin', 'được', 'nhưng', 'mà', 'còn', 'ngu', 'nữa', ',', 'haha', '.', 'Con', 'cặc', ',', 'đjt', 'mẹ', ',', 'thg', 'đàn', 'bà', 'bê', 'đê', 'chúa']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "sent = 'hoàng gia phú đẹp trai vl, đéo thể tin được nhưng mà còn ngu nữa, haha. Con cặc, đjt mẹ, thg đàn bà bê đê chúa'\n",
    "print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6XcMZxuX55S"
   },
   "source": [
    "##Teencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UIJ6ZG7GX55V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "teencode_df = pd.read_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/teencode.txt',names=['teencode','map'],sep='\\t',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHSWaanAX55V"
   },
   "outputs": [],
   "source": [
    "teencode_list = teencode_df['teencode'].to_list()\n",
    "map_list = teencode_df['map'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-0TgZqOX55W"
   },
   "outputs": [],
   "source": [
    "def searchTeencode(word):\n",
    "  try:\n",
    "    global teencode_count\n",
    "    index = teencode_list.index(word)\n",
    "    map_word = map_list[index]\n",
    "    teencode_count += 1\n",
    "    return map_word\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lysu4TNoX55W"
   },
   "source": [
    "##Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLlFC4WkX55W"
   },
   "outputs": [],
   "source": [
    "# stopword = ['bị', 'bởi', 'cả', 'các', 'cái', 'cần', 'càng', 'chỉ', 'chiếc', 'cho', 'chứ', 'chưa', 'chuyện', 'có', 'có_thể', 'cứ',\n",
    "#             'của', 'cùng', 'cũng', 'đã', 'đang', 'đây', 'để', 'đến_nỗi', 'đều', 'điều', 'do', 'đó', 'được', 'dưới', 'gì', 'khi',\n",
    "#             'không', 'là', 'lại', 'lên', 'lúc', 'mà', 'mỗi', 'một_cách', 'này', 'nên', 'nếu', 'ngay', 'nhiều', 'như', 'nhưng',\n",
    "#             'những', 'nơi', 'nữa', 'phải', 'qua', 'ra', 'rằng', 'rất', 'rồi', 'sau', 'sẽ', 'so', 'sự', 'tại', 'theo', 'thì', 'trên',\n",
    "#             'trước', 'từ', 'từng', 'và', 'vẫn', 'vào', 'vậy', 'vì', 'việc', 'với', 'vừa']\n",
    "\n",
    "STOPWORDS = '/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/vietnamese_stopwords.txt'\n",
    "\n",
    "# features extraction\n",
    "with open(STOPWORDS, \"r\") as ins:\n",
    "    stopword = []\n",
    "    for line in ins:\n",
    "        stopword.append(line.strip('\\n'))\n",
    "\n",
    "def remove_stopwords(line):\n",
    "    global stopword_count\n",
    "    words = []\n",
    "    for word in line:\n",
    "        if word not in stopword:\n",
    "            words.append(word)\n",
    "        if word in stopword:\n",
    "            stopword_count += 1\n",
    "    return words\n",
    "\n",
    "# with open(STOPWORDS, \"r\") as ins:\n",
    "#     stop_words = []\n",
    "#     for line in ins:\n",
    "#         stop_words.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJNM3QlPX55X"
   },
   "source": [
    "##Combine teencode + stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9QzShKHfQQn"
   },
   "outputs": [],
   "source": [
    "stopword_count = 0\n",
    "teencode_count =0\n",
    "def stopWords_Teencode(sentence):\n",
    "  lenn = 0\n",
    "  sentence = str(sentence)\n",
    "  #Tokenize\n",
    "  List_tokens = word_tokenize(sentence)\n",
    "\n",
    "  #Teencode\n",
    "  for tokens_idx, text_tokens in enumerate(List_tokens):\n",
    "    deteencoded = searchTeencode(text_tokens)\n",
    "    if (deteencoded != None):\n",
    "        List_tokens[tokens_idx] = deteencoded\n",
    "\n",
    "  #Stopwords\n",
    "  list_without_sw = remove_stopwords(List_tokens)\n",
    "\n",
    "  return TreebankWordDetokenizer().detokenize(list_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3Da10OvX55X"
   },
   "outputs": [],
   "source": [
    "sent = 'hoàng gia phú đẹp trai vl, đéo thể tin được nhưng mà còn ngu nữa, haha. Con cặc, đjt mẹ, thg đàn bà bê đê chúa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1688455048715,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "uRMH7iWoX55Y",
    "outputId": "bad0ec24-0241-42db-da2f-1e544a52a09b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hoàng gia phú đẹp trai vãi lồn, đéo thể ngu, haha . Con cặc, đjt mẹ, thg đàn bê đê chúa'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords_Teencode(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uaFYbQHMHnrB"
   },
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNyvrRjUX55Y"
   },
   "outputs": [],
   "source": [
    "stopword_count = 0\n",
    "teencode_count =0\n",
    "dataset['Comment'] = dataset['Comment'].apply(lambda x:stopWords_Teencode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1688455084263,
     "user": {
      "displayName": "Anh Phạm Thị Trâm",
      "userId": "15461037140885299192"
     },
     "user_tz": -420
    },
    "id": "CkEe5boKX55Z",
    "outputId": "7d7cae3c-a2f3-41c4-c0be-07e2c0ef0faf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18569, 2797)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_count, teencode_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VF__6Sobo7tz"
   },
   "source": [
    "# EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbfP4n2rhP37"
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('/content/drive/MyDrive/MachineLearning/FinalTerm/ProcessingData/NLTK(Phase2).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kaLFKBDg69D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1VIF50b-oczf6GVQB-wSTYD48Mb2cSTFs",
     "timestamp": 1688406331213
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "vifree-txt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
