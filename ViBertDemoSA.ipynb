{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (0.31.1)\n",
      "Requirement already satisfied: packaging in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.0 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [propcache]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.3.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [propcache]\n",
      "\u001b[2K    Uninstalling fsspec-2025.3.2:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [propcache]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.3.2━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/15\u001b[0m [propcache]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]ess]]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 fsspec-2025.3.0 multidict-6.4.3 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 898k/898k [00:00<00:00, 3.30MB/s]\n",
      "Downloading data: 100%|██████████| 22.9k/22.9k [00:00<00:00, 16.4MB/s]\n",
      "Downloading data: 100%|██████████| 22.9k/22.9k [00:00<00:00, 11.7MB/s]\n",
      "Downloading data: 100%|██████████| 119k/119k [00:00<00:00, 30.5MB/s]\n",
      "Downloading data: 100%|██████████| 3.17k/3.17k [00:00<00:00, 2.40MB/s]\n",
      "Downloading data: 100%|██████████| 3.17k/3.17k [00:00<00:00, 1.40MB/s]\n",
      "Downloading data: 100%|██████████| 248k/248k [00:00<00:00, 26.3MB/s]\n",
      "Downloading data: 100%|██████████| 6.33k/6.33k [00:00<00:00, 4.45MB/s]\n",
      "Downloading data: 100%|██████████| 6.33k/6.33k [00:00<00:00, 4.55MB/s]\n",
      "Generating train split: 11426 examples [00:00, 48932.72 examples/s]\n",
      "Generating validation split: 1583 examples [00:00, 53961.49 examples/s]\n",
      "Generating test split: 3166 examples [00:00, 51801.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"uit-nlp/vietnamese_students_feedback\") # https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8573337\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'slide giáo trình đầy đủ .', 'sentiment': 2, 'topic': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = dataset['train'].to_pandas()\n",
    "val_df = dataset['validation'].to_pandas()\n",
    "test_df = dataset['test'].to_pandas()\n",
    "\n",
    "# Concatenate all DataFrames into one (so data is more objective and balance)\n",
    "df = pd.concat([train_df, val_df, test_df], ignore_index=True)\n",
    "\n",
    "# Randomly mix/shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.rename(columns = {'sentence': 'content', 'sentiment': 'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 31460 samples.\n",
      "Label distribution: [ 6669  4698 20093]\n"
     ]
    }
   ],
   "source": [
    "texts, labels = load_vietnamese_sa_dataset('/home/elo/.cache/kagglehub/datasets/linhlpv/vietnamese-sentiment-analyst/versions/2/data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_additional = pd.DataFrame({\n",
    "    'content': texts,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Remove rows where the label could not be mapped (i.e., 'label' is None).\n",
    "# This handles cases where the original rating was not in [1, 2, 3, 4, 5].\n",
    "df_additional.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "df_additional['label'] = df_additional['label'].astype(int)\n",
    "\n",
    "df = pd.concat([df, df_additional], ignore_index=True)\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cô cho thực hành rất nhiều .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Áo chất cứng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hàng y hình</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quần giống hình nhưng ko có phần chọn size, sh...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thầy rất dễ chịu .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47630</th>\n",
       "      <td>cô rất thương sinh viên , hầu như tất cả sinh ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47631</th>\n",
       "      <td>Shop phục vụ rất tốt Rất đáng tiền</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47632</th>\n",
       "      <td>Thời gian giao hàng rất nhanh</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47633</th>\n",
       "      <td>hy vọng ý kiến của em được xem xét !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47634</th>\n",
       "      <td>cho nhiều bài tập đa dạng .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47635 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label\n",
       "0                           cô cho thực hành rất nhiều .      2\n",
       "1                                           Áo chất cứng      0\n",
       "2                                            Hàng y hình      1\n",
       "3      Quần giống hình nhưng ko có phần chọn size, sh...      1\n",
       "4                                     thầy rất dễ chịu .      2\n",
       "...                                                  ...    ...\n",
       "47630  cô rất thương sinh viên , hầu như tất cả sinh ...      2\n",
       "47631                 Shop phục vụ rất tốt Rất đáng tiền      2\n",
       "47632                      Thời gian giao hàng rất nhanh      2\n",
       "47633               hy vọng ý kiến của em được xem xét !      0\n",
       "47634                        cho nhiều bài tập đa dạng .      2\n",
       "\n",
       "[47635 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to sentiment_analysis_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the path for the output CSV file\n",
    "output_csv_path = 'sentiment_analysis_dataset.csv'\n",
    "\n",
    "df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"DataFrame saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 1942 Vietnamese stopwords.\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# Vietnamese BERT Models Evaluation Notebook\n",
    "# Filename: vietnamese_bert_evaluation.ipynb\n",
    "\n",
    "# Cell 1: Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "# Explanation for the AdamW import change:\n",
    "# The error \"cannot import name 'AdamW' from 'transformers'\" occurs because\n",
    "# the AdamW optimizer is no longer available directly under the 'transformers'\n",
    "# top-level package in recent versions of the Hugging Face transformers library.\n",
    "# It was deprecated and then removed.\n",
    "#\n",
    "# The recommended way to import AdamW is now directly from 'torch.optim'.\n",
    "# The 'get_linear_schedule_with_warmup' function, however, remains correctly\n",
    "# imported from 'transformers'.\n",
    "#\n",
    "# Old import line (causing the error):\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "#\n",
    "# Corrected imports:\n",
    "from torch.optim import AdamW  # Import AdamW from torch.optim\n",
    "from transformers import get_linear_schedule_with_warmup # This import is still correct\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "from underthesea import word_tokenize\n",
    "import emoji\n",
    "# For Vietnamese text processing\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Cell 2: Load Vietnamese Stopwords\n",
    "try:\n",
    "    with open('/data/elo/khanglg/FreeTxt-Flask/vietnamese-stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        vi_stopwords = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Successfully loaded {len(vi_stopwords)} Vietnamese stopwords.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Vietnamese stopwords file not found. Please check the path.\")\n",
    "    vi_stopwords = []\n",
    "\n",
    "# Define punctuation\n",
    "PUNCS = '''!→()-[]{};:'\"\\,<>?@#$%^&*_~'''\n",
    "\n",
    "# Cell 3: Device Configuration\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "def normalize_unicode(text):\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# Cell 4: Text Preprocessing Function\n",
    "# Old preprocess_text function\n",
    "# def preprocess_text(text, language='vi'):\n",
    "#     \"\"\"\n",
    "#     Preprocesses text for sentiment analysis:\n",
    "#     - Converts input to string to handle potential NaNs or other types\n",
    "#     - Removes URLs, mentions, hashtags\n",
    "#     - Removes punctuation\n",
    "#     - Converts to lowercase\n",
    "#     - Removes stopwords\n",
    "#     \"\"\"\n",
    "#     text = str(text) # Convert text to string to prevent TypeError\n",
    "#     text = re.sub(r\"http\\\\S+|@\\\\S+|#\\\\S+\", \"\", text)\n",
    "#     text = re.sub(f\"[{re.escape(''.join(PUNCS))}]\", \"\", text.lower())\n",
    "#     text = \" \".join(word for word in text.split() if word not in vi_stopwords)\n",
    "#     return text\n",
    "def preprocess_text(text, language='vi'): # Keep the name consistent if other cells call it\n",
    "    \"\"\"\n",
    "    Improved preprocessing for text, especially Vietnamese:\n",
    "    - Unicode normalization (NFC)\n",
    "    - URL, mention, hashtag removal\n",
    "    - Word segmentation (using underthesea for Vietnamese)\n",
    "    - Stopword removal on token list\n",
    "    - Punctuation removal\n",
    "    - Whitespace normalization\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    text = normalize_unicode(text) # Normalize Unicode first\n",
    "    text = emoji.demojize(text)\n",
    "    # Remove URLs, mentions, and hashtags before word segmentation\n",
    "    # as they might interfere or be wrongly segmented.\n",
    "    text = re.sub(r\"http\\S+|@\\S+|#\\S+\", \"\", text)\n",
    "    \n",
    "    if language == 'vi':\n",
    "        # Tokenize to list for better stopword filtering and processing\n",
    "        tokens = word_tokenize(text, format=\"list\") # Returns list of words\n",
    "    else:\n",
    "        # For other languages, or if underthesea is not to be used for them,\n",
    "        # split by space. BERT tokenizers will handle subwords.\n",
    "        tokens = text.split()\n",
    "\n",
    "    # Lowercase, filter stopwords, and keep only tokens with word characters\n",
    "    # (helps remove leftover punctuation tokens before final PUNCS removal)\n",
    "    processed_tokens = []\n",
    "    for t in tokens:\n",
    "        t_lower = t.lower()\n",
    "        # Check if it's a stopword (only for Vietnamese in this specific path)\n",
    "        is_stopword = (language == 'vi' and t_lower in vi_stopwords)\n",
    "        \n",
    "        # Keep token if it's not a stopword AND contains at least one word character\n",
    "        # re.search(r'\\w', t_lower) checks if there is any alphanumeric character.\n",
    "        if not is_stopword and re.search(r'\\w', t_lower):\n",
    "            processed_tokens.append(t_lower)\n",
    "            \n",
    "    # Rejoin tokens\n",
    "    processed_text = \" \".join(processed_tokens)\n",
    "    processed_text = re.sub(f\"[{re.escape(''.join(PUNCS))}]\", \"\", processed_text) # Apply to string\n",
    "    \n",
    "    # Normalize extra whitespace that might have been introduced or left over\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "\n",
    "# Cell 5: Load and Prepare Dataset\n",
    "# Note: You'll need to replace this with your actual dataset loading code\n",
    "def load_vietnamese_sa_dataset(file_path, has_header=True):\n",
    "    \"\"\"\n",
    "    Loads a Vietnamese sentiment analysis dataset.\n",
    "    Expected format: CSV/TSV with text and label columns\n",
    "    Returns: texts, labels\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.tsv'):\n",
    "            df = pd.read_csv(file_path, sep='\\t')\n",
    "        else:\n",
    "            raise ValueError(\"File format not supported. Please use CSV or TSV.\")\n",
    "        \n",
    "        \n",
    "        # Adapt these column names to match your dataset\n",
    "        texts = df['content'].values  # Replace 'text' with your actual column name\n",
    "        labels = df['label'].values  # Replace 'label' with your actual column name\n",
    "        # Ensure labels are consistently processed and converted to integers.\n",
    "        \n",
    "        # Updated mapping to handle 'POS', 'NEU', 'NEG' labels.\n",
    "        # We'll map them to 0 (Negative), 1 (Neutral), 2 (Positive).\n",
    "        # Consider converting labels to a consistent case (e.g., uppercase) before mapping.\n",
    "        current_label_mapping = {\n",
    "            'NEG': 0,  # Negative\n",
    "            'NEU': 1,  # Neutral\n",
    "            'POS': 2   # Positive\n",
    "        }\n",
    "\n",
    "        processed_labels = []\n",
    "        # Iterate through labels, convert to string, strip whitespace, convert to uppercase, then map.\n",
    "        for label_val in labels: # labels is initially df['label'].values\n",
    "            # Convert to string, strip leading/trailing whitespace, and convert to uppercase\n",
    "            # to handle potential case variations (e.g., 'pos', 'Pos', 'POS').\n",
    "            s_label = str(label_val).strip().upper()\n",
    "            \n",
    "            mapped_value = current_label_mapping.get(s_label)\n",
    "            \n",
    "            if mapped_value is not None:\n",
    "                processed_labels.append(mapped_value)\n",
    "            else:\n",
    "                # If not in mapping, it's an unhandled label.\n",
    "                # The original code also had a try-except for direct int conversion,\n",
    "                # but with 'POS', 'NEU', 'NEG', this path is unlikely to be useful\n",
    "                # unless the dataset mixes these string labels with numeric ones.\n",
    "                # For clarity and to strictly adhere to the new requirement,\n",
    "                # we will raise an error if a label is not in the defined mapping.\n",
    "                raise ValueError(\n",
    "                    f\"Label '{s_label}' (derived from original value: '{label_val}') is not in the \"\n",
    "                    f\"defined mapping {current_label_mapping}. \"\n",
    "                    f\"Please check your dataset or update the label mapping. \"\n",
    "                    f\"Expected labels are 'POS', 'NEU', 'NEG' (case-insensitive).\"\n",
    "                )\n",
    "        \n",
    "        # Convert the list of processed labels to a NumPy array of int64\n",
    "        labels = np.array(processed_labels, dtype=np.int64)\n",
    "        \n",
    "        # Map labels if needed (example mapping)\n",
    "        # Assuming labels are: negative=0, neutral=1, positive=2\n",
    "        # If your dataset uses different values, adjust this mapping\n",
    "        label_mapping = {'0': 0, '1': 1, '2': 2}  # Adjust as needed\n",
    "        if isinstance(labels[0], str):\n",
    "            labels = np.array([label_mapping.get(label, label) for label in labels])\n",
    "        \n",
    "        print(f\"Loaded dataset with {len(texts)} samples.\")\n",
    "        print(f\"Label distribution: {np.bincount(labels)}\")\n",
    "        \n",
    "        return texts, labels\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Sample usage\n",
    "# texts, labels = load_vietnamese_sa_dataset('path/to/your/vietnamese_sentiment_dataset.csv')\n",
    "\n",
    "# Cell 6: Load and Compare Models\n",
    "def load_model(model_name, num_labels=3):\n",
    "    \"\"\"\n",
    "    Loads a pretrained model and tokenizer, and adapts it for sentiment classification.\n",
    "    Args:\n",
    "        model_name: HuggingFace model name/path\n",
    "        num_labels: Number of sentiment classes (3 for negative/neutral/positive)\n",
    "    Returns:\n",
    "        tokenizer, model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # If the model already has a classification head, use AutoModelForSequenceClassification\n",
    "        try:\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "        except:\n",
    "            # If it's a base model without classification head, load base model and add classification layer\n",
    "            base_model = AutoModel.from_pretrained(model_name)\n",
    "            \n",
    "            # Create a simple classification head\n",
    "            class SentimentClassifier(torch.nn.Module):\n",
    "                def __init__(self, base_model, num_labels):\n",
    "                    super(SentimentClassifier, self).__init__()\n",
    "                    self.base_model = base_model\n",
    "                    self.dropout = torch.nn.Dropout(0.1)\n",
    "                    self.classifier = torch.nn.Linear(base_model.config.hidden_size, num_labels)\n",
    "                \n",
    "                def forward(self, input_ids, attention_mask):\n",
    "                    outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "                    pooled_output = self.dropout(pooled_output)\n",
    "                    logits = self.classifier(pooled_output)\n",
    "                    return logits\n",
    "            \n",
    "            model = SentimentClassifier(base_model, num_labels)\n",
    "        \n",
    "        model.to(device)\n",
    "        return tokenizer, model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# # Load VisoBERT and PhoBERT\n",
    "# print(\"Loading VisoBERT...\")\n",
    "# viso_tokenizer, viso_model = load_model(\"binhvq/visoBERT-base\")\n",
    "\n",
    "# print(\"Loading PhoBERT...\")\n",
    "# pho_tokenizer, pho_model = load_model(\"vinai/phobert-base\")\n",
    "\n",
    "# Cell 7: Prepare data for models\n",
    "def prepare_data_for_model(texts, labels, tokenizer, max_length=128, batch_size=16):\n",
    "    \"\"\"\n",
    "    Converts texts and labels into PyTorch dataset and dataloaders.\n",
    "    \"\"\"\n",
    "    # Preprocess the texts\n",
    "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
    "    \n",
    "    # Tokenize texts\n",
    "    encodings = tokenizer(\n",
    "        preprocessed_texts,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Create PyTorch dataset\n",
    "    dataset = TensorDataset(\n",
    "        encodings['input_ids'],\n",
    "        encodings['attention_mask'],\n",
    "        torch.tensor(labels)\n",
    "    )\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        sampler=SequentialSampler(val_dataset),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Cell 8: Training and Evaluation Functions\n",
    "def train_model(model, train_dataloader, val_dataloader, epochs=3):\n",
    "    \"\"\"\n",
    "    Trains the model and evaluates after each epoch.\n",
    "    \"\"\"\n",
    "    # Set up optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for batch in tqdm(train_dataloader):\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1]\n",
    "            }\n",
    "            labels = batch[2]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            if isinstance(model, torch.nn.Module) and not hasattr(model, 'config'):\n",
    "                logits = model(**inputs)\n",
    "                loss_fn = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fn(logits, labels)\n",
    "            else:\n",
    "                outputs = model(**inputs, labels=labels)\n",
    "                loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        print(f\"Average training loss: {avg_train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_accuracy, val_report = evaluate_model(model, val_dataloader)\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        print(f\"Classification Report:\\n{val_report}\")\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            # You can save the model here if needed\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluates the model and returns accuracy and classification report.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1]\n",
    "        }\n",
    "        labels = batch[2]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if isinstance(model, torch.nn.Module) and not hasattr(model, 'config'):\n",
    "                logits = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        pred_labels = labels.cpu().numpy()\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(pred_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions)\n",
    "    \n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "# print(pho_report)\n",
    "\n",
    "# Cell 10: Inference Example\n",
    "def predict_sentiment(text, tokenizer, model, preprocess=True):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a single text.\n",
    "    Returns the predicted class and confidence.\n",
    "    \"\"\"\n",
    "    if preprocess:\n",
    "        text = preprocess_text(text)\n",
    "    \n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, torch.nn.Module) and not hasattr(model, 'config'):\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        else:\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(probabilities)\n",
    "    confidence = probabilities[predicted_class]\n",
    "    \n",
    "    # Map class to sentiment\n",
    "    sentiment_map = {0: 'Tiêu cực', 1: 'Trung tính', 2: 'Tích cực'}\n",
    "    predicted_sentiment = sentiment_map.get(predicted_class, 'Unknown')\n",
    "    \n",
    "    return predicted_sentiment, confidence, probabilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning will use device: cuda:1\n",
      "Selected model for fine-tuning: bkai-foundation-models/vietnamese-bi-encoder\n",
      "Number of labels to be used: 3\n",
      "Output directory: FreeTxt-Flask/results_sa_finetune_{model_identifier}\n"
     ]
    }
   ],
   "source": [
    "# Cell: Additional Imports and Configuration\n",
    "\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os # Already imported, but good to remember it's used\n",
    "\n",
    "# --- Choose your model and define parameters ---\n",
    "# You can cycle through these by uncommenting the one you want to use\n",
    "MODEL_NAME = \"bkai-foundation-models/vietnamese-bi-encoder\"\n",
    "# MODEL_NAME = \"vinai/phobert-base-v2\"\n",
    "# MODEL_NAME = \"microsoft/Multilingual-MiniLM-L12-H384\"\n",
    "\n",
    "# Ensure the NUM_LABELS matches your dataset's label scheme (0, 1, 2 for negative, neutral, positive)\n",
    "NUM_LABELS = df['label'].nunique() # Dynamically get number of unique labels\n",
    "if NUM_LABELS != 3: # Or your expected number of classes\n",
    "    print(f\"WARNING: Expected 3 labels, but found {NUM_LABELS}. Check your 'label' column.\")\n",
    "    # You might want to assert this or handle it if NUM_LABELS is not what you expect for SA.\n",
    "    # For now, we'll proceed with the detected number of labels.\n",
    "\n",
    "# --- Preprocessing & Tokenization ---\n",
    "# MAX_SEQ_LENGTH can be adjusted based on your data and model\n",
    "MAX_SEQ_LENGTH = 128\n",
    "\n",
    "# --- Training Configuration ---\n",
    "# Constructing output directory name based on the chosen model\n",
    "model_identifier = MODEL_NAME.split('/')[-1] # e.g., 'vietnamese-bi-encoder'\n",
    "OUTPUT_DIR = \"FreeTxt-Flask/results_sa_finetune_{model_identifier}\"\n",
    "LOGGING_DIR = \"FreeTxt-Flask/logs_sa_finetune_{model_identifier}\"\n",
    "\n",
    "NUM_TRAIN_EPOCHS = 5\n",
    "LEARNING_RATE = 2e-5\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 16 # Adjust based on GPU memory\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.1\n",
    "EVALUATION_STRATEGY = \"epoch\"\n",
    "SAVE_STRATEGY = \"epoch\"\n",
    "LOAD_BEST_MODEL_AT_END = True\n",
    "METRIC_FOR_BEST_MODEL = \"f1\" # or \"accuracy\"\n",
    "SEED = 42\n",
    "\n",
    "# --- Environment ---\n",
    "# Ensure device is set (it's likely already set in your notebook, e.g., to cuda:1 or cuda:3)\n",
    "# If not, uncomment and adjust:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Fine-tuning will use device: {device}\") # Print the device being used from earlier cell\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Selected model for fine-tuning: {MODEL_NAME}\")\n",
    "print(f\"Number of labels to be used: {NUM_LABELS}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_finetuning = df['content'].tolist()\n",
    "labels_for_finetuning = df['label'].astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts for fine-tuning: 47635\n",
      "Number of labels for fine-tuning: 47635\n",
      "Sample text: cô cho thực hành rất nhiều .\n",
      "Sample label: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of texts for fine-tuning: {len(texts_for_finetuning)}\")\n",
    "print(f\"Number of labels for fine-tuning: {len(labels_for_finetuning)}\")\n",
    "print(f\"Sample text: {texts_for_finetuning[0]}\")\n",
    "print(f\"Sample label: {labels_for_finetuning[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading base model bkai-foundation-models/vietnamese-bi-encoder for fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at bkai-foundation-models/vietnamese-bi-encoder and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading base model {MODEL_NAME} for fine-tuning...\")\n",
    "bkai_tokenizer, bkai_model = load_model(\"bkai-foundation-models/vietnamese-bi-encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing training and validation dataloaders using your 'prepare_data_for_model'...\n",
      "Number of batches in train_dataloader_ft: 2382\n",
      "Number of batches in val_dataloader_ft: 596\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPreparing training and validation dataloaders using your 'prepare_data_for_model'...\")\n",
    "train_dataloader_ft, val_dataloader_ft = prepare_data_for_model(\n",
    "    texts_for_finetuning,\n",
    "    labels_for_finetuning,\n",
    "    bkai_tokenizer,\n",
    "    max_length=MAX_SEQ_LENGTH,\n",
    "    batch_size=PER_DEVICE_TRAIN_BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in train_dataloader_ft: {len(train_dataloader_ft)}\")\n",
    "print(f\"Number of batches in val_dataloader_ft: {len(val_dataloader_ft)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine-tuning of bkai-foundation-models/vietnamese-bi-encoder for 5 epochs...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2382/2382 [10:30<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5758565687687831\n",
      "Validation Accuracy: 0.7836674713970819\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.74      2849\n",
      "           1       0.48      0.14      0.22      1082\n",
      "           2       0.81      0.94      0.87      5596\n",
      "\n",
      "    accuracy                           0.78      9527\n",
      "   macro avg       0.69      0.60      0.61      9527\n",
      "weighted avg       0.76      0.78      0.76      9527\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2382/2382 [10:58<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.4889420593011965\n",
      "Validation Accuracy: 0.7974178650152199\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77      2849\n",
      "           1       0.44      0.24      0.31      1082\n",
      "           2       0.85      0.92      0.88      5596\n",
      "\n",
      "    accuracy                           0.80      9527\n",
      "   macro avg       0.68      0.64      0.65      9527\n",
      "weighted avg       0.78      0.80      0.78      9527\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 1766/2382 [09:18<03:14,  3.16it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting fine-tuning of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_TRAIN_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Your train_model function returns the trained model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m fine_tuned_model_manual \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbkai_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass the newly loaded model instance\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloader_ft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TRAIN_EPOCHS\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning finished.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 297\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, epochs)\u001b[0m\n\u001b[1;32m    294\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    295\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 297\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_train_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TRAIN_EPOCHS = 5 \n",
    "print(f\"\\nStarting fine-tuning of {MODEL_NAME} for {NUM_TRAIN_EPOCHS} epochs...\")\n",
    "    # Your train_model function returns the trained model\n",
    "fine_tuned_model_manual = train_model(\n",
    "        bkai_model, # Pass the newly loaded model instance\n",
    "        train_dataloader_ft,\n",
    "        val_dataloader_ft,\n",
    "        epochs=NUM_TRAIN_EPOCHS\n",
    "    )\n",
    "print(\"Fine-tuning finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fine_tuned_model_manual' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bkai_accuracy, bkai_report \u001b[38;5;241m=\u001b[39m evaluate_model(\u001b[43mfine_tuned_model_manual\u001b[49m, val_dataloader_ft)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fine_tuned_model_manual' is not defined"
     ]
    }
   ],
   "source": [
    "bkai_accuracy, bkai_report = evaluate_model(fine_tuned_model_manual, val_dataloader_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BKAI Vietnamese Bi-Encoder...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1573/1573 [03:58<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5936510810902068\n",
      "Validation Accuracy: 0.7786077558804831\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.63      0.69      1383\n",
      "           1       0.39      0.23      0.29       904\n",
      "           2       0.83      0.95      0.89      4005\n",
      "\n",
      "    accuracy                           0.78      6292\n",
      "   macro avg       0.66      0.61      0.62      6292\n",
      "weighted avg       0.75      0.78      0.76      6292\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1573/1573 [03:59<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.5048919593073423\n",
      "Validation Accuracy: 0.7905276541640178\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.74      0.73      1383\n",
      "           1       0.40      0.28      0.33       904\n",
      "           2       0.87      0.92      0.90      4005\n",
      "\n",
      "    accuracy                           0.79      6292\n",
      "   macro avg       0.67      0.65      0.65      6292\n",
      "weighted avg       0.77      0.79      0.78      6292\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1573/1573 [03:57<00:00,  6.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.44505883378165584\n",
      "Validation Accuracy: 0.7863954227590592\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73      1383\n",
      "           1       0.39      0.29      0.33       904\n",
      "           2       0.86      0.93      0.89      4005\n",
      "\n",
      "    accuracy                           0.79      6292\n",
      "   macro avg       0.66      0.64      0.65      6292\n",
      "weighted avg       0.77      0.79      0.78      6292\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BKAI\n",
    "print(\"Evaluating BKAI Vietnamese Bi-Encoder...\")\n",
    "bkai_train_dataloader, bkai_val_dataloader = prepare_data_for_model(texts, labels, bkai_tokenizer)\n",
    "bkai_model = train_model(bkai_model, bkai_train_dataloader, bkai_val_dataloader)\n",
    "bkai_accuracy, bkai_report = evaluate_model(bkai_model, bkai_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: 'Món ăn này thực sự rất ngon, tôi rất thích nó!'\n",
      "BKAI prediction: Tích cực (confidence: 0.9734)\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Món ăn này thực sự rất ngon, tôi rất thích nó!\"\n",
    "bkai_sentiment, bkai_conf, bkai_probs = predict_sentiment(sample_text, bkai_tokenizer, bkai_model)\n",
    "# pho_sentiment, pho_conf, pho_probs = predict_sentiment(sample_text, pho_tokenizer, pho_model)\n",
    "\n",
    "print(f\"Sample text: '{sample_text}'\")\n",
    "print(f\"BKAI prediction: {bkai_sentiment} (confidence: {bkai_conf:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vifree-txt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
