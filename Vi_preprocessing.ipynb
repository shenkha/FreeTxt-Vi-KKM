{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "# Make sure underthesea is installed: pip install underthesea\n",
    "from underthesea import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# --- Dictionaries and Data for Vietnamese Character Processing ---\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split('|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split('|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "# --- Load Teencode Data ---\n",
    "# Path to the teencode file.\n",
    "_teencode_file_path = './teencode.txt' # As specified in prompt context\n",
    "try:\n",
    "    teencode_df = pd.read_csv(_teencode_file_path, names=['teencode', 'map'], sep='\\t', header=None)\n",
    "    teencode_map_default = pd.Series(teencode_df['map'].values, index=teencode_df['teencode']).to_dict()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Teencode file not found at {_teencode_file_path}. Teencode replacement will be limited.\")\n",
    "    teencode_map_default = {}\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading teencode file '{_teencode_file_path}': {e}. Teencode replacement will be limited.\")\n",
    "    teencode_map_default = {}\n",
    "\n",
    "\n",
    "# --- Load Stopwords Data ---\n",
    "_stopwords_file_path = '/data/elo/khanglg/FreeTxt-Flask/vietnamese-stopwords.txt' # Path from existing code\n",
    "try:\n",
    "    with open(_stopwords_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords_list_default = [line.strip() for line in f if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Stopwords file not found at {_stopwords_file_path}. Stopword removal will be limited.\")\n",
    "    stopwords_list_default = []\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading stopwords file '{_stopwords_file_path}': {e}. Stopword removal will be limited.\")\n",
    "    stopwords_list_default = []\n",
    "\n",
    "\n",
    "# Emoji pattern from the user's snippet (more comprehensive)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u2640-\\u2642\"\n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\u3030\"\n",
    "    u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def convert_unicode_legacy(txt): # Renamed to avoid clash if user defines convert_unicode\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "def text_unicode_normalize(text): # From user's snippet\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        if x == 9: # u\n",
    "            if index > 0 and chars[index - 1].lower() == 'q':\n",
    "                chars[index] = 'u'; qu_or_gi = True\n",
    "        elif x == 5: # i\n",
    "            if index > 0 and chars[index - 1].lower() == 'g':\n",
    "                chars[index] = 'i'; qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y; chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1: # Fix: check qu_or_gi correctly\n",
    "            nguyen_am_index.append(index)\n",
    "\n",
    "    if not nguyen_am_index: return \"\".join(chars)\n",
    "\n",
    "    # Determine which vowel to place the tone mark on\n",
    "    idx_to_mark = nguyen_am_index[0] # Default to the first vowel in the group\n",
    "    if len(nguyen_am_index) >= 2:\n",
    "        # Priority for ê, ơ, ô\n",
    "        priority_vowel_found = False\n",
    "        for idx_candidate in nguyen_am_index:\n",
    "            x_vowel, _ = nguyen_am_to_ids.get(chars[idx_candidate], (-1,-1))\n",
    "            if x_vowel in [4, 7, 8]: # ê, ô, ơ\n",
    "                idx_to_mark = idx_candidate\n",
    "                priority_vowel_found = True\n",
    "                break\n",
    "        \n",
    "        if not priority_vowel_found:\n",
    "            # Rules for diphthongs/triphthongs (simplified from original logic)\n",
    "            # If the vowel group is at the end of the word\n",
    "            if nguyen_am_index[-1] == len(chars) -1:\n",
    "                # If ends with i, u, y (closed vowels/semivowels), mark the vowel before it\n",
    "                x_last_vowel, _ = nguyen_am_to_ids.get(chars[nguyen_am_index[-1]], (-1,-1))\n",
    "                if x_last_vowel in [5, 9, 10, 11]: # i, u, ư, y\n",
    "                     idx_to_mark = nguyen_am_index[-2] if len(nguyen_am_index) > 1 else nguyen_am_index[-1]\n",
    "                else: # Otherwise, mark the first vowel of the group (e.g., 'oa', 'oe')\n",
    "                    idx_to_mark = nguyen_am_index[0]\n",
    "            else: # Vowel group is followed by consonants (e.g., 'uyen', 'oan')\n",
    "                if len(nguyen_am_index) == 3: # Triphthongs like 'uye', 'oai' -> mark middle\n",
    "                    idx_to_mark = nguyen_am_index[1]\n",
    "                elif len(nguyen_am_index) == 2: # Diphthongs like 'uyê', 'oa' -> mark second\n",
    "                    idx_to_mark = nguyen_am_index[1]\n",
    "                # else (single vowel before consonant), default (first vowel) is fine\n",
    "\n",
    "    # Apply the tone mark\n",
    "    x_target_vowel, _ = nguyen_am_to_ids.get(chars[idx_to_mark], (-1,-1))\n",
    "    if x_target_vowel != -1 and dau_cau != 0:\n",
    "        chars[idx_to_mark] = bang_nguyen_am[x_target_vowel][dau_cau]\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        # Preserve surrounding punctuation by splitting and processing only the word part\n",
    "        match = re.match(r'(^[\\W_]*)([\\wÀ-Ỹà-ỹ._]*[\\wÀ-Ỹà-ỹ]+)([\\W_]*$)', word)\n",
    "        if match:\n",
    "            prefix, core_word, suffix = match.groups()\n",
    "            normalized_core_word = chuan_hoa_dau_tu_tieng_viet(core_word)\n",
    "            words[index] = prefix + normalized_core_word + suffix\n",
    "        else: # If word doesn't match (e.g. pure punctuation or malformed), try to normalize if it's a simple word\n",
    "            words[index] = chuan_hoa_dau_tu_tieng_viet(word) \n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "def preprocess_vietnamese_text_underthesea(\n",
    "    text,\n",
    "    custom_teencode_map=None,\n",
    "    custom_stopwords_list=None,\n",
    "    use_teencode=True,\n",
    "    use_stopwords=False,\n",
    "    remove_all_punctuation=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Comprehensive Vietnamese text preprocessing using underthesea.\n",
    "    Args:\n",
    "        text (str): Input Vietnamese text.\n",
    "        custom_teencode_map (dict, optional): Custom teencode mapping.\n",
    "                                              Defaults to loaded teencode_map_default.\n",
    "        custom_stopwords_list (list, optional): Custom list of stopwords.\n",
    "                                                Defaults to loaded stopwords_list_default.\n",
    "        use_teencode (bool): Whether to perform teencode replacement.\n",
    "        use_stopwords (bool): Whether to perform stopword removal.\n",
    "        remove_all_punctuation (bool): If True, removes all punctuation. Defaults to False.\n",
    "    Returns:\n",
    "        str: Processed text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Use custom maps/lists if provided, otherwise use the loaded defaults\n",
    "    current_teencode_map = custom_teencode_map if custom_teencode_map is not None else teencode_map_default\n",
    "    current_stopwords_list = custom_stopwords_list if custom_stopwords_list is not None else stopwords_list_default\n",
    "\n",
    "    # 1. Lowercase\n",
    "    processed_text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs, mentions, hashtags\n",
    "    processed_text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", processed_text)\n",
    "\n",
    "    # 3. Legacy Unicode conversion (e.g., Windows-1258 to Unicode)\n",
    "    processed_text = convert_unicode_legacy(processed_text)\n",
    "\n",
    "    # 4. Standard Unicode Normalization (NFC)\n",
    "    processed_text = text_unicode_normalize(processed_text)\n",
    "\n",
    "    # 5. Remove Emojis\n",
    "    processed_text = re.sub(emoji_pattern, \" \", processed_text) # Replace with space to avoid merging words\n",
    "\n",
    "    # 6. Reduce repeated alphabetic characters (e.g., \"chàoooo\" -> \"chào\")\n",
    "    # This regex handles Vietnamese characters correctly.\n",
    "    processed_text = re.sub(r'([a-zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ])\\1+', r'\\1', processed_text)\n",
    "\n",
    "    # 7. Reduce repeated special characters (non-alphanumeric, non-whitespace)\n",
    "    # This regex handles Vietnamese characters by excluding them from \"special characters\".\n",
    "    processed_text = re.sub(r'([^a-z0-9àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ\\s])\\1+', r'\\1', processed_text)\n",
    "\n",
    "    # 8. Normalize punctuation spacing\n",
    "    # Ensure space around punctuation that is between words or at the end/start of a word.\n",
    "    # This helps the tokenizer.\n",
    "    escaped_punctuation = re.escape(string.punctuation)\n",
    "    processed_text = re.sub(r\"(\\w)\\s*([\" + escaped_punctuation + r\"])\\s*(\\w)\", r\"\\1 \\2 \\3\", processed_text)\n",
    "    processed_text = re.sub(r\"(\\w)\\s*([\" + escaped_punctuation + r\"])\", r\"\\1 \\2\", processed_text) # Word followed by punctuation\n",
    "    processed_text = re.sub(r\"([\" + escaped_punctuation + r\"])\\s*(\\w)\", r\"\\1 \\2\", processed_text) # Punctuation followed by word\n",
    "\n",
    "\n",
    "    # 9. Reduce repeated punctuation characters (e.g., \"!!!\" to \"!\")\n",
    "    # This should be done AFTER spacing normalization if we want \"!!! \" -> \"! \"\n",
    "    # The previous regex for repeated special chars might have handled some of this,\n",
    "    # but this is more specific to string.punctuation.\n",
    "    processed_text = re.sub(r\"([\" + escaped_punctuation + r\"])\\1+\", r\"\\1\", processed_text)\n",
    "\n",
    "\n",
    "    # 10. Vietnamese tone mark normalization\n",
    "    processed_text = chuan_hoa_dau_cau_tieng_viet(processed_text)\n",
    "\n",
    "    # 11. Remove all punctuation (optional)\n",
    "    if remove_all_punctuation:\n",
    "        # Create a translation table that maps all punctuation to None\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        processed_text = processed_text.translate(translator)\n",
    "\n",
    "    # 12. Final whitespace cleanup (multiple spaces to single, strip ends)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    # 13. Strip leading/trailing punctuation or space robustly (if not all removed)\n",
    "    if not remove_all_punctuation and processed_text: # Check if processed_text is not empty\n",
    "        # Characters to strip: all punctuation and whitespace\n",
    "        strip_chars = string.punctuation + string.whitespace\n",
    "        # Strip from the end\n",
    "        while processed_text and processed_text[-1] in strip_chars:\n",
    "            processed_text = processed_text[:-1]\n",
    "        # Strip from the beginning\n",
    "        while processed_text and processed_text[0] in strip_chars:\n",
    "            processed_text = processed_text[1:]\n",
    "    \n",
    "    # If all processing results in an empty string, return it.\n",
    "    if not processed_text:\n",
    "        return \"\"\n",
    "\n",
    "    # 14. Tokenization using underthesea\n",
    "    tokens = word_tokenize(processed_text, format=\"list\") # Get list of tokens\n",
    "\n",
    "    # 15. Teencode Replacement (on tokens)\n",
    "    if use_teencode and current_teencode_map:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            replacement = current_teencode_map.get(token, token)\n",
    "            # If teencode replacement results in multiple words, split them.\n",
    "            # This requires re-tokenization or careful splitting.\n",
    "            # For simplicity, we assume teencode maps to single words or phrases\n",
    "            # that underthesea can handle if joined and re-tokenized.\n",
    "            new_tokens.append(replacement)\n",
    "        \n",
    "        # If teencode replacement might introduce multi-word strings, re-tokenize.\n",
    "        # This ensures that phrases from teencode are properly tokenized.\n",
    "        if any(\" \" in t for t in new_tokens): # Check if any token now contains a space\n",
    "            temp_token_string = \" \".join(new_tokens)\n",
    "            tokens = word_tokenize(temp_token_string, format=\"list\")\n",
    "        else:\n",
    "            tokens = new_tokens\n",
    "\n",
    "\n",
    "    # 16. Stopword Removal (on tokens)\n",
    "    if use_stopwords and current_stopwords_list:\n",
    "        tokens = [token for token in tokens if token not in current_stopwords_list and token.strip()]\n",
    "\n",
    "    # 17. Join tokens to form the final processed string\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Xiiiiiiin chàooooo, bạn khoẻee khôngggg ??? Tui là   Hùng nè... ghé thăm web https://example.com chơi nhaaa :)))) 😂😂😂 #VuiVe'\n",
      "Processed: 'xin chào , bạn khỏee không ? tui là hùng nè . ghé thăm web chơi nha'\n",
      "\n",
      "Original: 'Trời ơi CLM, đC k hÙng? Mày bị đIên à????'\n",
      "Processed: 'trời ơi cái lồn má , được k hùng ? mày bị điên à'\n",
      "\n",
      "Original: 'Shop bán đồ đắt qúa đi, hok có tiền mua đâuuuu :('\n",
      "Processed (no stopwords): 'shop bán đồ đắt quá đi , không có tiền mua đâu'\n",
      "\n",
      "Original: 'Tuyệt vời!!! sản phẩm này rất tốtttt. highly recommend nha mn. k chê vào đâu đc. @user123'\n",
      "Processed (all punct removed): 'tuyệt vời sản phẩm này rất tốt highly recomend nha mọi người k chê vào đâu được'\n",
      "\n",
      "Original: 'cuộc sống bất công biết mấy cho vừai định nghĩa thế gian này muôn mặt'\n",
      "Processed: 'cuộc sống bất công biết mấy cho vưài định nghĩa thế gian này muôn mặt'\n",
      "\n",
      "Original: 'xạo choá quá anh'\n",
      "Processed: 'xạo chóa quá anh'\n",
      "\n",
      "Original: 'anh hoà, đang làm.. gì'\n",
      "Processed: 'anh hòa , đang làm . gi'\n",
      "\n",
      "Original: 'Đi đâu đó bạn ơi????? kkkk :D :D'\n",
      "Processed: 'đi đâu đó bạn ơi ? k : d : d'\n",
      "\n",
      "Original (no teencode/stopwords): 'Xiiiiiiin chàooooo, bạn khoẻee khôngggg ??? Tui là   Hùng nè... ghé thăm web https://example.com chơi nhaaa :)))) 😂😂😂 #VuiVe'\n",
      "Processed (no teencode/stopwords): 'xin chào , bạn khỏee không ? tui là hùng nè . ghé thăm web chơi nha'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "sample_text_1 = \"Xiiiiiiin chàooooo, bạn khoẻee khôngggg ??? Tui là   Hùng nè... ghé thăm web https://example.com chơi nhaaa :)))) 😂😂😂 #VuiVe\"\n",
    "sample_text_2 = \"Trời ơi CLM, đC k hÙng? Mày bị đIên à????\"\n",
    "sample_text_3 = \"Shop bán đồ đắt qúa đi, hok có tiền mua đâuuuu :(\"\n",
    "sample_text_4 = \"Tuyệt vời!!! sản phẩm này rất tốtttt. highly recommend nha mn. k chê vào đâu đc. @user123\"\n",
    "sample_text_5 = \"cuộc sống bất công biết mấy cho vừai định nghĩa thế gian này muôn mặt\"\n",
    "sample_text_6 = \"xạo choá quá anh\" # from notebook (accent issue) -> xạo chóa quá anh\n",
    "sample_text_7 = \"anh hoà, đang làm.. gì\" # from notebook (accent issue) -> anh hòa, đang làm. gì\n",
    "sample_text_8 = \"Đi đâu đó bạn ơi????? kkkk :D :D\"\n",
    "\n",
    "print(f\"Original: '{sample_text_1}'\")\n",
    "processed_1 = preprocess_vietnamese_text_underthesea(sample_text_1)\n",
    "print(f\"Processed: '{processed_1}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_2}'\")\n",
    "processed_2 = preprocess_vietnamese_text_underthesea(sample_text_2)\n",
    "print(f\"Processed: '{processed_2}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_3}'\")\n",
    "processed_3 = preprocess_vietnamese_text_underthesea(sample_text_3, use_stopwords=False)\n",
    "print(f\"Processed (no stopwords): '{processed_3}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_4}'\")\n",
    "processed_4 = preprocess_vietnamese_text_underthesea(sample_text_4, remove_all_punctuation=True)\n",
    "print(f\"Processed (all punct removed): '{processed_4}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_5}'\")\n",
    "processed_5 = preprocess_vietnamese_text_underthesea(sample_text_5)\n",
    "print(f\"Processed: '{processed_5}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_6}'\")\n",
    "processed_6 = preprocess_vietnamese_text_underthesea(sample_text_6)\n",
    "print(f\"Processed: '{processed_6}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_7}'\")\n",
    "processed_7 = preprocess_vietnamese_text_underthesea(sample_text_7)\n",
    "print(f\"Processed: '{processed_7}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_8}'\")\n",
    "processed_8 = preprocess_vietnamese_text_underthesea(sample_text_8)\n",
    "print(f\"Processed: '{processed_8}'\\n\")\n",
    "\n",
    "print(f\"Original (no teencode/stopwords): '{sample_text_1}'\")\n",
    "processed_9 = preprocess_vietnamese_text_underthesea(sample_text_1, use_teencode=False, use_stopwords=False)\n",
    "print(f\"Processed (no teencode/stopwords): '{processed_9}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Hello World quá là đẹp lunnn á'\n",
      "Processed: 'helo world quá là đẹp lun á'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_vi_en = \"Hello World quá là đẹp lunnn á\"\n",
    "print(f\"Original: '{sample_vi_en}'\")\n",
    "processed_vi_en = preprocess_vietnamese_text_underthesea(sample_vi_en)\n",
    "print(f\"Processed: '{processed_vi_en}'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vifree-txt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
