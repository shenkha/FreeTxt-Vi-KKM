{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "# Make sure underthesea is installed: pip install underthesea\n",
    "from underthesea import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# --- Dictionaries and Data for Vietnamese Character Processing ---\n",
    "bang_nguyen_am = [['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],\n",
    "                  ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],\n",
    "                  ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],\n",
    "                  ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e'],\n",
    "                  ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],\n",
    "                  ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i'],\n",
    "                  ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o'],\n",
    "                  ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],\n",
    "                  ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],\n",
    "                  ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u'],\n",
    "                  ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],\n",
    "                  ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = '√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'.split('|')\n",
    "    charutf8 = \"√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥\".split('|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "# --- Load Teencode Data ---\n",
    "# Path to the teencode file.\n",
    "_teencode_file_path = './teencode.txt' # As specified in prompt context\n",
    "try:\n",
    "    teencode_df = pd.read_csv(_teencode_file_path, names=['teencode', 'map'], sep='\\t', header=None)\n",
    "    teencode_map_default = pd.Series(teencode_df['map'].values, index=teencode_df['teencode']).to_dict()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Teencode file not found at {_teencode_file_path}. Teencode replacement will be limited.\")\n",
    "    teencode_map_default = {}\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading teencode file '{_teencode_file_path}': {e}. Teencode replacement will be limited.\")\n",
    "    teencode_map_default = {}\n",
    "\n",
    "\n",
    "# --- Load Stopwords Data ---\n",
    "_stopwords_file_path = '/data/elo/khanglg/FreeTxt-Flask/vietnamese-stopwords.txt' # Path from existing code\n",
    "try:\n",
    "    with open(_stopwords_file_path, 'r', encoding='utf-8') as f:\n",
    "        stopwords_list_default = [line.strip() for line in f if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Stopwords file not found at {_stopwords_file_path}. Stopword removal will be limited.\")\n",
    "    stopwords_list_default = []\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Error loading stopwords file '{_stopwords_file_path}': {e}. Stopword removal will be limited.\")\n",
    "    stopwords_list_default = []\n",
    "\n",
    "\n",
    "# Emoji pattern from the user's snippet (more comprehensive)\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u2640-\\u2642\"\n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\u3030\"\n",
    "    u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def convert_unicode_legacy(txt): # Renamed to avoid clash if user defines convert_unicode\n",
    "    return re.sub(\n",
    "        r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "def text_unicode_normalize(text): # From user's snippet\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        if x == 9: # u\n",
    "            if index > 0 and chars[index - 1].lower() == 'q':\n",
    "                chars[index] = 'u'; qu_or_gi = True\n",
    "        elif x == 5: # i\n",
    "            if index > 0 and chars[index - 1].lower() == 'g':\n",
    "                chars[index] = 'i'; qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y; chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1: # Fix: check qu_or_gi correctly\n",
    "            nguyen_am_index.append(index)\n",
    "\n",
    "    if not nguyen_am_index: return \"\".join(chars)\n",
    "\n",
    "    # Determine which vowel to place the tone mark on\n",
    "    idx_to_mark = nguyen_am_index[0] # Default to the first vowel in the group\n",
    "    if len(nguyen_am_index) >= 2:\n",
    "        # Priority for √™, ∆°, √¥\n",
    "        priority_vowel_found = False\n",
    "        for idx_candidate in nguyen_am_index:\n",
    "            x_vowel, _ = nguyen_am_to_ids.get(chars[idx_candidate], (-1,-1))\n",
    "            if x_vowel in [4, 7, 8]: # √™, √¥, ∆°\n",
    "                idx_to_mark = idx_candidate\n",
    "                priority_vowel_found = True\n",
    "                break\n",
    "        \n",
    "        if not priority_vowel_found:\n",
    "            # Rules for diphthongs/triphthongs (simplified from original logic)\n",
    "            # If the vowel group is at the end of the word\n",
    "            if nguyen_am_index[-1] == len(chars) -1:\n",
    "                # If ends with i, u, y (closed vowels/semivowels), mark the vowel before it\n",
    "                x_last_vowel, _ = nguyen_am_to_ids.get(chars[nguyen_am_index[-1]], (-1,-1))\n",
    "                if x_last_vowel in [5, 9, 10, 11]: # i, u, ∆∞, y\n",
    "                     idx_to_mark = nguyen_am_index[-2] if len(nguyen_am_index) > 1 else nguyen_am_index[-1]\n",
    "                else: # Otherwise, mark the first vowel of the group (e.g., 'oa', 'oe')\n",
    "                    idx_to_mark = nguyen_am_index[0]\n",
    "            else: # Vowel group is followed by consonants (e.g., 'uyen', 'oan')\n",
    "                if len(nguyen_am_index) == 3: # Triphthongs like 'uye', 'oai' -> mark middle\n",
    "                    idx_to_mark = nguyen_am_index[1]\n",
    "                elif len(nguyen_am_index) == 2: # Diphthongs like 'uy√™', 'oa' -> mark second\n",
    "                    idx_to_mark = nguyen_am_index[1]\n",
    "                # else (single vowel before consonant), default (first vowel) is fine\n",
    "\n",
    "    # Apply the tone mark\n",
    "    x_target_vowel, _ = nguyen_am_to_ids.get(chars[idx_to_mark], (-1,-1))\n",
    "    if x_target_vowel != -1 and dau_cau != 0:\n",
    "        chars[idx_to_mark] = bang_nguyen_am[x_target_vowel][dau_cau]\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        # Preserve surrounding punctuation by splitting and processing only the word part\n",
    "        match = re.match(r'(^[\\W_]*)([\\w√Ä-·ª∏√†-·ªπ._]*[\\w√Ä-·ª∏√†-·ªπ]+)([\\W_]*$)', word)\n",
    "        if match:\n",
    "            prefix, core_word, suffix = match.groups()\n",
    "            normalized_core_word = chuan_hoa_dau_tu_tieng_viet(core_word)\n",
    "            words[index] = prefix + normalized_core_word + suffix\n",
    "        else: # If word doesn't match (e.g. pure punctuation or malformed), try to normalize if it's a simple word\n",
    "            words[index] = chuan_hoa_dau_tu_tieng_viet(word) \n",
    "    return \" \".join(words)\n",
    "\n",
    "# --- Main Preprocessing Function ---\n",
    "def preprocess_vietnamese_text_underthesea(\n",
    "    text,\n",
    "    custom_teencode_map=None,\n",
    "    custom_stopwords_list=None,\n",
    "    use_teencode=True,\n",
    "    use_stopwords=False,\n",
    "    remove_all_punctuation=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Comprehensive Vietnamese text preprocessing using underthesea.\n",
    "    Args:\n",
    "        text (str): Input Vietnamese text.\n",
    "        custom_teencode_map (dict, optional): Custom teencode mapping.\n",
    "                                              Defaults to loaded teencode_map_default.\n",
    "        custom_stopwords_list (list, optional): Custom list of stopwords.\n",
    "                                                Defaults to loaded stopwords_list_default.\n",
    "        use_teencode (bool): Whether to perform teencode replacement.\n",
    "        use_stopwords (bool): Whether to perform stopword removal.\n",
    "        remove_all_punctuation (bool): If True, removes all punctuation. Defaults to False.\n",
    "    Returns:\n",
    "        str: Processed text.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # Use custom maps/lists if provided, otherwise use the loaded defaults\n",
    "    current_teencode_map = custom_teencode_map if custom_teencode_map is not None else teencode_map_default\n",
    "    current_stopwords_list = custom_stopwords_list if custom_stopwords_list is not None else stopwords_list_default\n",
    "\n",
    "    # 1. Lowercase\n",
    "    processed_text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs, mentions, hashtags\n",
    "    processed_text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", processed_text)\n",
    "\n",
    "    # 3. Legacy Unicode conversion (e.g., Windows-1258 to Unicode)\n",
    "    processed_text = convert_unicode_legacy(processed_text)\n",
    "\n",
    "    # 4. Standard Unicode Normalization (NFC)\n",
    "    processed_text = text_unicode_normalize(processed_text)\n",
    "\n",
    "    # 5. Remove Emojis\n",
    "    processed_text = re.sub(emoji_pattern, \" \", processed_text) # Replace with space to avoid merging words\n",
    "\n",
    "    # 6. Reduce repeated alphabetic characters (e.g., \"ch√†oooo\" -> \"ch√†o\")\n",
    "    # This regex handles Vietnamese characters correctly.\n",
    "    processed_text = re.sub(r'([a-z√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë])\\1+', r'\\1', processed_text)\n",
    "\n",
    "    # 7. Reduce repeated special characters (non-alphanumeric, non-whitespace)\n",
    "    # This regex handles Vietnamese characters by excluding them from \"special characters\".\n",
    "    processed_text = re.sub(r'([^a-z0-9√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë\\s])\\1+', r'\\1', processed_text)\n",
    "\n",
    "    # 8. Normalize punctuation spacing\n",
    "    # Ensure space around punctuation that is between words or at the end/start of a word.\n",
    "    # This helps the tokenizer.\n",
    "    escaped_punctuation = re.escape(string.punctuation)\n",
    "    processed_text = re.sub(r\"(\\w)\\s*([\" + escaped_punctuation + r\"])\\s*(\\w)\", r\"\\1 \\2 \\3\", processed_text)\n",
    "    processed_text = re.sub(r\"(\\w)\\s*([\" + escaped_punctuation + r\"])\", r\"\\1 \\2\", processed_text) # Word followed by punctuation\n",
    "    processed_text = re.sub(r\"([\" + escaped_punctuation + r\"])\\s*(\\w)\", r\"\\1 \\2\", processed_text) # Punctuation followed by word\n",
    "\n",
    "\n",
    "    # 9. Reduce repeated punctuation characters (e.g., \"!!!\" to \"!\")\n",
    "    # This should be done AFTER spacing normalization if we want \"!!! \" -> \"! \"\n",
    "    # The previous regex for repeated special chars might have handled some of this,\n",
    "    # but this is more specific to string.punctuation.\n",
    "    processed_text = re.sub(r\"([\" + escaped_punctuation + r\"])\\1+\", r\"\\1\", processed_text)\n",
    "\n",
    "\n",
    "    # 10. Vietnamese tone mark normalization\n",
    "    processed_text = chuan_hoa_dau_cau_tieng_viet(processed_text)\n",
    "\n",
    "    # 11. Remove all punctuation (optional)\n",
    "    if remove_all_punctuation:\n",
    "        # Create a translation table that maps all punctuation to None\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        processed_text = processed_text.translate(translator)\n",
    "\n",
    "    # 12. Final whitespace cleanup (multiple spaces to single, strip ends)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "    \n",
    "    # 13. Strip leading/trailing punctuation or space robustly (if not all removed)\n",
    "    if not remove_all_punctuation and processed_text: # Check if processed_text is not empty\n",
    "        # Characters to strip: all punctuation and whitespace\n",
    "        strip_chars = string.punctuation + string.whitespace\n",
    "        # Strip from the end\n",
    "        while processed_text and processed_text[-1] in strip_chars:\n",
    "            processed_text = processed_text[:-1]\n",
    "        # Strip from the beginning\n",
    "        while processed_text and processed_text[0] in strip_chars:\n",
    "            processed_text = processed_text[1:]\n",
    "    \n",
    "    # If all processing results in an empty string, return it.\n",
    "    if not processed_text:\n",
    "        return \"\"\n",
    "\n",
    "    # 14. Tokenization using underthesea\n",
    "    tokens = word_tokenize(processed_text, format=\"list\") # Get list of tokens\n",
    "\n",
    "    # 15. Teencode Replacement (on tokens)\n",
    "    if use_teencode and current_teencode_map:\n",
    "        new_tokens = []\n",
    "        for token in tokens:\n",
    "            replacement = current_teencode_map.get(token, token)\n",
    "            # If teencode replacement results in multiple words, split them.\n",
    "            # This requires re-tokenization or careful splitting.\n",
    "            # For simplicity, we assume teencode maps to single words or phrases\n",
    "            # that underthesea can handle if joined and re-tokenized.\n",
    "            new_tokens.append(replacement)\n",
    "        \n",
    "        # If teencode replacement might introduce multi-word strings, re-tokenize.\n",
    "        # This ensures that phrases from teencode are properly tokenized.\n",
    "        if any(\" \" in t for t in new_tokens): # Check if any token now contains a space\n",
    "            temp_token_string = \" \".join(new_tokens)\n",
    "            tokens = word_tokenize(temp_token_string, format=\"list\")\n",
    "        else:\n",
    "            tokens = new_tokens\n",
    "\n",
    "\n",
    "    # 16. Stopword Removal (on tokens)\n",
    "    if use_stopwords and current_stopwords_list:\n",
    "        tokens = [token for token in tokens if token not in current_stopwords_list and token.strip()]\n",
    "\n",
    "    # 17. Join tokens to form the final processed string\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Xiiiiiiin ch√†ooooo, b·∫°n kho·∫ªee kh√¥ngggg ??? Tui l√†   H√πng n√®... gh√© thƒÉm web https://example.com ch∆°i nhaaa :)))) üòÇüòÇüòÇ #VuiVe'\n",
      "Processed: 'xin ch√†o , b·∫°n kh·ªèee kh√¥ng ? tui l√† h√πng n√® . gh√© thƒÉm web ch∆°i nha'\n",
      "\n",
      "Original: 'Tr·ªùi ∆°i CLM, ƒëC k h√ông? M√†y b·ªã ƒëI√™n √†????'\n",
      "Processed: 'tr·ªùi ∆°i c√°i l·ªìn m√° , ƒë∆∞·ª£c k h√πng ? m√†y b·ªã ƒëi√™n √†'\n",
      "\n",
      "Original: 'Shop b√°n ƒë·ªì ƒë·∫Øt q√∫a ƒëi, hok c√≥ ti·ªÅn mua ƒë√¢uuuu :('\n",
      "Processed (no stopwords): 'shop b√°n ƒë·ªì ƒë·∫Øt qu√° ƒëi , kh√¥ng c√≥ ti·ªÅn mua ƒë√¢u'\n",
      "\n",
      "Original: 'Tuy·ªát v·ªùi!!! s·∫£n ph·∫©m n√†y r·∫•t t·ªëtttt. highly recommend nha mn. k ch√™ v√†o ƒë√¢u ƒëc. @user123'\n",
      "Processed (all punct removed): 'tuy·ªát v·ªùi s·∫£n ph·∫©m n√†y r·∫•t t·ªët highly recomend nha m·ªçi ng∆∞·ªùi k ch√™ v√†o ƒë√¢u ƒë∆∞·ª£c'\n",
      "\n",
      "Original: 'cu·ªôc s·ªëng b·∫•t c√¥ng bi·∫øt m·∫•y cho v·ª´ai ƒë·ªãnh nghƒ©a th·∫ø gian n√†y mu√¥n m·∫∑t'\n",
      "Processed: 'cu·ªôc s·ªëng b·∫•t c√¥ng bi·∫øt m·∫•y cho v∆∞√†i ƒë·ªãnh nghƒ©a th·∫ø gian n√†y mu√¥n m·∫∑t'\n",
      "\n",
      "Original: 'x·∫°o cho√° qu√° anh'\n",
      "Processed: 'x·∫°o ch√≥a qu√° anh'\n",
      "\n",
      "Original: 'anh ho√†, ƒëang l√†m.. g√¨'\n",
      "Processed: 'anh h√≤a , ƒëang l√†m . gi'\n",
      "\n",
      "Original: 'ƒêi ƒë√¢u ƒë√≥ b·∫°n ∆°i????? kkkk :D :D'\n",
      "Processed: 'ƒëi ƒë√¢u ƒë√≥ b·∫°n ∆°i ? k : d : d'\n",
      "\n",
      "Original (no teencode/stopwords): 'Xiiiiiiin ch√†ooooo, b·∫°n kho·∫ªee kh√¥ngggg ??? Tui l√†   H√πng n√®... gh√© thƒÉm web https://example.com ch∆°i nhaaa :)))) üòÇüòÇüòÇ #VuiVe'\n",
      "Processed (no teencode/stopwords): 'xin ch√†o , b·∫°n kh·ªèee kh√¥ng ? tui l√† h√πng n√® . gh√© thƒÉm web ch∆°i nha'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "\n",
    "sample_text_1 = \"Xiiiiiiin ch√†ooooo, b·∫°n kho·∫ªee kh√¥ngggg ??? Tui l√†   H√πng n√®... gh√© thƒÉm web https://example.com ch∆°i nhaaa :)))) üòÇüòÇüòÇ #VuiVe\"\n",
    "sample_text_2 = \"Tr·ªùi ∆°i CLM, ƒëC k h√ông? M√†y b·ªã ƒëI√™n √†????\"\n",
    "sample_text_3 = \"Shop b√°n ƒë·ªì ƒë·∫Øt q√∫a ƒëi, hok c√≥ ti·ªÅn mua ƒë√¢uuuu :(\"\n",
    "sample_text_4 = \"Tuy·ªát v·ªùi!!! s·∫£n ph·∫©m n√†y r·∫•t t·ªëtttt. highly recommend nha mn. k ch√™ v√†o ƒë√¢u ƒëc. @user123\"\n",
    "sample_text_5 = \"cu·ªôc s·ªëng b·∫•t c√¥ng bi·∫øt m·∫•y cho v·ª´ai ƒë·ªãnh nghƒ©a th·∫ø gian n√†y mu√¥n m·∫∑t\"\n",
    "sample_text_6 = \"x·∫°o cho√° qu√° anh\" # from notebook (accent issue) -> x·∫°o ch√≥a qu√° anh\n",
    "sample_text_7 = \"anh ho√†, ƒëang l√†m.. g√¨\" # from notebook (accent issue) -> anh h√≤a, ƒëang l√†m. g√¨\n",
    "sample_text_8 = \"ƒêi ƒë√¢u ƒë√≥ b·∫°n ∆°i????? kkkk :D :D\"\n",
    "\n",
    "print(f\"Original: '{sample_text_1}'\")\n",
    "processed_1 = preprocess_vietnamese_text_underthesea(sample_text_1)\n",
    "print(f\"Processed: '{processed_1}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_2}'\")\n",
    "processed_2 = preprocess_vietnamese_text_underthesea(sample_text_2)\n",
    "print(f\"Processed: '{processed_2}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_3}'\")\n",
    "processed_3 = preprocess_vietnamese_text_underthesea(sample_text_3, use_stopwords=False)\n",
    "print(f\"Processed (no stopwords): '{processed_3}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_4}'\")\n",
    "processed_4 = preprocess_vietnamese_text_underthesea(sample_text_4, remove_all_punctuation=True)\n",
    "print(f\"Processed (all punct removed): '{processed_4}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_5}'\")\n",
    "processed_5 = preprocess_vietnamese_text_underthesea(sample_text_5)\n",
    "print(f\"Processed: '{processed_5}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_6}'\")\n",
    "processed_6 = preprocess_vietnamese_text_underthesea(sample_text_6)\n",
    "print(f\"Processed: '{processed_6}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_7}'\")\n",
    "processed_7 = preprocess_vietnamese_text_underthesea(sample_text_7)\n",
    "print(f\"Processed: '{processed_7}'\\n\")\n",
    "\n",
    "print(f\"Original: '{sample_text_8}'\")\n",
    "processed_8 = preprocess_vietnamese_text_underthesea(sample_text_8)\n",
    "print(f\"Processed: '{processed_8}'\\n\")\n",
    "\n",
    "print(f\"Original (no teencode/stopwords): '{sample_text_1}'\")\n",
    "processed_9 = preprocess_vietnamese_text_underthesea(sample_text_1, use_teencode=False, use_stopwords=False)\n",
    "print(f\"Processed (no teencode/stopwords): '{processed_9}'\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 'Hello World qu√° l√† ƒë·∫πp lunnn √°'\n",
      "Processed: 'helo world qu√° l√† ƒë·∫πp lun √°'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_vi_en = \"Hello World qu√° l√† ƒë·∫πp lunnn √°\"\n",
    "print(f\"Original: '{sample_vi_en}'\")\n",
    "processed_vi_en = preprocess_vietnamese_text_underthesea(sample_vi_en)\n",
    "print(f\"Processed: '{processed_vi_en}'\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vifree-txt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
