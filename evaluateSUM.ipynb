{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from underthesea import sent_tokenize as underthesea_sent_tokenize # Renamed to avoid conflict\n",
    "from summa.summarizer import summarize as summa_summarizer\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "import emoji # For preprocessing if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'Document', 'Summary', 'Dataset'],\n",
      "        num_rows: 74564\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    dataset = load_dataset(\"OpenHust/vietnamese-summarization\")\n",
    "    print(\"Dataset loaded successfully:\")\n",
    "    print(dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "\n",
    "# You can inspect the dataset structure, e.g., print the first example from the training set\n",
    "# if 'train' in dataset:\n",
    "#     print(\"\\nExample from training set:\")\n",
    "#     print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'Document', 'Summary', 'Dataset'],\n",
       "    num_rows: 74564\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_summarize(article, ratio):\n",
    "    \"\"\"\n",
    "    Summarize the given article using the TextRank algorithm.\n",
    "\n",
    "    Parameters:\n",
    "    article (str): The text to be summarized.\n",
    "    ratio (float): A number between 0 and 1 that determines the proportion of the number of sentences in the original text to be included in the summary.\n",
    "\n",
    "    Returns:\n",
    "    str: The summarized text.\n",
    "    \"\"\"\n",
    "    # Summa's summarizer works best with text that has clear sentence boundaries.\n",
    "    # It uses NLTK's sent_tokenize internally by default for English.\n",
    "    # For Vietnamese, ensure sentences are well-separated (e.g., by periods).\n",
    "    return summa_summarizer(article, ratio=ratio, language='english') # Summa might not have explicit Vietnamese support, relies on sentence splitting. NLTK's punkt can handle VI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hà Nội, thủ đô của Việt Nam, là một thành phố cổ kính với hơn một nghìn năm lịch sử.\n",
      "Nơi đây nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và một nền văn hóa ẩm thực đường phố phong phú.\n",
      "Hồ Hoàn Kiếm, trái tim của thành phố, mang trong mình những truyền thuyết lịch sử và là nơi người dân địa phương tụ tập vui chơi.\n",
      "Ẩm thực Hà Nội cũng là một điểm nhấn đặc biệt, với các món ăn như phở, bún chả, và cà phê trứng đã trở thành biểu tượng.\n"
     ]
    }
   ],
   "source": [
    "from summa.summarizer import summarize\n",
    "\n",
    "example_long_text_vi = \"\"\"\n",
    "Hà Nội, thủ đô của Việt Nam, là một thành phố cổ kính với hơn một nghìn năm lịch sử. \n",
    "Nơi đây nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và một nền văn hóa ẩm thực đường phố phong phú. \n",
    "Du khách đến Hà Nội thường bị cuốn hút bởi sự pha trộn giữa nét cổ điển và hiện đại. \n",
    "Một trong những điểm đến không thể bỏ qua là Văn Miếu - Quốc Tử Giám, trường đại học đầu tiên của Việt Nam. \n",
    "Hồ Hoàn Kiếm, trái tim của thành phố, mang trong mình những truyền thuyết lịch sử và là nơi người dân địa phương tụ tập vui chơi. \n",
    "Ẩm thực Hà Nội cũng là một điểm nhấn đặc biệt, với các món ăn như phở, bún chả, và cà phê trứng đã trở thành biểu tượng. \n",
    "Tuy nhiên, thành phố cũng đối mặt với những thách thức như tắc nghẽn giao thông và ô nhiễm không khí. \n",
    "Chính quyền thành phố đang nỗ lực cải thiện cơ sở hạ tầng và nâng cao chất lượng cuộc sống cho người dân. \n",
    "Nhìn chung, Hà Nội vẫn là một điểm đến hấp dẫn với nhiều điều thú vị để khám phá.\n",
    "\"\"\"\n",
    "summary = summarize(example_long_text_vi, ratio=0.5)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extractive_summarizer(input_text, chosen_ratio):\n",
    "    \"\"\"\n",
    "    Summarizes the input text using the TextRank algorithm, with a fallback to the first sentence if the summary is empty.\n",
    "\n",
    "    Parameters:\n",
    "    input_text (str or iterable): The text to be summarized. If it's not a string, it will be converted to one.\n",
    "    chosen_ratio (float): A number between 0 and 1 that determines the proportion of the original text to be included in the summary. It's capped at a minimum of 0.05.\n",
    "\n",
    "    Returns:\n",
    "    str: The summary of the input text or the first sentence if the summary is empty.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_text, str):\n",
    "        input_text = ' '.join(map(str, input_text))\n",
    "    \n",
    "    chosen_ratio = max(chosen_ratio, 0.05) # Ensure a minimum ratio\n",
    "\n",
    "    summary = text_rank_summarize(input_text, ratio=chosen_ratio)\n",
    "    \n",
    "    # Fallback if summary is empty\n",
    "    if summary:\n",
    "        return summary\n",
    "    else:\n",
    "        # Using underthesea for more robust Vietnamese sentence tokenization for fallback\n",
    "        sentences = underthesea_sent_tokenize(input_text)\n",
    "        if sentences:\n",
    "            return sentences[0]\n",
    "        else:\n",
    "            return \"Unable to summarize the input text or extract the first sentence.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hà Nội, thủ đô của Việt Nam, là một thành phố cổ kính với hơn một nghìn năm lịch sử.\\nNơi đây nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và một nền văn hóa ẩm thực đường phố phong phú.\\nHồ Hoàn Kiếm, trái tim của thành phố, mang trong mình những truyền thuyết lịch sử và là nơi người dân địa phương tụ tập vui chơi.\\nẨm thực Hà Nội cũng là một điểm nhấn đặc biệt, với các món ăn như phở, bún chả, và cà phê trứng đã trở thành biểu tượng.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_extractive_summarizer(example_long_text_vi,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TextRank Summarization Demo ---\n",
      "Original Article (length: 528 chars):\n",
      "\n",
      "Hà Nội mùa thu, cây cơm nguội vàng, cây bàng lá đỏ, nằm kề bên nhau, phố xưa nhà cổ, mái ngói thâm nâu. \n",
      "Hồ Gươm, Tháp Rùa, nghiêng soi bóng, nước xanh như lọc. \n",
      "Những con đường ngoại ô thành phố vẫn giữ được vẻ đẹp bình dị với những cánh đồng lúa xanh mướt và những đàn cò trắng bay lượn. \n",
      "Người dân Hà Nội nổi tiếng với sự thanh lịch, mến khách và tài hoa trong ẩm thực cũng như các nghề thủ công truyền thống. \n",
      "Mỗi độ thu về, Hà Nội lại khoác lên mình một chiếc áo mới, lãng mạn và đầy chất thơ, níu chân biết bao du khách.\n",
      "\n",
      "\n",
      "TextRank Summary (ratio: 0.4, length: 234 chars):\n",
      "Người dân Hà Nội nổi tiếng với sự thanh lịch, mến khách và tài hoa trong ẩm thực cũng như các nghề thủ công truyền thống.\n",
      "Mỗi độ thu về, Hà Nội lại khoác lên mình một chiếc áo mới, lãng mạn và đầy chất thơ, níu chân biết bao du khách.\n"
     ]
    }
   ],
   "source": [
    "sample_vietnamese_article_for_textrank = \"\"\"\n",
    "Hà Nội mùa thu, cây cơm nguội vàng, cây bàng lá đỏ, nằm kề bên nhau, phố xưa nhà cổ, mái ngói thâm nâu. \n",
    "Hồ Gươm, Tháp Rùa, nghiêng soi bóng, nước xanh như lọc. \n",
    "Những con đường ngoại ô thành phố vẫn giữ được vẻ đẹp bình dị với những cánh đồng lúa xanh mướt và những đàn cò trắng bay lượn. \n",
    "Người dân Hà Nội nổi tiếng với sự thanh lịch, mến khách và tài hoa trong ẩm thực cũng như các nghề thủ công truyền thống. \n",
    "Mỗi độ thu về, Hà Nội lại khoác lên mình một chiếc áo mới, lãng mạn và đầy chất thơ, níu chân biết bao du khách.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- TextRank Summarization Demo ---\")\n",
    "print(f\"Original Article (length: {len(sample_vietnamese_article_for_textrank)} chars):\\n{sample_vietnamese_article_for_textrank}\")\n",
    "\n",
    "summary_ratio = 0.4 # Summarize to 40% of original sentences\n",
    "textrank_summary = run_extractive_summarizer(sample_vietnamese_article_for_textrank, summary_ratio)\n",
    "print(f\"\\nTextRank Summary (ratio: {summary_ratio}, length: {len(textrank_summary)} chars):\\n{textrank_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model: VietAI/vit5-base-vietnews-summarization\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME_SUMM = \"VietAI/vit5-base-vietnews-summarization\"\n",
    "\n",
    "try:\n",
    "    tokenizer_summ = AutoTokenizer.from_pretrained(MODEL_NAME_SUMM)\n",
    "    model_summ = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_SUMM)\n",
    "    model_summ.to(device)\n",
    "    model_summ.eval()\n",
    "    print(f\"Successfully loaded model: {MODEL_NAME_SUMM}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading summarization model {MODEL_NAME_SUMM}: {e}\")\n",
    "    tokenizer_summ = None\n",
    "    model_summ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Đây là một trong những nội dung tại văn bản vừa được UBND TP Hà Nội ban hành về việc tăng cường công tác quản lý nuôi , giết mổ , kinh doanh và sử dụng thịt chó , mèo trên địa bàn .Theo đó , các sở , ngành trên địa bàn phải vào cuộc ngay để hướng tới thay đổi thói quen của người dân khi dùng chó , mèo làm thực phẩm .Gây phản cảm với du khách , người nước ngoàiCũng trong văn bản này , UBND TP Hà Nội thừa nhận rằng việc kinh doanh , giết mổ và sử dụng thịt chó , mèo tại Hà Nội thời gian qua đã tạo ra những hình ảnh phản cảm đối với khách tham quan du lịch , người nước ngoài đang sinh sống và làm việc tại Hà Nội , gây ảnh hưởng tới hình ảnh của một thủ đô \" văn minh , hiện đại \" .Trong thực tế , theo ghi nhận của Tuổi Trẻ tại phố Tam Trinh , ngay đoạn đầu cầu Mai Động ( quận Hoàng Mai ) , một đoạn phố dài với gần chục cửa hàng buôn bán thịt chó sống , nhà hàng phục vụ các món chế biến từ thịt chó vẫn hoạt động tấp nập nhiều năm nay .Chị Th . , một chủ sạp bán thịt chó sống ở đây , cho hay trong 10 ngày đầu mỗi tháng âm lịch , các cửa hàng vắng vẻ hơn vì thói quen \" kiêng \" ăn thịt chó vào đầu tháng tránh xui xẻo .Riêng những ngày cuối tháng , các cửa hàng ở đây tấp nập người mua bán .Theo chị Th . , trung bình mỗi tháng chị bán được khoảng 3 tạ thịt chó sống .Tương tự , tại phố Nguyễn Khang ( quận Cầu Giấy ) , hàng loạt nhà hàng thịt chó quy mô lớn ( 1-5 tầng ) như Sơn Hải , Chiếu Hoa , Mơ Hoa quán ... luôn tấp nập , mỗi ngày tiếp cả nghìn thực khách .Tuy nhiên , hàng chục quán thịt chó trên đường Âu Cơ ( phường Nhật Tân , quận Tây Hồ ) từng nổi tiếng một thời như Trần Mục , Hồ Kiếm , A Trang , Anh Tú Xịn , Anh Tú Nhà Kính , Anh Tú Nhà Lá ... đã đóng cửa không rõ lý do , chỉ còn duy nhất cửa hàng có tên Anh Tú Béo .Nhiều nguy cơ lây lan dịch bệnhTheo chuyên gia truyền thông Phan Kiền , chó không phải là loài động vật nằm trong danh mục cấm săn bắt , mua bán và giết mổ , thịt chó thương phẩm cũng không phải là mặt hàng thực phẩm cấm lưu hành nên rất khó dùng luật để cấm .Do đó , việc Hà Nội khuyến khích người dân hạn chế , tiến tới không ăn thịt chó là cách làm vừa văn minh vừađúng luật .\" Hoàn toàn có thể thay đổi một thói quen xấu nếu được truyền thông tốt , mà việc bỏ ăn tiết canh là một ví dụ .Trước đây các hàng quán \" lòng lợn , tiết canh \" mọc khắp nơi bởi đây là món khoái khẩu của dân nhậu .Nhưng khi báo chí đưa tin nhiều về các ca nhiễm khuẩn cầu lợn gây tử vong với những triệu chứng khủng khiếp , gần như món ăn này bị tẩy chay , hiện rất ít người dùng món ăn này \" - vị này nói .Trong khi đó , không chỉ những bạn trẻ nuôi chó cảnh làm thú cưng phản đối việc xem chó là một loại thực phẩm , mà ngay cả những người từng xem thịt chó là món \" khoái khẩu \" cũng ủng hộ việc bỏ ăn thịt chó .Dù từng ăn thịt chó , anh Nguyễn Văn Thuận , một nhân viên văn phòng ở quận Thanh Xuân , cũng ủng hộ chủ trương của Hà Nội .Theo anh Thuận , một khi thịt chó còn thịnh hành và có giá trị thương phẩm cao , nạn trộm chó vẫn tồn tại và khó tránh khỏi cảnh người trộm chó bị đánh \" thừa sống thiếu chết \" như đã từng xảy ra .Ngoài ra nguy cơ lây lan dịch bệnh , đặc biệt là bệnh dại , do hoạt động buôn bán và vận chuyển chó sống giữa các địa phương cũng là vấn đề đáng lo ngại .Do đó , muốn hạn chế nạn trộm chó và ngăn ngừa nguy cơ lây lan dịch bệnh , cần khuyến khích không nên ăn thịt chó .\" Thịt chó cũng ngon và lạ miệng nhưng nếu cấm ăn thịt chó , mình thấy cũng không vấn đề gì vì còn rất nhiều loại thực phẩm khác thay thế \" - anh Thuận nói .'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = dataset[0]['Document']\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Các quận , huyện , thị xã tuyên truyền bằng nhiều hình thức để người dân từ bỏ thói quen ăn thịt chó , mèo nhằm tránh nguy cơ mắc bệnh truyền nhiễm ( bệnh dại , xoắn khuẩn , tả ... ) cũng như không gây ảnh hưởng tới hình ảnh thủ đô văn minh , hiện đại .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum = dataset[0]['Summary']\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sau khi báo chí đưa tin nhiều về các ca nhiễm khuẩn cầu lợn gây chết hàng loạt, gần 100 nhà hàng thịt chó ở Hà Nội đã đóng cửa không rõ lý do, chỉ còn duy nhất một cửa hàng bán thịt chó duy nhất ở quận Cầu Giấy và quận Hà Đông - nơi trước đây từng được coi là \" thánh địa \" của du lịch - đã bày tỏ quan điểm chấp nhận việc bỏ thói quen này để tránh gây phản cảm với du khách nước ngoài.Ựcả người dân thủ đô đều lên tiếng phản đối.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sum = abstractive_summarize_vietnamese(doc, model_summ, tokenizer_summ)\n",
    "doc_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đây là một trong những nội dung tại văn bản vừa được UBND TP Hà Nội ban hành về việc tăng cường công tác quản lý nuôi , giết mổ , kinh doanh và sử dụng thịt chó , mèo trên địa bàn . Theo đó , các sở , ngành trên địa bàn phải vào cuộc ngay để hướng tới thay đổi thói quen của người dân khi dùng chó , mèo làm thực phẩm . Gây phản cảm với du khách , người nước ngoàiCũng trong văn bản này , UBND TP Hà Nội thừa nhận rằng việc kinh doanh , giết mổ và sử dụng thịt chó , mèo tại Hà Nội thời gian qua đã tạo ra những hình ảnh phản cảm đối với khách tham quan du lịch , người nước ngoài đang sinh sống và làm việc tại Hà Nội , gây ảnh hưởng tới hình ảnh của một thủ đô \" văn minh , hiện đại \" . Trong thực tế , theo ghi nhận của Tuổi Trẻ tại phố Tam Trinh , ngay đoạn đầu cầu Mai Động ( quận Hoàng Mai ) , một đoạn phố dài với gần chục cửa hàng buôn bán thịt chó sống , nhà hàng phục vụ các món chế biến từ thịt chó vẫn hoạt động tấp nập nhiều năm nay . Chị Th . , một chủ sạp bán thịt chó sống ở đây , cho hay trong 10 ngày đầu mỗi tháng âm lịch , các cửa hàng vắng vẻ hơn vì thói quen \" kiêng \" ăn thịt chó vào đầu tháng tránh xui xẻo . Riêng những ngày cuối tháng , các cửa hàng ở đây tấp nập người mua bán . Theo chị Th . , trung bình mỗi tháng chị bán được khoảng 3 tạ thịt chó sống . Tương tự , tại phố Nguyễn Khang ( quận Cầu Giấy ) , hàng loạt nhà hàng thịt chó quy mô lớn ( 1-5 tầng ) như Sơn Hải , Chiếu Hoa , Mơ Hoa quán . luôn tấp nập , mỗi ngày tiếp cả nghìn thực khách . Tuy nhiên , hàng chục quán thịt chó trên đường Âu Cơ ( phường Nhật Tân , quận Tây Hồ ) từng nổi tiếng một thời như Trần Mục , Hồ Kiếm , A Trang , Anh Tú Xịn , Anh Tú Nhà Kính , Anh Tú Nhà Lá . đã đóng cửa không rõ lý do , chỉ còn duy nhất cửa hàng có tên Anh Tú Béo . Nhiều nguy cơ lây lan dịch bệnhTheo chuyên gia truyền thông Phan Kiền , chó không phải là loài động vật nằm trong danh mục cấm săn bắt , mua bán và giết mổ , thịt chó thương phẩm cũng không phải là mặt hàng thực phẩm cấm lưu hành nên rất khó dùng luật để cấm . Do đó , việc Hà Nội khuyến khích người dân hạn chế , tiến tới không ăn thịt chó là cách làm vừa văn minh vừađúng luật .\" Hoàn toàn có thể thay đổi một thói quen xấu nếu được truyền thông tốt , mà việc bỏ ăn tiết canh là một ví dụ . Trước đây các hàng quán \" lòng lợn , tiết canh \" mọc khắp nơi bởi đây là món khoái khẩu của dân nhậu . Nhưng khi báo chí đưa tin nhiều về các ca nhiễm khuẩn cầu lợn gây tử vong với những triệu chứng khủng khiếp , gần như món ăn này bị tẩy chay , hiện rất ít người dùng món ăn này \" - vị này nói . Trong khi đó , không chỉ những bạn trẻ nuôi chó cảnh làm thú cưng phản đối việc xem chó là một loại thực phẩm , mà ngay cả những người từng xem thịt chó là món \" khoái khẩu \" cũng ủng hộ việc bỏ ăn thịt chó . Dù từng ăn thịt chó , anh Nguyễn Văn Thuận , một nhân viên văn phòng ở quận Thanh Xuân , cũng ủng hộ chủ trương của Hà Nội . Theo anh Thuận , một khi thịt chó còn thịnh hành và có giá trị thương phẩm cao , nạn trộm chó vẫn tồn tại và khó tránh khỏi cảnh người trộm chó bị đánh \" thừa sống thiếu chết \" như đã từng xảy ra . Ngoài ra nguy cơ lây lan dịch bệnh , đặc biệt là bệnh dại , do hoạt động buôn bán và vận chuyển chó sống giữa các địa phương cũng là vấn đề đáng lo ngại . Do đó , muốn hạn chế nạn trộm chó và ngăn ngừa nguy cơ lây lan dịch bệnh , cần khuyến khích không nên ăn thịt chó .\" Thịt chó cũng ngon và lạ miệng nhưng nếu cấm ăn thịt chó , mình thấy cũng không vấn đề gi vì còn rất nhiều loại thực phẩm khác thay thế \" - anh Thuận nói .\n",
      "Sau khi báo chí đưa tin nhiều về các ca nhiễm khuẩn cầu lợn gây chết hàng loạt, gần 100 nhà hàng thịt chó ở Hà Nội đã đóng cửa không rõ lý do, chỉ còn duy nhất một cửa hàng bán thịt chó duy nhất ở quận Cầu Giấy và quận Hà Đông - nơi trước đây từng được coi là \" thánh địa \" của du lịch - đã bày tỏ quan điểm chấp nhận việc bỏ thói quen này để tránh gây phản cảm với du khách nước ngoài.Ựcả người dân thủ đô đều lên tiếng phản đối.\n"
     ]
    }
   ],
   "source": [
    "predoc = preprocess_text_for_abstractive_summarization(doc)\n",
    "print(predoc)\n",
    "pre_doc_sum = abstractive_summarize_vietnamese(predoc, model_summ, tokenizer_summ)\n",
    "print(pre_doc_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "Reference Summary: Các quận , huyện , thị xã tuyên truyền bằng nhiều hình thức để người dân từ bỏ thói quen ăn thịt chó , mèo nhằm tránh nguy cơ mắc bệnh truyền nhiễm ( bệnh dại , xoắn khuẩn , tả ... ) cũng như không gây ảnh hưởng tới hình ảnh thủ đô văn minh , hiện đại .\n",
      "Generated Summary: Sau khi báo chí đưa tin nhiều về các ca nhiễm khuẩn cầu lợn gây chết hàng loạt, gần 100 nhà hàng thịt chó ở Hà Nội đã đóng cửa không rõ lý do, chỉ còn duy nhất một cửa hàng bán thịt chó duy nhất ở quận Cầu Giấy và quận Hà Đông - nơi trước đây từng được coi là \" thánh địa \" của du lịch - đã bày tỏ quan điểm chấp nhận việc bỏ thói quen này để tránh gây phản cảm với du khách nước ngoài.Ựcả người dân thủ đô đều lên tiếng phản đối.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.94k/5.94k [00:00<00:00, 36.4kB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 6.30MB/s]                   \n",
      "Downloading extra modules: 100%|██████████| 3.34k/3.34k [00:00<00:00, 9.75MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.0000\n",
      "ROUGE-1: 0.5903\n",
      "ROUGE-2: 0.2311\n",
      "ROUGE-L: 0.3084\n",
      "ROUGE-Lsum: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 7.02k/7.02k [00:00<00:00, 30.3MB/s]\n",
      "[nltk_data] Downloading package wordnet to /home/elo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/elo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/elo/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.2540\n",
      "BERTScore Precision: 0.6741\n",
      "BERTScore Recall: 0.6934\n",
      "BERTScore F1: 0.6836\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics between reference summary (sum) and generated summary (doc_sum)\n",
    "import evaluate\n",
    "\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(f\"Reference Summary: {sum}\")\n",
    "print(f\"Generated Summary: {doc_sum}\")\n",
    "print()\n",
    "\n",
    "# BLEU Score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu_metric.compute(predictions=[doc_sum], references=[[sum]])\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "# ROUGE Scores\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "rouge_scores = rouge_metric.compute(predictions=[doc_sum], references=[sum])\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}\")\n",
    "\n",
    "# METEOR Score\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "meteor_score = meteor_metric.compute(predictions=[doc_sum], references=[sum])\n",
    "print(f\"METEOR Score: {meteor_score['meteor']:.4f}\")\n",
    "\n",
    "# BERTScore\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "bertscore_results = bertscore_metric.compute(predictions=[doc_sum], references=[sum], lang=\"vi\")\n",
    "print(f\"BERTScore Precision: {bertscore_results['precision'][0]:.4f}\")\n",
    "print(f\"BERTScore Recall: {bertscore_results['recall'][0]:.4f}\")\n",
    "print(f\"BERTScore F1: {bertscore_results['f1'][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics ---\n",
      "Reference Summary: Các quận , huyện , thị xã tuyên truyền bằng nhiều hình thức để người dân từ bỏ thói quen ăn thịt chó , mèo nhằm tránh nguy cơ mắc bệnh truyền nhiễm ( bệnh dại , xoắn khuẩn , tả ... ) cũng như không gây ảnh hưởng tới hình ảnh thủ đô văn minh , hiện đại .\n",
      "Generated Summary: Sau khi báo chí đưa tin nhiều về các ca nhiễm khuẩn cầu lợn gây chết hàng loạt, gần 100 nhà hàng thịt chó ở Hà Nội đã đóng cửa không rõ lý do, chỉ còn duy nhất một cửa hàng bán thịt chó duy nhất ở quận Cầu Giấy và quận Hà Đông - nơi trước đây từng được coi là \" thánh địa \" của du lịch - đã bày tỏ quan điểm chấp nhận việc bỏ thói quen này để tránh gây phản cảm với du khách nước ngoài.Ựcả người dân thủ đô đều lên tiếng phản đối.\n",
      "\n",
      "BLEU Score: 0.0000\n",
      "ROUGE-1: 0.5903\n",
      "ROUGE-2: 0.2311\n",
      "ROUGE-L: 0.3084\n",
      "ROUGE-Lsum: 0.3084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/elo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/elo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/elo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.2540\n",
      "BERTScore Precision: 0.6741\n",
      "BERTScore Recall: 0.6934\n",
      "BERTScore F1: 0.6836\n"
     ]
    }
   ],
   "source": [
    "# Evaluate metrics between reference summary (sum) and generated summary (doc_sum)\n",
    "import evaluate\n",
    "\n",
    "print(\"--- Evaluation Metrics ---\")\n",
    "print(f\"Reference Summary: {sum}\")\n",
    "print(f\"Generated Summary: {pre_doc_sum}\")\n",
    "print()\n",
    "\n",
    "# BLEU Score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu_metric.compute(predictions=[pre_doc_sum], references=[[sum]])\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "\n",
    "# ROUGE Scores\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "rouge_scores = rouge_metric.compute(predictions=[pre_doc_sum], references=[sum])\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}\")\n",
    "\n",
    "# METEOR Score\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "meteor_score = meteor_metric.compute(predictions=[pre_doc_sum], references=[sum])\n",
    "print(f\"METEOR Score: {meteor_score['meteor']:.4f}\")\n",
    "\n",
    "# BERTScore\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "bertscore_results = bertscore_metric.compute(predictions=[pre_doc_sum], references=[sum], lang=\"vi\")\n",
    "print(f\"BERTScore Precision: {bertscore_results['precision'][0]:.4f}\")\n",
    "print(f\"BERTScore Recall: {bertscore_results['recall'][0]:.4f}\")\n",
    "print(f\"BERTScore F1: {bertscore_results['f1'][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "# Assuming 'dicchar', 'emoji_pattern', 'bang_nguyen_am', 'nguyen_am_to_ids'\n",
    "# and 'chuan_hoa_dau_tu_tieng_viet' and its helpers are defined as in your SA script.\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    u\"\\U0001f926-\\U0001f937\"\n",
    "    u\"\\U00010000-\\U0010ffff\"\n",
    "    u\"\\u200d\"\n",
    "    u\"\\u2640-\\u2642\"\n",
    "    u\"\\u2600-\\u2B55\"\n",
    "    u\"\\u23cf\"\n",
    "    u\"\\u23e9\"\n",
    "    u\"\\u231a\"\n",
    "    u\"\\u3030\"\n",
    "    u\"\\ufe0f\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
    "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
    "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
    "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
    "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
    "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
    "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
    "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
    "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
    "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
    "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
    "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
    "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
    "\n",
    "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
    "    words = sentence.split()\n",
    "    for index, word in enumerate(words):\n",
    "        # Preserve surrounding punctuation by splitting and processing only the word part\n",
    "        match = re.match(r'(^[\\W_]*)([\\wÀ-Ỹà-ỹ._]*[\\wÀ-Ỹà-ỹ]+)([\\W_]*$)', word)\n",
    "        if match:\n",
    "            prefix, core_word, suffix = match.groups()\n",
    "            normalized_core_word = chuan_hoa_dau_tu_tieng_viet(core_word)\n",
    "            words[index] = prefix + normalized_core_word + suffix\n",
    "        else: # If word doesn't match (e.g. pure punctuation or malformed), try to normalize if it's a simple word\n",
    "            words[index] = chuan_hoa_dau_tu_tieng_viet(word) \n",
    "    return \" \".join(words)\n",
    "\n",
    "def is_valid_vietnam_word(word):\n",
    "    chars = list(word)\n",
    "    nguyen_am_index = -1\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x != -1:\n",
    "            if nguyen_am_index == -1:\n",
    "                nguyen_am_index = index\n",
    "            else:\n",
    "                if index - nguyen_am_index != 1:\n",
    "                    return False\n",
    "                nguyen_am_index = index\n",
    "    return True\n",
    "\n",
    "def chuan_hoa_dau_tu_tieng_viet(word):\n",
    "    if not is_valid_vietnam_word(word):\n",
    "        return word\n",
    "    chars = list(word)\n",
    "    dau_cau = 0\n",
    "    nguyen_am_index = []\n",
    "    qu_or_gi = False\n",
    "    for index, char in enumerate(chars):\n",
    "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
    "        if x == -1: continue\n",
    "        if x == 9: # u\n",
    "            if index > 0 and chars[index - 1].lower() == 'q':\n",
    "                chars[index] = 'u'; qu_or_gi = True\n",
    "        elif x == 5: # i\n",
    "            if index > 0 and chars[index - 1].lower() == 'g':\n",
    "                chars[index] = 'i'; qu_or_gi = True\n",
    "        if y != 0:\n",
    "            dau_cau = y; chars[index] = bang_nguyen_am[x][0]\n",
    "        if not qu_or_gi or index != 1: # Fix: check qu_or_gi correctly\n",
    "            nguyen_am_index.append(index)\n",
    "\n",
    "    if not nguyen_am_index: return \"\".join(chars)\n",
    "\n",
    "    # Determine which vowel to place the tone mark on\n",
    "    idx_to_mark = nguyen_am_index[0] # Default to the first vowel in the group\n",
    "    if len(nguyen_am_index) >= 2:\n",
    "        # Priority for ê, ơ, ô\n",
    "        priority_vowel_found = False\n",
    "        for idx_candidate in nguyen_am_index:\n",
    "            x_vowel, _ = nguyen_am_to_ids.get(chars[idx_candidate], (-1,-1))\n",
    "            if x_vowel in [4, 7, 8]: # ê, ô, ơ\n",
    "                idx_to_mark = idx_candidate\n",
    "                priority_vowel_found = True\n",
    "                break\n",
    "        \n",
    "        if not priority_vowel_found:\n",
    "            # Rules for diphthongs/triphthongs (simplified from original logic)\n",
    "            # If the vowel group is at the end of the word\n",
    "            if nguyen_am_index[-1] == len(chars) -1:\n",
    "                # If ends with i, u, y (closed vowels/semivowels), mark the vowel before it\n",
    "                x_last_vowel, _ = nguyen_am_to_ids.get(chars[nguyen_am_index[-1]], (-1,-1))\n",
    "                if x_last_vowel in [5, 9, 10, 11]: # i, u, ư, y\n",
    "                     idx_to_mark = nguyen_am_index[-2] if len(nguyen_am_index) > 1 else nguyen_am_index[-1]\n",
    "                else: # Otherwise, mark the first vowel of the group (e.g., 'oa', 'oe')\n",
    "                    idx_to_mark = nguyen_am_index[0]\n",
    "            else: # Vowel group is followed by consonants (e.g., 'uyen', 'oan')\n",
    "                if len(nguyen_am_index) == 3: # Triphthongs like 'uye', 'oai' -> mark middle\n",
    "                    idx_to_mark = nguyen_am_index[1]\n",
    "                elif len(nguyen_am_index) == 2: # Diphthongs like 'uyê', 'oa' -> mark second\n",
    "                    idx_to_mark = nguyen_am_index[1]\n",
    "                # else (single vowel before consonant), default (first vowel) is fine\n",
    "\n",
    "    # Apply the tone mark\n",
    "    x_target_vowel, _ = nguyen_am_to_ids.get(chars[idx_to_mark], (-1,-1))\n",
    "    if x_target_vowel != -1 and dau_cau != 0:\n",
    "        chars[idx_to_mark] = bang_nguyen_am[x_target_vowel][dau_cau]\n",
    "    return \"\".join(chars)\n",
    "\n",
    "nguyen_am_to_ids = {}\n",
    "for i in range(len(bang_nguyen_am)):\n",
    "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
    "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
    "\n",
    "def loaddicchar():\n",
    "    dic = {}\n",
    "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split('|')\n",
    "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split('|')\n",
    "    for i in range(len(char1252)):\n",
    "        dic[char1252[i]] = charutf8[i]\n",
    "    return dic\n",
    "dicchar = loaddicchar()\n",
    "\n",
    "def normalize_unicode_summ(text): # Specific to summarization\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def convert_unicode_legacy_summ(txt): # Specific to summarization\n",
    "    # Ensure dicchar is accessible here, e.g., passed as arg or global\n",
    "    # For simplicity, assuming it's global or accessible\n",
    "    return re.sub(\n",
    "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
    "        lambda x: dicchar[x.group()], txt)\n",
    "\n",
    "# (You'll need chuan_hoa_dau_cau_tieng_viet and its dependencies like\n",
    "#  is_valid_vietnam_word, nguyen_am_to_ids, bang_nguyen_am from your SA script)\n",
    "\n",
    "def preprocess_text_for_abstractive_summarization(text):\n",
    "    \"\"\"\n",
    "    Simplified and adapted preprocessing for abstractive summarization (e.g., ViT5).\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    # 1. Remove URLs, mentions, hashtags (optional, depends on dataset)\n",
    "    processed_text = re.sub(r\"http\\S+|www\\S+|@\\S+|#\\S+\", \"\", text) # Keep original case\n",
    "\n",
    "    # 2. Legacy Unicode conversion (e.g., Windows-1258 to Unicode)\n",
    "    processed_text = convert_unicode_legacy_summ(processed_text)\n",
    "\n",
    "    # 3. Standard Unicode Normalization (NFC)\n",
    "    processed_text = normalize_unicode_summ(processed_text)\n",
    "\n",
    "    # 4. Remove Emojis (replace with space)\n",
    "    processed_text = re.sub(emoji_pattern, \" \", processed_text)\n",
    "\n",
    "    # 5. Reduce repeated alphabetic characters (handles Vietnamese characters)\n",
    "    # Kept original case for this regex by removing flags=re.IGNORECASE or ensuring it handles case correctly\n",
    "    processed_text = re.sub(r'([a-zA-Zàáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ])\\1+', r'\\1', processed_text)\n",
    "\n",
    "    # 6. Reduce repeated special characters (non-alphanumeric, non-whitespace, non-Vietnamese)\n",
    "    # This regex is case-insensitive by nature of [^a-z0-9...]\n",
    "    processed_text = re.sub(r'([^a-zA-Z0-9àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ\\s])\\1+', r'\\1', processed_text)\n",
    "\n",
    "    # 7. Normalize punctuation spacing (carefully, preserving punctuation)\n",
    "    _punctuation_chars_summ = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' # Consider if all are needed\n",
    "    escaped_punctuation_summ = re.escape(_punctuation_chars_summ)\n",
    "    # Add space around punctuation, but not if it's part of a number (e.g., 3.14)\n",
    "    # This is a simplified version; robustly handling numbers.decimal vs sentence punctuation can be tricky.\n",
    "    # For ViT5, often minimal intervention is better.\n",
    "    # The primary goal is to separate stuck punctuation: \"word.Another\" -> \"word . Another\"\n",
    "    processed_text = re.sub(r'(?<=[^\\W\\d_])([' + escaped_punctuation_summ + r'])', r' \\1', processed_text) # Add space before if preceded by word char\n",
    "    processed_text = re.sub(r'([' + escaped_punctuation_summ + r'])(?=[^\\W\\d_])', r'\\1 ', processed_text) # Add space after if followed by word char\n",
    "\n",
    "\n",
    "    # 8. Reduce repeated punctuation characters (e.g., \"!!!\" to \"!\")\n",
    "    processed_text = re.sub(r\"([\" + escaped_punctuation_summ + r\"])\\1+\", r\"\\1\", processed_text)\n",
    "\n",
    "    # 9. Vietnamese tone mark normalization\n",
    "    processed_text = chuan_hoa_dau_cau_tieng_viet(processed_text) # This function should handle case appropriately\n",
    "\n",
    "    # 10. Final whitespace cleanup (multiple spaces to single, strip ends)\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_unicode_summ(text):\n",
    "#     return unicodedata.normalize('NFC', text)\n",
    "\n",
    "# def preprocess_text_for_abstractive_summarization(text):\n",
    "#     text = str(text)\n",
    "#     text = normalize_unicode_summ(text)\n",
    "#     # Minimal cleaning for abstractive models; they often handle noise well.\n",
    "#     # Remove excessive newlines or tabs, but preserve sentence structure.\n",
    "#     text = re.sub(r'\\\\n+', '\\\\n', text) # Consolidate multiple newlines\n",
    "#     text = re.sub(r'[\\\\t ]+', ' ', text) # Consolidate multiple spaces/tabs to single space\n",
    "#     text = text.strip()\n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summarize_vietnamese(document_text, model, tokenizer, \n",
    "                                     target_ratio = None,\n",
    "                                     max_input_length=1024, # Max length for T5 base input\n",
    "                                     min_summary_length=100, \n",
    "                                     max_summary_length=500, \n",
    "                                     num_beams=4, \n",
    "                                     length_penalty=1.2, \n",
    "                                     no_repeat_ngram_size=3, \n",
    "                                     early_stopping=True):\n",
    "    if not model or not tokenizer:\n",
    "        return \"Error: Abstractive summarization model or tokenizer not loaded.\"\n",
    "\n",
    "    document_text = preprocess_text_for_abstractive_summarization(document_text)\n",
    "    min_len_to_use = min_summary_length\n",
    "    max_len_to_use = max_summary_length\n",
    "\n",
    "    if target_ratio is not None and 0 < target_ratio <= 1.0:\n",
    "        try:\n",
    "            sentences_in_doc = underthesea_sent_tokenize(document_text)\n",
    "            num_sentences_in_doc = len(sentences_in_doc)\n",
    "\n",
    "            if num_sentences_in_doc > 0:\n",
    "                target_num_sentences = max(1, int(num_sentences_in_doc * target_ratio))\n",
    "                \n",
    "                # Heuristic: Average tokens per Vietnamese sentence for news-style text.\n",
    "                # This is an estimate and might need adjustment based on your specific domain.\n",
    "                # Common Vietnamese sentence length: 15-25 words.\n",
    "                # Tokens per word for Vietnamese BERT-style tokenizers can be 1 to 1.5+\n",
    "                # Let's estimate ~20-30 tokens per sentence.\n",
    "                avg_tokens_per_sentence_heuristic = 25 \n",
    "                \n",
    "                ideal_summary_tokens = target_num_sentences * avg_tokens_per_sentence_heuristic\n",
    "                \n",
    "                # Set min/max summary lengths based on this ideal token count\n",
    "                # Ensure min_len is at least a small absolute number (e.g., 15-20 tokens)\n",
    "                # Ensure max_len is also reasonable (e.g., not exceeding ~250-300 for typical summaries)\n",
    "                min_len_to_use = max(100, int(ideal_summary_tokens * 0.6)) \n",
    "                max_len_to_use = min(500, int(ideal_summary_tokens * 1.4) + 10) # Add a small buffer \n",
    "\n",
    "                # Ensure min_length < max_length and handle edge cases\n",
    "                if min_len_to_use >= max_len_to_use:\n",
    "                    min_len_to_use = max(15, max_len_to_use // 2)\n",
    "                if max_len_to_use <= min_len_to_use : # if max_len_to_use became too small\n",
    "                    max_len_to_use = min_len_to_use + 15 # Ensure there's some room\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Ratio mode: Error calculating ratio-based lengths ({e}), using default summary lengths.\")\n",
    "\n",
    "    input_text = document_text\n",
    "\n",
    "    encoding = tokenizer(input_text, return_tensors=\"pt\", max_length=max_input_length, truncation=True, padding=\"longest\").to(device)\n",
    "\n",
    "    input_ids = encoding.input_ids\n",
    "    attention_mask = encoding.attention_mask\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_len_to_use,\n",
    "        min_length=min_len_to_use,\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        early_stopping=early_stopping\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summarize_vietnamese(documents_texts, model, tokenizer, \n",
    "                                     target_ratio = None,\n",
    "                                     max_input_length=1024, # Max length for T5 base input\n",
    "                                     min_summary_length=100, \n",
    "                                     max_summary_length=500, \n",
    "                                     num_beams=4, \n",
    "                                     length_penalty=1.2, \n",
    "                                     no_repeat_ngram_size=3, \n",
    "                                     early_stopping=True):\n",
    "    if not model or not tokenizer:\n",
    "        error_message = \"Error: Abstractive summarization model or tokenizer not loaded.\"\n",
    "        if isinstance(documents_texts, list):\n",
    "            return [error_message] * len(documents_texts)\n",
    "        else:\n",
    "            return error_message\n",
    "\n",
    "    is_batch = isinstance(documents_texts, list)\n",
    "    if not is_batch:\n",
    "        # If a single string is passed, wrap it in a list for uniform processing\n",
    "        documents_texts = [documents_texts] \n",
    "    \n",
    "    # Preprocess each document text individually\n",
    "    # Ensure preprocess_text_for_abstractive_summarization is defined correctly elsewhere\n",
    "    processed_texts = [preprocess_text_for_abstractive_summarization(doc) for doc in documents_texts]    \n",
    "\n",
    "    min_len_final = min_summary_length\n",
    "    max_len_final = max_summary_length\n",
    "\n",
    "    # Target ratio logic: Apply only if it's a single document input (not a batch)\n",
    "    # and target_ratio is provided. For batch, fixed min/max will be used.\n",
    "    if not is_batch and target_ratio is not None and 0 < target_ratio <= 1.0:\n",
    "        doc_for_ratio_calc = processed_texts[0] # The single preprocessed document\n",
    "        try:\n",
    "            # Ensure underthesea_sent_tokenize is imported and available\n",
    "            sentences_in_doc = underthesea_sent_tokenize(doc_for_ratio_calc) \n",
    "            num_sentences_in_doc = len(sentences_in_doc)\n",
    "\n",
    "            if num_sentences_in_doc > 0:\n",
    "                target_num_sentences = max(1, int(num_sentences_in_doc * target_ratio))\n",
    "                avg_tokens_per_sentence_heuristic = 25 \n",
    "                ideal_summary_tokens = target_num_sentences * avg_tokens_per_sentence_heuristic\n",
    "                \n",
    "                min_len_final = max(100, int(ideal_summary_tokens * 0.6)) \n",
    "                max_len_final = min(500, int(ideal_summary_tokens * 1.4) + 10)\n",
    "\n",
    "                if min_len_final >= max_len_final:\n",
    "                    min_len_final = max(15, max_len_final // 2) \n",
    "                if max_len_final <= min_len_final: \n",
    "                    max_len_final = min_len_final + 15 \n",
    "                \n",
    "                # Optional: for debugging\n",
    "                # print(f\"  Ratio mode (single doc): Input sentences: {num_sentences_in_doc}, Target sentences (est.): {target_num_sentences}\")\n",
    "                # print(f\"  Ratio mode (single doc): Est. ideal tokens: {ideal_summary_tokens}, Using min_length: {min_len_final}, max_length: {max_len_final}\")\n",
    "            # else:\n",
    "                # print(\"  Ratio mode (single doc): No sentences found in input, using default summary lengths.\")\n",
    "        except Exception as e:\n",
    "            # print(f\"  Ratio mode (single doc): Error calculating ratio-based lengths ({e}), using default summary lengths.\")\n",
    "            # Fallback to the global min_summary_length and max_summary_length if ratio calc fails\n",
    "            min_len_final = min_summary_length \n",
    "            max_len_final = max_summary_length\n",
    "    # else:\n",
    "        # Optional: for debugging\n",
    "        # print(f\"  Ratio not provided or invalid, or processing a batch. Using default summary lengths: min={min_len_final}, max={max_len_final}\")\n",
    "\n",
    "    input_texts_for_tokenizer = processed_texts # List of preprocessed texts\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        input_texts_for_tokenizer, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=max_input_length, \n",
    "        truncation=True, \n",
    "        padding=\"longest\"\n",
    "    ).to(device)\n",
    "\n",
    "    input_ids = encoding.input_ids\n",
    "    attention_mask = encoding.attention_mask\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_len_final, # Use the determined max length\n",
    "        min_length=min_len_final, # Use the determined min length\n",
    "        num_beams=num_beams,\n",
    "        length_penalty=length_penalty,\n",
    "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "        early_stopping=early_stopping\n",
    "    )\n",
    "\n",
    "    # Decode all summaries in the batch\n",
    "    summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # If the original input was a single string, return a single string for backward compatibility.\n",
    "    if not is_batch:\n",
    "        return summaries[0] if summaries else \"\"\n",
    "    \n",
    "    return summaries # Return the list of summaries for batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing abstractive_summarize_vietnamese with batch input ---\n",
      "Number of documents in batch: 3\n",
      "\n",
      "--- Test 1: Batch summarization with fixed min/max lengths (20/100) ---\n",
      "\n",
      "Summary for Document 1:\n",
      "Generated Summary: Các món ăn đường phố như phở, bún chả, và cà phê trứng đã trở thành biểu tượng. Trong đó, Hà Nội nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và nền văn hoá ẩm thực đường phố mang đậm văn hoá vùng kinh tế châu Á - Thái Bình Dương cũng mang trong mình một nét đặc trưng trong văn hoá giao thông đặc sắc. Hãy cùng ngắm nhìn thành phố từ những góc phố, con phố và cuộc sống nơi đây qua bài viết sau.\n",
      "\n",
      "Summary for Document 2:\n",
      "Generated Summary: Việc kinh doanh, giết mổ và sử dụng thịt chó, mèo ở Hà Nội thời gian qua đã gây những hình ảnh không đẹp, phản cảm đối du khách trong và ngoài nước. Hà Nội cũng đã và đang có nhiều biện pháp để chấn chỉnh tình trạng này. Hãy cùng nhìn lại thủ đô văn minh, hiện đại của Hà Nội. Tại sao thịt chó lại có hình ảnh phản cảm đến du khách nước ngoài?... Rất nhiều người đã đặt câu hỏi về tính phản cảm của người dân.\n",
      "\n",
      "Summary for Document 3:\n",
      "Generated Summary: Du lịch Việt Nam đang phát triển mạnh. Các bạn trẻ yêu biển, các bạn có biết cách bảo vệ môi trường biển như thế nào để phát triển du lịch bền vững, đẹp hơn nữa và đảm bảo chủ quyền biển đảo. Hãy cùng chúng tôi tìm hiểu những kinh nghiệm, bí quyết giúp bạn thoát khỏi cảnh đánh mất dần vẻ đẹp của biển cả và sức sống của hòn đảo.ỂỂ gợi ý văn hoá, nghệ thuật và ứng phó với biến đổi khí hậu nhé!.\n",
      "\n",
      "--- Test 2: Batch summarization with summary_ratio=0.25 ---\n",
      "(Expecting lengths to be governed by min_summary_length=30, max_summary_length=200 from function defaults if ratio doesn't override for batch)\n",
      "\n",
      "Summary for Document 1 (ratio 0.25 attempt):\n",
      "Generated Summary: Các món ăn đường phố như phở, bún chả, và cà phê trứng đã trở thành biểu tượng. Trong đó, Hà Nội nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và nền văn hoá ẩm thực đường phố mang đậm văn hoá vùng kinh tế châu Á - Thái Bình Dương cũng mang trong mình một nét đặc trưng trong văn hoá giao thông đặc sắc. Hãy cùng ngắm nhìn thành phố từ những góc phố, con phố và cuộc sống nơi đây qua bài viết sau.\n",
      "Word count: 91\n",
      "\n",
      "Summary for Document 2 (ratio 0.25 attempt):\n",
      "Generated Summary: Việc kinh doanh, giết mổ và sử dụng thịt chó, mèo ở Hà Nội thời gian qua đã gây những hình ảnh không đẹp, phản cảm đối du khách trong và ngoài nước. Hà Nội cũng đã và đang có nhiều biện pháp để chấn chỉnh tình trạng này. Hãy cùng nhìn lại thủ đô văn minh, hiện đại của Hà Nội. Tại sao thịt chó lại có hình ảnh phản cảm đến du khách nước ngoài?... Rất nhiều người đã đặt câu hỏi về tính phản cảm của người dân.\n",
      "Word count: 91\n",
      "\n",
      "Summary for Document 3 (ratio 0.25 attempt):\n",
      "Generated Summary: Du lịch Việt Nam đang phát triển mạnh. Các bạn trẻ yêu biển, các bạn có biết cách bảo vệ môi trường biển như thế nào để phát triển du lịch bền vững, đẹp hơn nữa và đảm bảo chủ quyền biển đảo. Hãy cùng chúng tôi tìm hiểu những kinh nghiệm, bí quyết giúp bạn thoát khỏi cảnh đánh mất dần vẻ đẹp của biển cả và sức sống của hòn đảo.ỂỂ gợi ý văn hoá, nghệ thuật và ứng phó với biến đổi khí hậu nhé!.\n",
      "Word count: 88\n",
      "\n",
      "--- Batch summarization test finished ---\n",
      "Note: The behavior of `summary_ratio` in batch mode depends on the `abstractive_summarize_vietnamese` function's implementation.\n",
      "If it calculates a single min/max length for the entire batch (e.g., based on average or first item), results will reflect that.\n",
      "If it falls back to global min/max lengths for batches, that will be observed.\n"
     ]
    }
   ],
   "source": [
    "# Test batch generation with abstractive_summarize_vietnamese\n",
    "\n",
    "# Define a batch of sample Vietnamese documents\n",
    "# Document 1 from notebook_cell_output_7\n",
    "doc1_batch_test = 'Hà Nội, thủ đô của Việt Nam, là một thành phố cổ kính với hơn một nghìn năm lịch sử.\\nNơi đây nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và một nền văn hóa ẩm thực đường phố phong phú.\\nHồ Hoàn Kiếm, trái tim của thành phố, mang trong mình những truyền thuyết lịch sử và là nơi người dân địa phương tụ tập vui chơi.\\nẨm thực Hà Nội cũng là một điểm nhấn đặc biệt, với các món ăn như phở, bún chả, và cà phê trứng đã trở thành biểu tượng.'\n",
    "\n",
    "# Document 2 based on notebook_cell_output_5 (partial content)\n",
    "doc2_batch_test = 'Đây là một trong những nội dung tại văn bản vừa được UBND TP Hà Nội ban hành về việc tăng cường công tác quản lý nuôi , giết mổ , kinh doanh và sử dụng thịt chó , mèo trên địa bàn .Theo đó , các sở , ngành trên địa bàn phải vào cuộc ngay để hướng tới thay đổi thói quen của người dân khi dùng chó , mèo làm thực phẩm .Gây phản cảm với du khách , người nước ngoàiCũng trong văn bản này , UBND TP Hà Nội thừa nhận rằng việc kinh doanh , giết mổ và sử dụng thịt chó , mèo tại Hà Nội thời gian qua đã tạo những hình ảnh không đẹp , phản cảm đối với du khách trong và ngoài nước , làm ảnh hưởng đến hình ảnh của một thủ đô văn minh , hiện đại .'\n",
    "\n",
    "# Document 3: A generic short text\n",
    "doc3_batch_test = \"Việt Nam là một quốc gia có tiềm năng lớn về du lịch biển với đường bờ biển dài và nhiều bãi biển đẹp. Phát triển du lịch bền vững gắn liền với bảo vệ môi trường biển là một ưu tiên hàng đầu.\"\n",
    "\n",
    "sample_documents_batch = [doc1_batch_test, doc2_batch_test, doc3_batch_test]\n",
    "\n",
    "print(\"--- Testing abstractive_summarize_vietnamese with batch input ---\")\n",
    "print(f\"Number of documents in batch: {len(sample_documents_batch)}\")\n",
    "\n",
    "# Ensure model, tokenizer, device are loaded and available in the global scope\n",
    "# from previous cells.\n",
    "# Also, abstractive_summarize_vietnamese and its helper \n",
    "# preprocess_text_for_abstractive_summarization must be defined.\n",
    "\n",
    "# Test 1: Batch summarization with specified min/max lengths\n",
    "print(\"\\n--- Test 1: Batch summarization with fixed min/max lengths (20/100) ---\")\n",
    "try:\n",
    "    batch_summaries_fixed_lengths = abstractive_summarize_vietnamese(\n",
    "        documents_texts=sample_documents_batch,\n",
    "        model=model_summ, \n",
    "        tokenizer=tokenizer_summ, \n",
    "        max_input_length=1024,  # Default from function\n",
    "        min_summary_length=100,  # Custom min length for this test\n",
    "        max_summary_length=500, # Custom max length for this test\n",
    "        num_beams=4,            # Common value, can be function's default (5)\n",
    "        length_penalty=1.0,     # Function's default\n",
    "        no_repeat_ngram_size=3, # Function's default\n",
    "        early_stopping=True     # Function's default\n",
    "    )\n",
    "\n",
    "    if isinstance(batch_summaries_fixed_lengths, list):\n",
    "        for i, summary in enumerate(batch_summaries_fixed_lengths):\n",
    "            print(f\"\\nSummary for Document {i+1}:\")\n",
    "            # print(f\"Original (first 100 chars): {sample_documents_batch[i][:100].replace(chr(10), ' ')}...\")\n",
    "            print(f\"Generated Summary: {summary}\")\n",
    "    else:\n",
    "        print(f\"ERROR: Expected a list of summaries, but got: {type(batch_summaries_fixed_lengths)}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Test 1 (fixed lengths): {e}\")\n",
    "\n",
    "# Test 2: Batch summarization with summary_ratio\n",
    "# Note: The current implementation of abstractive_summarize_vietnamese, based on the provided snippets,\n",
    "# is expected to use the global min_summary_length and max_summary_length for batch processing,\n",
    "# even if summary_ratio is provided, because model.generate() needs fixed lengths for a batch.\n",
    "print(\"\\n--- Test 2: Batch summarization with summary_ratio=0.25 ---\")\n",
    "print(\"(Expecting lengths to be governed by min_summary_length=30, max_summary_length=200 from function defaults if ratio doesn't override for batch)\")\n",
    "try:\n",
    "    batch_summaries_ratio_test = abstractive_summarize_vietnamese(\n",
    "        documents_texts=sample_documents_batch,\n",
    "        model=model_summ,\n",
    "        tokenizer=tokenizer_summ,\n",
    "        # Using function's default min/max lengths which would be used if ratio is ignored for batch\n",
    "        target_ratio=0.5,\n",
    "        min_summary_length=100, \n",
    "        max_summary_length=500,\n",
    "        # Other parameters can be defaults or specified\n",
    "        num_beams=4,\n",
    "        length_penalty=1.0,\n",
    "        no_repeat_ngram_size=3,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    if isinstance(batch_summaries_ratio_test, list):\n",
    "        for i, summary in enumerate(batch_summaries_ratio_test):\n",
    "            print(f\"\\nSummary for Document {i+1} (ratio 0.25 attempt):\")\n",
    "            # print(f\"Original (first 100 chars): {sample_documents_batch[i][:100].replace(chr(10), ' ')}...\")\n",
    "            print(f\"Generated Summary: {summary}\")\n",
    "            print(f\"Word count: {len(summary.split())}\") # To observe actual length\n",
    "    else:\n",
    "        print(f\"ERROR: Expected a list of summaries for batch input with ratio, but got: {type(batch_summaries_ratio_test)}\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Test 2 (summary_ratio): {e}\")\n",
    "\n",
    "# print(\"\\n--- Batch summarization test finished ---\")\n",
    "# print(\"Note: The behavior of `summary_ratio` in batch mode depends on the `abstractive_summarize_vietnamese` function's implementation.\")\n",
    "# print(\"If it calculates a single min/max length for the entire batch (e.g., based on average or first item), results will reflect that.\")\n",
    "# print(\"If it falls back to global min/max lengths for batches, that will be observed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Các món ăn đường phố như phở, bún chả, và cà phê trứng đã trở thành biểu tượng. Trong đó, Hà Nội nổi tiếng với kiến trúc Pháp thuộc, những hồ nước yên bình và nền văn hoá ẩm thực đường phố phong phú.\n"
     ]
    }
   ],
   "source": [
    "print(abstractive_summarize_vietnamese(doc1_batch_test, model_summ, tokenizer_summ, target_ratio=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Loading and splitting the dataset...\n",
      "Dataset split complete:\n",
      "  Training set size: 59651\n",
      "  Validation set size: 14913\n",
      "\n",
      "STEP 2: Preparing validation data (preprocessing reference summaries)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing references: 100%|██████████| 14913/14913 [00:05<00:00, 2761.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation set prepared: 14913 documents, 14913 preprocessed reference summaries.\n",
      "  Proceeding with 14913 valid pairs for evaluation.\n",
      "\n",
      "STEP 3: Loading evaluation metrics...\n",
      "  BERTScore metric loaded successfully.\n",
      "\n",
      "STEP 4: Evaluating pre-trained model using `abstractive_summarize_vietnamese` function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   0%|          | 1/467 [00:03<28:38,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 0: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   0%|          | 2/467 [00:06<23:48,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 32: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.92 GiB is allocated by PyTorch, and 619.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   1%|          | 3/467 [00:08<19:43,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 64: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.80 GiB is allocated by PyTorch, and 746.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   1%|          | 4/467 [00:11<21:18,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 96: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   1%|          | 5/467 [00:13<19:34,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 128: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.92 GiB is allocated by PyTorch, and 619.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   1%|▏         | 6/467 [00:16<20:28,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 160: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   1%|▏         | 7/467 [00:18<18:36,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 192: CUDA out of memory. Tried to allocate 346.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.51 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   2%|▏         | 8/467 [00:20<17:48,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 224: CUDA out of memory. Tried to allocate 356.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.68 GiB is allocated by PyTorch, and 872.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   2%|▏         | 9/467 [00:22<16:25,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 256: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.91 GiB is allocated by PyTorch, and 637.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   2%|▏         | 10/467 [00:23<15:09,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 288: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   2%|▏         | 11/467 [00:26<16:23,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 320: CUDA out of memory. Tried to allocate 292.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.51 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   3%|▎         | 12/467 [00:27<14:53,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 352: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   3%|▎         | 13/467 [00:29<15:02,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 384: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.59 GiB is allocated by PyTorch, and 962.65 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   3%|▎         | 14/467 [00:31<13:48,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 416: CUDA out of memory. Tried to allocate 380.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.04 GiB is allocated by PyTorch, and 505.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   3%|▎         | 15/467 [00:32<13:08,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 448: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   3%|▎         | 16/467 [00:34<12:16,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 480: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   4%|▎         | 17/467 [00:36<14:05,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 512: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   4%|▍         | 18/467 [00:38<12:53,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 544: CUDA out of memory. Tried to allocate 352.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.61 GiB is allocated by PyTorch, and 938.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   4%|▍         | 19/467 [00:41<16:44,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 576: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   4%|▍         | 20/467 [00:43<15:33,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 608: CUDA out of memory. Tried to allocate 382.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.07 GiB is allocated by PyTorch, and 469.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   4%|▍         | 21/467 [00:45<15:39,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 640: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 24.93 GiB is allocated by PyTorch, and 1.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   5%|▍         | 22/467 [00:47<14:42,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 672: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   5%|▍         | 23/467 [00:49<14:28,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 704: CUDA out of memory. Tried to allocate 360.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.74 GiB is allocated by PyTorch, and 812.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   5%|▌         | 24/467 [00:51<16:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 736: CUDA out of memory. Tried to allocate 372.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.92 GiB is allocated by PyTorch, and 625.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   5%|▌         | 25/467 [00:53<15:38,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 768: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 26.12 GiB is allocated by PyTorch, and 421.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "Error generating summary for document index 800: CUDA out of memory. Tried to allocate 270.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.10 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   6%|▌         | 27/467 [00:58<16:46,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 832: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 241.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.74 GiB is allocated by PyTorch, and 803.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   6%|▌         | 28/467 [01:01<17:36,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 864: CUDA out of memory. Tried to allocate 348.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 241.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.19 GiB is allocated by PyTorch, and 1.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   6%|▌         | 29/467 [01:03<16:03,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 896: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 241.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.74 GiB is allocated by PyTorch, and 803.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   6%|▋         | 30/467 [01:05<16:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 928: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 241.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.74 GiB is allocated by PyTorch, and 803.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   7%|▋         | 31/467 [01:07<14:44,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 960: CUDA out of memory. Tried to allocate 322.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.13 GiB is allocated by PyTorch, and 1.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   7%|▋         | 32/467 [01:08<13:12,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 992: CUDA out of memory. Tried to allocate 328.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.21 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   7%|▋         | 33/467 [01:12<17:32,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 1024: CUDA out of memory. Tried to allocate 364.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.78 GiB is allocated by PyTorch, and 770.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   7%|▋         | 34/467 [01:14<15:59,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 1056: CUDA out of memory. Tried to allocate 348.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 241.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.21 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   7%|▋         | 35/467 [01:15<14:30,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating summary for document index 1088: CUDA out of memory. Tried to allocate 356.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 239.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.66 GiB is allocated by PyTorch, and 890.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summaries as batch:   7%|▋         | 35/467 [01:18<16:08,  2.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:2484\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2484\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_beam_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2485\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/generation/utils.py:3904\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3902\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m-> 3904\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1905\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1905\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1906\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1908\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1915\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1916\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1921\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1131\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:706\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[0;32m--> 706\u001b[0m     cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:636\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    635\u001b[0m normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 636\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:527\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 5 has a total capacty of 79.15 GiB of which 241.88 MiB is free. Process 1611699 has 1.34 GiB memory in use. Process 2886948 has 1.56 GiB memory in use. Process 3013142 has 1.56 GiB memory in use. Process 981290 has 27.21 GiB memory in use. Process 2518409 has 20.19 GiB memory in use. Including non-PyTorch memory, this process has 27.02 GiB memory in use. Of the allocated memory 25.74 GiB is allocated by PyTorch, and 803.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 123\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Call your existing function to generate the summary\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# It uses its own internal preprocessing for the document text.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     batch_document_texts \u001b[38;5;241m=\u001b[39m eval_docs[i:i\u001b[38;5;241m+\u001b[39minference_batch_size]\n\u001b[0;32m--> 123\u001b[0m     batch_generated_summaries \u001b[38;5;241m=\u001b[39m \u001b[43mabstractive_summarize_vietnamese\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_document_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_summ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_summ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_input_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Default from function\u001b[39;49;00m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_summary_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Custom min length for this test\u001b[39;49;00m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_summary_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Custom max length for this test\u001b[39;49;00m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# Common value, can be function's default (5)\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Function's default\u001b[39;49;00m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Function's default\u001b[39;49;00m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Function's default\u001b[39;49;00m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     generated_summaries_for_eval\u001b[38;5;241m.\u001b[39mextend(batch_generated_summaries)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[21], line 78\u001b[0m, in \u001b[0;36mabstractive_summarize_vietnamese\u001b[0;34m(documents_texts, model, tokenizer, target_ratio, max_input_length, min_summary_length, max_summary_length, num_beams, length_penalty, no_repeat_ngram_size, early_stopping)\u001b[0m\n\u001b[1;32m     75\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m     76\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mattention_mask\n\u001b[0;32m---> 78\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the determined max length\u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_len_final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use the determined min length\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Decode all summaries in the batch\u001b[39;00m\n\u001b[1;32m     90\u001b[0m summaries \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(summary_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/elo/.conda/envs/vifree-txt-3.10/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm # Ensure this is imported for progress bars\n",
    "import time\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re # Make sure re is imported for preprocessing functions\n",
    "import unicodedata # Make sure unicodedata is imported for preprocessing\n",
    "\n",
    "# --- Ensure all necessary helper functions for preprocessing are defined above this cell ---\n",
    "# This includes:\n",
    "# - emoji_pattern, bang_nguyen_am, nguyen_am_to_ids, dicchar, loaddicchar (from cell execution_count: 11)\n",
    "# - normalize_unicode_nfc_summ, convert_unicode_legacy_summ (from cell execution_count: 11)\n",
    "# - is_valid_vietnam_word, chuan_hoa_dau_tu_tieng_viet (these are complex, ensure they are defined correctly if used by preprocess_text_for_abstractive_summarization)\n",
    "# - preprocess_text_for_abstractive_summarization (from cell execution_count: 11)\n",
    "# - abstractive_summarize_vietnamese (from cell execution_count: 24)\n",
    "# - underthesea_sent_tokenize (if used by abstractive_summarize_vietnamese for ratio calculation)\n",
    "\n",
    "# --- Cell for Dataset Splitting, Preprocessing References, and Evaluation ---\n",
    "\n",
    "print(\"STEP 1: Loading and splitting the dataset...\")\n",
    "try:\n",
    "    # Load the full dataset\n",
    "    full_dataset = load_dataset(\"OpenHust/vietnamese-summarization\")\n",
    "    if 'train' not in full_dataset:\n",
    "        raise ValueError(\"'train' split not found in the loaded dataset.\")\n",
    "\n",
    "    # Convert the 'train' split to a pandas DataFrame for splitting\n",
    "    df = full_dataset['train'].to_pandas()\n",
    "\n",
    "    # Split 80% for training, 20% for validation\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"  Training set size: {len(train_df)}\")\n",
    "    print(f\"  Validation set size: {len(val_df)}\")\n",
    "\n",
    "    # Convert pandas DataFrames back to Hugging Face Dataset objects\n",
    "    train_dataset_hf = Dataset.from_pandas(train_df)\n",
    "    val_dataset_hf = Dataset.from_pandas(val_df)\n",
    "\n",
    "    # Create a new DatasetDict\n",
    "    split_dataset = DatasetDict({\n",
    "        'train': train_dataset_hf,\n",
    "        'validation': val_dataset_hf\n",
    "    })\n",
    "    current_val_dataset = split_dataset['validation'] # Use this for evaluation\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during dataset loading/splitting: {e}\")\n",
    "    # Fallback or stop execution if dataset is crucial\n",
    "    current_val_dataset = None\n",
    "\n",
    "\n",
    "if current_val_dataset:\n",
    "    print(\"\\nSTEP 2: Preparing validation data (preprocessing reference summaries)...\")\n",
    "    # We will feed raw documents to `abstractive_summarize_vietnamese` as it preprocesses internally.\n",
    "    # We need to preprocess the reference summaries for a fair comparison with the output.\n",
    "    raw_val_docs = []\n",
    "    preprocessed_val_references = []\n",
    "\n",
    "    for example in tqdm(current_val_dataset, desc=\"Preprocessing references\"):\n",
    "        doc = example.get('Document', '')\n",
    "        summary = example.get('Summary', '')\n",
    "\n",
    "        if doc and summary: # Ensure both document and summary exist\n",
    "            raw_val_docs.append(doc)\n",
    "            # Preprocess the reference summary using the same function that\n",
    "            # abstractive_summarize_vietnamese uses for its input.\n",
    "            preprocessed_val_references.append(preprocess_text_for_abstractive_summarization(summary))\n",
    "        elif doc: # If only document exists, still keep it for generation if desired (though no reference for it)\n",
    "            raw_val_docs.append(doc)\n",
    "            preprocessed_val_references.append(\"\") # No reference to compare against\n",
    "\n",
    "    print(f\"  Validation set prepared: {len(raw_val_docs)} documents, {len(preprocessed_val_references)} preprocessed reference summaries.\")\n",
    "\n",
    "    # Filter out pairs where the preprocessed reference became empty, if that's undesirable\n",
    "    valid_indices_for_eval = [\n",
    "        i for i, ref in enumerate(preprocessed_val_references) if ref.strip()\n",
    "    ]\n",
    "    \n",
    "    eval_docs = [raw_val_docs[i] for i in valid_indices_for_eval]\n",
    "    eval_references = [preprocessed_val_references[i] for i in valid_indices_for_eval]\n",
    "\n",
    "    if not eval_docs:\n",
    "        print(\"Warning: No valid document-reference pairs after preprocessing references. Evaluation cannot proceed.\")\n",
    "    else:\n",
    "        print(f\"  Proceeding with {len(eval_docs)} valid pairs for evaluation.\")\n",
    "\n",
    "        print(\"\\nSTEP 3: Loading evaluation metrics...\")\n",
    "        rouge_metric = evaluate.load('rouge')\n",
    "        try:\n",
    "            bertscore_metric = evaluate.load('bertscore')\n",
    "            use_bertscore = True\n",
    "            print(\"  BERTScore metric loaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: BERTScore metric not loaded: {e}. Proceeding with ROUGE only.\")\n",
    "            use_bertscore = False\n",
    "\n",
    "        print(\"\\nSTEP 4: Evaluating pre-trained model using `abstractive_summarize_vietnamese` function...\")\n",
    "        # Ensure model_summ, tokenizer_summ, and device are defined and loaded from previous cells\n",
    "        if 'model_summ' not in locals() or 'tokenizer_summ' not in locals() or 'device' not in locals():\n",
    "            print(\"Error: `model_summ`, `tokenizer_summ`, or `device` is not defined. Please run the model loading cell.\")\n",
    "        else:\n",
    "            model_summ.eval() # Ensure model is in evaluation mode\n",
    "            generated_summaries_for_eval = []\n",
    "            inference_batch_size = 32\n",
    "            start_time_eval = time.time()\n",
    "\n",
    "            # Consider batching if `abstractive_summarize_vietnamese` processes one by one and it's slow\n",
    "            # For now, processing one by one as per typical use of such a helper function\n",
    "            # for i, doc_text in enumerate(tqdm(eval_docs, desc=\"Generating summaries\")):\n",
    "            for i in tqdm(range(0,len(eval_docs),inference_batch_size), desc = \"Generating summaries as batch\"):\n",
    "                try:\n",
    "                    # Call your existing function to generate the summary\n",
    "                    # It uses its own internal preprocessing for the document text.\n",
    "                    batch_document_texts = eval_docs[i:i+inference_batch_size]\n",
    "                    batch_generated_summaries = abstractive_summarize_vietnamese(\n",
    "                        batch_document_texts,\n",
    "                        model_summ,\n",
    "                        tokenizer_summ,\n",
    "                        max_input_length=1024,  # Default from function\n",
    "                        min_summary_length=100,  # Custom min length for this test\n",
    "                        max_summary_length=500, # Custom max length for this test\n",
    "                        num_beams=4,            # Common value, can be function's default (5)\n",
    "                        length_penalty=1.0,     # Function's default\n",
    "                        no_repeat_ngram_size=3, # Function's default\n",
    "                        early_stopping=True     # Function's default\n",
    "                    )\n",
    "                    generated_summaries_for_eval.extend(batch_generated_summaries)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating summary for document index {i}: {e}\")\n",
    "                    # generated_summaries_for_eval.append(\"\") # Append empty string on error\n",
    "                    generated_summaries_for_eval.extend([\"\"] * len(batch_document_texts)) # Add empty strings for failed batch\n",
    "\n",
    "\n",
    "                if i > 0 and i % 50 == 0 : # Optional: Clear cache periodically if memory is an issue\n",
    "                     if device.type == 'cuda': torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "\n",
    "            end_time_eval = time.time()\n",
    "            print(f\"  Summary generation complete. Time taken: {end_time_eval - start_time_eval:.2f} seconds.\")\n",
    "\n",
    "            print(\"\\nSTEP 5: Computing metrics...\")\n",
    "            try:\n",
    "                rouge_scores = rouge_metric.compute(\n",
    "                    predictions=generated_summaries_for_eval,\n",
    "                    references=eval_references,\n",
    "                    use_stemmer=True\n",
    "                )\n",
    "                print(\"\\n--- ROUGE Scores (Pre-trained Model) ---\")\n",
    "                for metric, score in rouge_scores.items(): print(f\"  {metric}: {score*100:.2f}%\")\n",
    "            except Exception as e: print(f\"Error computing ROUGE scores: {e}\")\n",
    "\n",
    "            if use_bertscore:\n",
    "                try:\n",
    "                    bertscore_results = bertscore_metric.compute(\n",
    "                        predictions=generated_summaries_for_eval,\n",
    "                        references=eval_references,\n",
    "                        lang='vi', model_type=\"vinai/phobert-base\", device=device,\n",
    "                        batch_size=32, verbose=False # Bertscore also has batch_size\n",
    "                    )\n",
    "                    print(\"\\n--- BERTScore (Pre-trained Model) ---\")\n",
    "                    avg_precision = np.mean(bertscore_results['precision']) if bertscore_results['precision'] else 0\n",
    "                    avg_recall = np.mean(bertscore_results['recall']) if bertscore_results['recall'] else 0\n",
    "                    avg_f1 = np.mean(bertscore_results['f1']) if bertscore_results['f1'] else 0\n",
    "                    print(f\"  Average Precision: {avg_precision*100:.2f}%\")\n",
    "                    print(f\"  Average Recall: {avg_recall*100:.2f}%\")\n",
    "                    print(f\"  Average F1-score: {avg_f1*100:.2f}%\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing BERTScore: {e}\")\n",
    "                    print(\"  BERTScore might require `pip install bert_score` and internet access.\")\n",
    "\n",
    "            print(\"\\nSTEP 6: Displaying a few examples...\")\n",
    "            for i in range(min(3, len(generated_summaries_for_eval))):\n",
    "                print(f\"\\n--- Example {i+1} ---\")\n",
    "                print(f\"  Original Document (first 300 chars): {eval_docs[i][:300]}...\")\n",
    "                print(f\"  Preprocessed Reference Summary: {eval_references[i]}\")\n",
    "                print(f\"  Generated Summary (ViT5 Pre-trained): {generated_summaries_for_eval[i]}\")\n",
    "\n",
    "            print(\"\\nSTEP 7: Saving evaluation results...\")\n",
    "            # (Saving logic remains the same as your previous cell)\n",
    "            results_payload = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'model_name': MODEL_NAME_SUMM if 'MODEL_NAME_SUMM' in locals() else \"N/A\",\n",
    "                'num_validation_samples_evaluated': len(eval_docs),\n",
    "                'inference_batch_size': inference_batch_size,\n",
    "                'evaluation_time_seconds': end_time_eval - start_time_eval,\n",
    "                'rouge_scores': rouge_scores if 'rouge_scores' in locals() else None,\n",
    "                'example_outputs': [\n",
    "                    {\n",
    "                        'document_preview': eval_docs[j][:300]+\"...\",\n",
    "                        'reference_summary': eval_references[j],\n",
    "                        'generated_summary': generated_summaries_for_eval[j]\n",
    "                    } for j in range(min(10, len(eval_docs)))\n",
    "                ]\n",
    "            }\n",
    "            if use_bertscore and 'bertscore_results' in locals() and bertscore_results:\n",
    "                results_payload['bertscore'] = {\n",
    "                    'precision_avg': np.mean(bertscore_results['precision']),\n",
    "                    'recall_avg': np.mean(bertscore_results['recall']),\n",
    "                    'f1_avg': np.mean(bertscore_results['f1'])\n",
    "                }\n",
    "            results_filename = f\"evaluation_results_batched_ViT5_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            try:\n",
    "                with open(results_filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(results_payload, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"  Evaluation results saved to: {results_filename}\")\n",
    "            except Exception as e: print(f\"Error saving results to JSON: {e}\")\n",
    "else:\n",
    "    print(\"Evaluation cannot proceed as `current_val_dataset` is not available.\")\n",
    "print(\"\\n--- Batched Evaluation Cell Execution Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Abstractive Summarization Demo (VietAI/vit5-base-vietnews-summarization) ---\n",
      "Original Article (length: 751 chars):\n",
      "\n",
      "Đại dịch COVID-19 đã gây ra những tác động sâu rộng đến kinh tế toàn cầu trong suốt hơn hai năm qua. \n",
      "Nhiều ngành công nghiệp chủ chốt như du lịch, hàng không, và dịch vụ ăn uống đã phải đối mặt với những thách thức chưa từng có. \n",
      "Các chính phủ trên khắp thế giới đã tung ra các gói kích thích kinh tế khổng lồ để hỗ trợ doanh nghiệp và người lao động. \n",
      "Tuy nhiên, quá trình phục hồi được dự báo sẽ không đồng đều và còn nhiều bất ổn phía trước, đặc biệt là với sự xuất hiện của các biến chủng mới. \n",
      "Chuyển đổi số và ứng dụng công nghệ được coi là chìa khóa quan trọng giúp các nền kinh tế thích ứng và vượt qua khủng hoảng. \n",
      "Việc đảm bảo chuỗi cung ứng toàn cầu và tăng cường hợp tác quốc tế cũng là những yếu tố then chốt cho sự phục hồi bền vững.\n",
      "\n",
      "  Ratio not provided or invalid, using default summary lengths: min=100, max=500\n",
      "\n",
      "Abstractive Summary (length: 427 chars):\n",
      "Sự phát triển của các nền kinh tế đang chịu sự ảnh hưởng nặng nề từ sự phát triển nhanh chóng của công nghệ và sự thay đổi chóng mặt của các nước châu Âu châu Á trong suốt 20 năm qua đã chứng tỏ sự thất bại trong việc đạt được sự ổn định ở nhiều lĩnh vực, trong đó có lĩnh vực du lịch, hàng không, và dịch vụ ăn uống, ở nhiều nơi trên thế giới. Tỉ lệ thất nghiệp tăng, cơ cấu tỉ giá hối đoái và tăng trưởng kinh tế ở mức cao...\n"
     ]
    }
   ],
   "source": [
    "sample_vietnamese_article_for_abstractive = \"\"\"\n",
    "Đại dịch COVID-19 đã gây ra những tác động sâu rộng đến kinh tế toàn cầu trong suốt hơn hai năm qua. \n",
    "Nhiều ngành công nghiệp chủ chốt như du lịch, hàng không, và dịch vụ ăn uống đã phải đối mặt với những thách thức chưa từng có. \n",
    "Các chính phủ trên khắp thế giới đã tung ra các gói kích thích kinh tế khổng lồ để hỗ trợ doanh nghiệp và người lao động. \n",
    "Tuy nhiên, quá trình phục hồi được dự báo sẽ không đồng đều và còn nhiều bất ổn phía trước, đặc biệt là với sự xuất hiện của các biến chủng mới. \n",
    "Chuyển đổi số và ứng dụng công nghệ được coi là chìa khóa quan trọng giúp các nền kinh tế thích ứng và vượt qua khủng hoảng. \n",
    "Việc đảm bảo chuỗi cung ứng toàn cầu và tăng cường hợp tác quốc tế cũng là những yếu tố then chốt cho sự phục hồi bền vững.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Abstractive Summarization Demo (VietAI/vit5-base-vietnews-summarization) ---\")\n",
    "print(f\"Original Article (length: {len(sample_vietnamese_article_for_abstractive)} chars):\\n{sample_vietnamese_article_for_abstractive}\")\n",
    "\n",
    "if model_summ and tokenizer_summ:\n",
    "    abstractive_summary = abstractive_summarize_vietnamese(sample_vietnamese_article_for_abstractive, model_summ, tokenizer_summ)\n",
    "    print(f\"\\nAbstractive Summary (length: {len(abstractive_summary)} chars):\\n{abstractive_summary}\")\n",
    "else:\n",
    "    print(\"\\nAbstractive summarization model not loaded. Skipping demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summarization Comparison --- \n",
      "Original Text (length: 1013 chars):\n",
      "\n",
      "Thị trường bất động sản Việt Nam trong những năm gần đây đã trải qua nhiều biến động đáng chú ý. \n",
      "Sau giai đoạn tăng trưởng nóng, thị trường đã có dấu hiệu chững lại ở một số phân khúc, đặc biệt là đất nền và bất động sản nghỉ dưỡng ở các tỉnh lẻ. \n",
      "Nguyên nhân chính được cho là do các chính sách siết chặt tín dụng vào bất động sản, cũng như những biến động của kinh tế vĩ mô và tâm lý thận trọng của nhà đầu tư. \n",
      "Tuy nhiên, phân khúc nhà ở thực, đặc biệt là căn hộ chung cư tại các thành phố lớn như Hà Nội và TP.HCM, vẫn duy trì được sức hút nhất định do nhu cầu ở thực tế cao. \n",
      "Các chuyên gia dự báo rằng thị trường sẽ cần thêm thời gian để điều chỉnh và tìm lại điểm cân bằng mới. \n",
      "Trong dài hạn, với dân số trẻ và tốc độ đô thị hóa nhanh, tiềm năng phát triển của thị trường bất động sản Việt Nam vẫn được đánh giá cao. \n",
      "Các yếu tố như cải thiện cơ sở hạ tầng, sự minh bạch trong quy hoạch và chính sách pháp lý ổn định sẽ đóng vai trò quan trọng trong việc thúc đẩy sự phát triển bền vững của thị trường.\n",
      "\n",
      "\n",
      "TextRank Summary (ratio 0.3, length: 288 chars):\n",
      "Sau giai đoạn tăng trưởng nóng, thị trường đã có dấu hiệu chững lại ở một số phân khúc, đặc biệt là đất nền và bất động sản nghỉ dưỡng ở các tỉnh lẻ.\n",
      "Trong dài hạn, với dân số trẻ và tốc độ đô thị hóa nhanh, tiềm năng phát triển của thị trường bất động sản Việt Nam vẫn được đánh giá cao.\n",
      "\n",
      "Abstractive Summary (VietAI T5, length: 270 chars):\n",
      "Trước những biến động của thị trường bất động sản, hị rường BẤT ĐỘ sản Việ Nam vẫn chứng tỏ được sức hút mãnh liệt với các nhà đầu tư. Mặc dù vậy, cung cấp các gói dịch vụ hậu cần tốt, chất lượng tốt và sự minh bạch rong quy hoạch, chính sách pháp lý ổn định vẫn đủ tốt.\n"
     ]
    }
   ],
   "source": [
    "longer_vietnamese_text = \"\"\"\n",
    "Thị trường bất động sản Việt Nam trong những năm gần đây đã trải qua nhiều biến động đáng chú ý. \n",
    "Sau giai đoạn tăng trưởng nóng, thị trường đã có dấu hiệu chững lại ở một số phân khúc, đặc biệt là đất nền và bất động sản nghỉ dưỡng ở các tỉnh lẻ. \n",
    "Nguyên nhân chính được cho là do các chính sách siết chặt tín dụng vào bất động sản, cũng như những biến động của kinh tế vĩ mô và tâm lý thận trọng của nhà đầu tư. \n",
    "Tuy nhiên, phân khúc nhà ở thực, đặc biệt là căn hộ chung cư tại các thành phố lớn như Hà Nội và TP.HCM, vẫn duy trì được sức hút nhất định do nhu cầu ở thực tế cao. \n",
    "Các chuyên gia dự báo rằng thị trường sẽ cần thêm thời gian để điều chỉnh và tìm lại điểm cân bằng mới. \n",
    "Trong dài hạn, với dân số trẻ và tốc độ đô thị hóa nhanh, tiềm năng phát triển của thị trường bất động sản Việt Nam vẫn được đánh giá cao. \n",
    "Các yếu tố như cải thiện cơ sở hạ tầng, sự minh bạch trong quy hoạch và chính sách pháp lý ổn định sẽ đóng vai trò quan trọng trong việc thúc đẩy sự phát triển bền vững của thị trường.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Summarization Comparison --- \")\n",
    "print(f\"Original Text (length: {len(longer_vietnamese_text)} chars):\\n{longer_vietnamese_text}\")\n",
    "\n",
    "# TextRank\n",
    "textrank_comp_summary = run_extractive_summarizer(longer_vietnamese_text, chosen_ratio=0.3)\n",
    "print(f\"\\nTextRank Summary (ratio 0.3, length: {len(textrank_comp_summary)} chars):\\n{textrank_comp_summary}\")\n",
    "\n",
    "# Abstractive (VietAI T5)\n",
    "if model_summ and tokenizer_summ:\n",
    "    abstractive_comp_summary = abstractive_summarize_vietnamese(longer_vietnamese_text, model_summ, tokenizer_summ)\n",
    "    print(f\"\\nAbstractive Summary (VietAI T5, length: {len(abstractive_comp_summary)} chars):\\n{abstractive_comp_summary}\")\n",
    "else:\n",
    "    print(\"\\nAbstractive summarization model not loaded. Skipping comparison for it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Summarization Comparison --- \n",
      "Original Text (length: 528 chars):\n",
      "\n",
      "Hà Nội mùa thu, cây cơm nguội vàng, cây bàng lá đỏ, nằm kề bên nhau, phố xưa nhà cổ, mái ngói thâm nâu. \n",
      "Hồ Gươm, Tháp Rùa, nghiêng soi bóng, nước xanh như lọc. \n",
      "Những con đường ngoại ô thành phố vẫn giữ được vẻ đẹp bình dị với những cánh đồng lúa xanh mướt và những đàn cò trắng bay lượn. \n",
      "Người dân Hà Nội nổi tiếng với sự thanh lịch, mến khách và tài hoa trong ẩm thực cũng như các nghề thủ công truyền thống. \n",
      "Mỗi độ thu về, Hà Nội lại khoác lên mình một chiếc áo mới, lãng mạn và đầy chất thơ, níu chân biết bao du khách.\n",
      "\n",
      "\n",
      "TextRank Summary (ratio 0.3, length: 121 chars):\n",
      "Người dân Hà Nội nổi tiếng với sự thanh lịch, mến khách và tài hoa trong ẩm thực cũng như các nghề thủ công truyền thống.\n",
      "\n",
      "Abstractive Summary (VietAI T5, length: 372 chars):\n",
      "Những ngày đầu đông lạnh giá, rong ruổi trên các tuyến phố Hà Nội, bạn sẽ thấy những chiếc áo mới, những cánh đồng lúa xanh mướ, hay những đàn cò rắng bay lượn trên lối đi nội ô... Cứ độ hu về, ai nấy đều tìm thấy một Hà Nội bình yên và quyến rũ khó cưỡng để trốn cái nóng hầm hập của mùa thu. Hãy cùng ngắm nhìn những khung cảnh tuyệt đẹp của thành phố vào mùa huê 2018.Ó\n"
     ]
    }
   ],
   "source": [
    "longer_vietnamese_text = \"\"\"\n",
    "Hà Nội mùa thu, cây cơm nguội vàng, cây bàng lá đỏ, nằm kề bên nhau, phố xưa nhà cổ, mái ngói thâm nâu. \n",
    "Hồ Gươm, Tháp Rùa, nghiêng soi bóng, nước xanh như lọc. \n",
    "Những con đường ngoại ô thành phố vẫn giữ được vẻ đẹp bình dị với những cánh đồng lúa xanh mướt và những đàn cò trắng bay lượn. \n",
    "Người dân Hà Nội nổi tiếng với sự thanh lịch, mến khách và tài hoa trong ẩm thực cũng như các nghề thủ công truyền thống. \n",
    "Mỗi độ thu về, Hà Nội lại khoác lên mình một chiếc áo mới, lãng mạn và đầy chất thơ, níu chân biết bao du khách.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Summarization Comparison --- \")\n",
    "print(f\"Original Text (length: {len(longer_vietnamese_text)} chars):\\n{longer_vietnamese_text}\")\n",
    "\n",
    "# TextRank\n",
    "textrank_comp_summary = run_extractive_summarizer(longer_vietnamese_text, chosen_ratio=0.3)\n",
    "print(f\"\\nTextRank Summary (ratio 0.3, length: {len(textrank_comp_summary)} chars):\\n{textrank_comp_summary}\")\n",
    "\n",
    "# Abstractive (VietAI T5)\n",
    "if model_summ and tokenizer_summ:\n",
    "    abstractive_comp_summary = abstractive_summarize_vietnamese(longer_vietnamese_text, model_summ, tokenizer_summ)\n",
    "    print(f\"\\nAbstractive Summary (VietAI T5, length: {len(abstractive_comp_summary)} chars):\\n{abstractive_comp_summary}\")\n",
    "else:\n",
    "    print(\"\\nAbstractive summarization model not loaded. Skipping comparison for it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Abstractive Summarization Demo (VietAI/vit5-base-vietnews-summarization) ---\n",
      "Original Article (length: 751 chars):\n",
      "\n",
      "Đại dịch COVID-19 đã gây ra những tác động sâu rộng đến kinh tế toàn cầu trong suốt hơn hai năm qua. \n",
      "Nhiều ngành công nghiệp chủ chốt như du lịch, hàng không, và dịch vụ ăn uống đã phải đối mặt với những thách thức chưa từng có. \n",
      "Các chính phủ trên khắp thế giới đã tung ra các gói kích thích kinh tế khổng lồ để hỗ trợ doanh nghiệp và người lao động. \n",
      "Tuy nhiên, quá trình phục hồi được dự báo sẽ không đồng đều và còn nhiều bất ổn phía trước, đặc biệt là với sự xuất hiện của các biến chủng mới. \n",
      "Chuyển đổi số và ứng dụng công nghệ được coi là chìa khóa quan trọng giúp các nền kinh tế thích ứng và vượt qua khủng hoảng. \n",
      "Việc đảm bảo chuỗi cung ứng toàn cầu và tăng cường hợp tác quốc tế cũng là những yếu tố then chốt cho sự phục hồi bền vững.\n",
      "\n",
      "\n",
      "--- Summarizing with default lengths ---\n",
      "  Ratio not provided or invalid, using default summary lengths: min=100, max=500\n",
      "Abstractive Summary (default lengths, length: 427 chars):\n",
      "Sự phát triển của các nền kinh tế đang chịu sự ảnh hưởng nặng nề từ sự phát triển nhanh chóng của công nghệ và sự thay đổi chóng mặt của các nước châu Âu châu Á trong suốt 20 năm qua đã chứng tỏ sự thất bại trong việc đạt được sự ổn định ở nhiều lĩnh vực, trong đó có lĩnh vực du lịch, hàng không, và dịch vụ ăn uống, ở nhiều nơi trên thế giới. Tỉ lệ thất nghiệp tăng, cơ cấu tỉ giá hối đoái và tăng trưởng kinh tế ở mức cao...\n",
      "\n",
      "--- Summarizing with target_ratio=0.3 ---\n",
      "  Ratio mode: Input sentences: 6, Target sentences (est.): 1\n",
      "  Ratio mode: Est. ideal tokens: 25, Using min_length: 22, max_length: 45\n",
      "Abstractive Summary (ratio ~0.3, length: 135 chars):\n",
      "Sự phát triển của các nền kinh tế đang chịu sự ảnh hưởng nặng nề từ sự phát triển nhanh chóng của công nghệ và sự thay đổi nhanh chóng.\n",
      "\n",
      "--- Summarizing with target_ratio=0.6 ---\n",
      "  Ratio mode: Input sentences: 6, Target sentences (est.): 3\n",
      "  Ratio mode: Est. ideal tokens: 75, Using min_length: 100, max_length: 115\n",
      "Abstractive Summary (ratio ~0.6, length: 427 chars):\n",
      "Sự phát triển của các nền kinh tế đang chịu sự ảnh hưởng nặng nề từ sự phát triển nhanh chóng của công nghệ và sự thay đổi chóng mặt của các nước châu Âu châu Á trong suốt 20 năm qua đã chứng tỏ sự thất bại trong việc đạt được sự ổn định ở nhiều lĩnh vực, trong đó có lĩnh vực du lịch, hàng không, và dịch vụ ăn uống, ở nhiều nơi trên thế giới. Tỉ lệ thất nghiệp tăng, cơ cấu tỉ giá hối đoái và tăng trưởng kinh tế ở mức cao...\n"
     ]
    }
   ],
   "source": [
    "# (Assuming sample_vietnamese_article_for_abstractive is defined)\n",
    "# ... (model and tokenizer loading cell should be run first) ...\n",
    "\n",
    "print(\"--- Abstractive Summarization Demo (VietAI/vit5-base-vietnews-summarization) ---\")\n",
    "print(f\"Original Article (length: {len(sample_vietnamese_article_for_abstractive)} chars):\\n{sample_vietnamese_article_for_abstractive}\")\n",
    "\n",
    "if model_summ and tokenizer_summ:\n",
    "    # Example 1: Using default lengths\n",
    "    print(\"\\n--- Summarizing with default lengths ---\")\n",
    "    abstractive_summary_default = abstractive_summarize_vietnamese(\n",
    "        sample_vietnamese_article_for_abstractive, \n",
    "        model_summ, \n",
    "        tokenizer_summ\n",
    "    )\n",
    "    print(f\"Abstractive Summary (default lengths, length: {len(abstractive_summary_default)} chars):\\n{abstractive_summary_default}\")\n",
    "\n",
    "    # Example 2: Using a target ratio (e.g., 30% of input sentences)\n",
    "    print(\"\\n--- Summarizing with target_ratio=0.3 ---\")\n",
    "    desired_ratio = 0.3\n",
    "    abstractive_summary_ratio = abstractive_summarize_vietnamese(\n",
    "        sample_vietnamese_article_for_abstractive, \n",
    "        model_summ, \n",
    "        tokenizer_summ,\n",
    "        target_ratio=desired_ratio \n",
    "    )\n",
    "    print(f\"Abstractive Summary (ratio ~{desired_ratio}, length: {len(abstractive_summary_ratio)} chars):\\n{abstractive_summary_ratio}\")\n",
    "\n",
    "    # Example 3: Using a target ratio (e.g., 60% of input sentences)\n",
    "    print(\"\\n--- Summarizing with target_ratio=0.6 ---\")\n",
    "    desired_ratio_2 = 0.6\n",
    "    abstractive_summary_ratio_2 = abstractive_summarize_vietnamese(\n",
    "        sample_vietnamese_article_for_abstractive, \n",
    "        model_summ, \n",
    "        tokenizer_summ,\n",
    "        target_ratio=desired_ratio_2\n",
    "    )\n",
    "    print(f\"Abstractive Summary (ratio ~{desired_ratio_2}, length: {len(abstractive_summary_ratio_2)} chars):\\n{abstractive_summary_ratio_2}\")\n",
    "else:\n",
    "    print(\"\\nAbstractive summarization model not loaded. Skipping demo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vifree-txt-3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
